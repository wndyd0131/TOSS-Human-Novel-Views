{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5d6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 정리\n",
    "# 각도에 따라서 결과\n",
    "# 데이터셋 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53fc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TossLoraModule: Running in eps-prediction mode\n",
      "DiffusionWrapper has 863.65 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">delightful-lovebird-40</strong> at: <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/pvdyr4ff' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/pvdyr4ff</a><br> View project at: <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260215_032708-pvdyr4ff\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Documents\\JY\\TOSS\\wandb\\run-20260215_032852-fm0rehnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/fm0rehnb' target=\"_blank\">moonlit-lovebird-41</a></strong> to <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/fm0rehnb' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/fm0rehnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Modules with checkpointing enabled: 0\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.0.weight, shape=torch.Size([320, 51])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.0.bias, shape=torch.Size([320])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.2.weight, shape=torch.Size([320, 320])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.2.bias, shape=torch.Size([320])\n",
      "[INIT] Unfroze 4 pose_net parameters\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.254158e-03\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.246503e-03\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.257065e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.240621e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.243884e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.246121e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.246768e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.243797e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.266923e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.268102e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.236407e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.241654e-03\n",
      "[INIT] Enabled requires_grad for 24 LoRA parameters\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "trainable params: 346,564 || all params: 863,861,444 || trainable%: 0.0401\n",
      "[INIT] PEFT config active: {'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules=['output_blocks.9.1.transformer_blocks.0.attn1.to_q', 'output_blocks.9.1.transformer_blocks.0.attn1.to_k', 'output_blocks.9.1.transformer_blocks.0.attn1.to_v', 'output_blocks.10.1.transformer_blocks.0.attn1.to_q', 'output_blocks.10.1.transformer_blocks.0.attn1.to_k', 'output_blocks.10.1.transformer_blocks.0.attn1.to_v', 'output_blocks.11.1.transformer_blocks.0.attn1.to_q', 'output_blocks.11.1.transformer_blocks.0.attn1.to_k', 'output_blocks.11.1.transformer_blocks.0.attn1.to_v', 'middle_block.1.transformer_blocks.0.attn1.to_q', 'middle_block.1.transformer_blocks.0.attn1.to_k', 'middle_block.1.transformer_blocks.0.attn1.to_v'], exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n",
      "\n",
      "[DEBUG] LoRA Layer Analysis:\n",
      "  base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "    scaling: {'default': 1.0}\n",
      "    disable_adapters: False\n",
      "    merged: False\n",
      "    lora_A[default]: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "    lora_B[default]: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "\n",
      "[DEBUG] Active adapter: default\n",
      "[DEBUG] Active adapters: ['default']\n",
      "\n",
      "[DEBUG] Pose Net Weight Analysis:\n",
      "  pose_net structure: Sequential(\n",
      "  (0): Linear(in_features=51, out_features=320, bias=True)\n",
      "  (1): SiLU()\n",
      "  (2): Linear(in_features=320, out_features=320, bias=True)\n",
      ")\n",
      "  pose_enc type: vae\n",
      "\n",
      "  Layer: pose_net.0.weight\n",
      "    shape: torch.Size([320, 51])\n",
      "    requires_grad: True\n",
      "    mean: 0.000534\n",
      "    std: 0.080614\n",
      "    min: -0.140013\n",
      "    max: 0.140024\n",
      "    abs_mean: 0.069765\n",
      "\n",
      "  Layer: pose_net.0.bias\n",
      "    shape: torch.Size([320])\n",
      "    requires_grad: True\n",
      "    mean: 0.004848\n",
      "    std: 0.080584\n",
      "    min: -0.139812\n",
      "    max: 0.139948\n",
      "    abs_mean: 0.069405\n",
      "\n",
      "  Layer: pose_net.2.weight\n",
      "    shape: torch.Size([320, 320])\n",
      "    requires_grad: True\n",
      "    mean: -0.000219\n",
      "    std: 0.032260\n",
      "    min: -0.055901\n",
      "    max: 0.055902\n",
      "    abs_mean: 0.027938\n",
      "\n",
      "  Layer: pose_net.2.bias\n",
      "    shape: torch.Size([320])\n",
      "    requires_grad: True\n",
      "    mean: 0.000649\n",
      "    std: 0.032205\n",
      "    min: -0.055829\n",
      "    max: 0.055514\n",
      "    abs_mean: 0.027751\n",
      "\n",
      "  Input features (pose_net[0].in_features): 51\n",
      "  Output features (pose_net[2].out_features): 320\n",
      "CRITICAL MISSING (Should be empty): []\n"
     ]
    }
   ],
   "source": [
    "# Load Model (Training)\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "lora_config_params = model.model.diffusion_model.peft_config['default']\n",
    "\n",
    "# 1. Get raw state dict\n",
    "raw_sd = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "new_sd = {}\n",
    "\n",
    "# 2. Targeted Remapping\n",
    "for k, v in raw_sd.items():\n",
    "    # Fix the PEFT wrapper prefix first\n",
    "    nk = k.replace(\"model.diffusion_model.\", \"model.diffusion_model.base_model.model.\")\n",
    "    \n",
    "    # CRITICAL: If this is a LoRA target module, it MUST end in .base_layer.weight\n",
    "    # Check if the original key (k) contains any of your target module names\n",
    "    is_target = any(target in k for target in lora_config_params.target_modules)\n",
    "    \n",
    "    if is_target:\n",
    "        # e.g., to_q.weight -> to_q.base_layer.weight\n",
    "        nk = nk.replace(\".weight\", \".base_layer.weight\")\n",
    "        nk = nk.replace(\".bias\", \".base_layer.bias\")\n",
    "    \n",
    "    new_sd[nk] = v\n",
    "\n",
    "# 3. Load into model\n",
    "missing, unexpected = model.load_state_dict(new_sd, strict=False)\n",
    "print(\"Missing:\", missing)\n",
    "\n",
    "# 4. Verify: Only lora_A/lora_B should be missing now\n",
    "final_missing = [m for m in missing if \"lora_\" not in m]\n",
    "print(f\"CRITICAL MISSING (Should be empty): {final_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad28afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TossLoraModule: Running in eps-prediction mode\n",
      "DiffusionWrapper has 863.65 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">doting-lovebird-38</strong> at: <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/ldptap03' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/ldptap03</a><br> View project at: <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260215_032125-ldptap03\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Documents\\JY\\TOSS\\wandb\\run-20260215_032255-o7ympe7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/o7ympe7s' target=\"_blank\">head-over-heels-tulip-39</a></strong> to <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/o7ympe7s' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/toss-lora/runs/o7ympe7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Modules with checkpointing enabled: 0\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.0.weight, shape=torch.Size([320, 51])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.0.bias, shape=torch.Size([320])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.2.weight, shape=torch.Size([320, 320])\n",
      "[INIT] Unfreezing pose_net param: base_model.model.pose_net.2.bias, shape=torch.Size([320])\n",
      "[INIT] Unfroze 4 pose_net parameters\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.247397e-03\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.251353e-03\n",
      "[INIT] lora_B before init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.258138e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.263661e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.242719e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.249999e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.253143e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.244864e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.248253e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, mean=1.253072e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, mean=1.264362e-03\n",
      "[INIT] lora_B before init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=0.000000e+00\n",
      "[INIT] lora_B after init: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, mean=1.255999e-03\n",
      "[INIT] Enabled requires_grad for 24 LoRA parameters\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: shape=torch.Size([16, 320]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: shape=torch.Size([320, 16]), requires_grad=True\n",
      "trainable params: 346,564 || all params: 863,861,444 || trainable%: 0.0401\n",
      "[INIT] PEFT config active: {'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules=['output_blocks.9.1.transformer_blocks.0.attn1.to_q', 'output_blocks.9.1.transformer_blocks.0.attn1.to_k', 'output_blocks.9.1.transformer_blocks.0.attn1.to_v', 'output_blocks.10.1.transformer_blocks.0.attn1.to_q', 'output_blocks.10.1.transformer_blocks.0.attn1.to_k', 'output_blocks.10.1.transformer_blocks.0.attn1.to_v', 'output_blocks.11.1.transformer_blocks.0.attn1.to_q', 'output_blocks.11.1.transformer_blocks.0.attn1.to_k', 'output_blocks.11.1.transformer_blocks.0.attn1.to_v', 'middle_block.1.transformer_blocks.0.attn1.to_q', 'middle_block.1.transformer_blocks.0.attn1.to_k', 'middle_block.1.transformer_blocks.0.attn1.to_v'], exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n",
      "\n",
      "[DEBUG] LoRA Layer Analysis:\n",
      "  base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "    scaling: {'default': 1.0}\n",
      "    disable_adapters: False\n",
      "    merged: False\n",
      "    lora_A[default]: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "    lora_B[default]: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "\n",
      "[DEBUG] Active adapter: default\n",
      "[DEBUG] Active adapters: ['default']\n",
      "\n",
      "[DEBUG] Pose Net Weight Analysis:\n",
      "  pose_net structure: Sequential(\n",
      "  (0): Linear(in_features=51, out_features=320, bias=True)\n",
      "  (1): SiLU()\n",
      "  (2): Linear(in_features=320, out_features=320, bias=True)\n",
      ")\n",
      "  pose_enc type: vae\n",
      "\n",
      "  Layer: pose_net.0.weight\n",
      "    shape: torch.Size([320, 51])\n",
      "    requires_grad: True\n",
      "    mean: 0.001297\n",
      "    std: 0.080794\n",
      "    min: -0.139999\n",
      "    max: 0.140026\n",
      "    abs_mean: 0.069904\n",
      "\n",
      "  Layer: pose_net.0.bias\n",
      "    shape: torch.Size([320])\n",
      "    requires_grad: True\n",
      "    mean: 0.002174\n",
      "    std: 0.081415\n",
      "    min: -0.139683\n",
      "    max: 0.140020\n",
      "    abs_mean: 0.071006\n",
      "\n",
      "  Layer: pose_net.2.weight\n",
      "    shape: torch.Size([320, 320])\n",
      "    requires_grad: True\n",
      "    mean: 0.000043\n",
      "    std: 0.032278\n",
      "    min: -0.055902\n",
      "    max: 0.055900\n",
      "    abs_mean: 0.027947\n",
      "\n",
      "  Layer: pose_net.2.bias\n",
      "    shape: torch.Size([320])\n",
      "    requires_grad: True\n",
      "    mean: -0.001449\n",
      "    std: 0.032914\n",
      "    min: -0.055703\n",
      "    max: 0.055774\n",
      "    abs_mean: 0.028583\n",
      "\n",
      "  Input features (pose_net[0].in_features): 51\n",
      "  Output features (pose_net[2].out_features): 320\n",
      "Missing keys: ['model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.base_layer.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.base_layer.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.base_layer.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.base_layer.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'model.diffusion_model.base_model.model.pose_net.0.weight', 'model.diffusion_model.base_model.model.pose_net.0.bias', 'model.diffusion_model.base_model.model.pose_net.2.weight', 'model.diffusion_model.base_model.model.pose_net.2.bias']\n",
      "Unexpected keys: ['model_ema.decay', 'model_ema.num_updates', 'model_ema.diffusion_modeltime_embed0weight', 'model_ema.diffusion_modeltime_embed0bias', 'model_ema.diffusion_modeltime_embed2weight', 'model_ema.diffusion_modeltime_embed2bias', 'model_ema.diffusion_modelinput_blocks00weight', 'model_ema.diffusion_modelinput_blocks00bias', 'model_ema.diffusion_modelinput_blocks10in_layers0weight', 'model_ema.diffusion_modelinput_blocks10in_layers0bias', 'model_ema.diffusion_modelinput_blocks10in_layers2weight', 'model_ema.diffusion_modelinput_blocks10in_layers2bias', 'model_ema.diffusion_modelinput_blocks10emb_layers1weight', 'model_ema.diffusion_modelinput_blocks10emb_layers1bias', 'model_ema.diffusion_modelinput_blocks10out_layers0weight', 'model_ema.diffusion_modelinput_blocks10out_layers0bias', 'model_ema.diffusion_modelinput_blocks10out_layers3weight', 'model_ema.diffusion_modelinput_blocks10out_layers3bias', 'model_ema.diffusion_modelinput_blocks11normweight', 'model_ema.diffusion_modelinput_blocks11normbias', 'model_ema.diffusion_modelinput_blocks11proj_inweight', 'model_ema.diffusion_modelinput_blocks11proj_inbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks11proj_outweight', 'model_ema.diffusion_modelinput_blocks11proj_outbias', 'model_ema.diffusion_modelinput_blocks20in_layers0weight', 'model_ema.diffusion_modelinput_blocks20in_layers0bias', 'model_ema.diffusion_modelinput_blocks20in_layers2weight', 'model_ema.diffusion_modelinput_blocks20in_layers2bias', 'model_ema.diffusion_modelinput_blocks20emb_layers1weight', 'model_ema.diffusion_modelinput_blocks20emb_layers1bias', 'model_ema.diffusion_modelinput_blocks20out_layers0weight', 'model_ema.diffusion_modelinput_blocks20out_layers0bias', 'model_ema.diffusion_modelinput_blocks20out_layers3weight', 'model_ema.diffusion_modelinput_blocks20out_layers3bias', 'model_ema.diffusion_modelinput_blocks21normweight', 'model_ema.diffusion_modelinput_blocks21normbias', 'model_ema.diffusion_modelinput_blocks21proj_inweight', 'model_ema.diffusion_modelinput_blocks21proj_inbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks21proj_outweight', 'model_ema.diffusion_modelinput_blocks21proj_outbias', 'model_ema.diffusion_modelinput_blocks30opweight', 'model_ema.diffusion_modelinput_blocks30opbias', 'model_ema.diffusion_modelinput_blocks40in_layers0weight', 'model_ema.diffusion_modelinput_blocks40in_layers0bias', 'model_ema.diffusion_modelinput_blocks40in_layers2weight', 'model_ema.diffusion_modelinput_blocks40in_layers2bias', 'model_ema.diffusion_modelinput_blocks40emb_layers1weight', 'model_ema.diffusion_modelinput_blocks40emb_layers1bias', 'model_ema.diffusion_modelinput_blocks40out_layers0weight', 'model_ema.diffusion_modelinput_blocks40out_layers0bias', 'model_ema.diffusion_modelinput_blocks40out_layers3weight', 'model_ema.diffusion_modelinput_blocks40out_layers3bias', 'model_ema.diffusion_modelinput_blocks40skip_connectionweight', 'model_ema.diffusion_modelinput_blocks40skip_connectionbias', 'model_ema.diffusion_modelinput_blocks41normweight', 'model_ema.diffusion_modelinput_blocks41normbias', 'model_ema.diffusion_modelinput_blocks41proj_inweight', 'model_ema.diffusion_modelinput_blocks41proj_inbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks41proj_outweight', 'model_ema.diffusion_modelinput_blocks41proj_outbias', 'model_ema.diffusion_modelinput_blocks50in_layers0weight', 'model_ema.diffusion_modelinput_blocks50in_layers0bias', 'model_ema.diffusion_modelinput_blocks50in_layers2weight', 'model_ema.diffusion_modelinput_blocks50in_layers2bias', 'model_ema.diffusion_modelinput_blocks50emb_layers1weight', 'model_ema.diffusion_modelinput_blocks50emb_layers1bias', 'model_ema.diffusion_modelinput_blocks50out_layers0weight', 'model_ema.diffusion_modelinput_blocks50out_layers0bias', 'model_ema.diffusion_modelinput_blocks50out_layers3weight', 'model_ema.diffusion_modelinput_blocks50out_layers3bias', 'model_ema.diffusion_modelinput_blocks51normweight', 'model_ema.diffusion_modelinput_blocks51normbias', 'model_ema.diffusion_modelinput_blocks51proj_inweight', 'model_ema.diffusion_modelinput_blocks51proj_inbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks51proj_outweight', 'model_ema.diffusion_modelinput_blocks51proj_outbias', 'model_ema.diffusion_modelinput_blocks60opweight', 'model_ema.diffusion_modelinput_blocks60opbias', 'model_ema.diffusion_modelinput_blocks70in_layers0weight', 'model_ema.diffusion_modelinput_blocks70in_layers0bias', 'model_ema.diffusion_modelinput_blocks70in_layers2weight', 'model_ema.diffusion_modelinput_blocks70in_layers2bias', 'model_ema.diffusion_modelinput_blocks70emb_layers1weight', 'model_ema.diffusion_modelinput_blocks70emb_layers1bias', 'model_ema.diffusion_modelinput_blocks70out_layers0weight', 'model_ema.diffusion_modelinput_blocks70out_layers0bias', 'model_ema.diffusion_modelinput_blocks70out_layers3weight', 'model_ema.diffusion_modelinput_blocks70out_layers3bias', 'model_ema.diffusion_modelinput_blocks70skip_connectionweight', 'model_ema.diffusion_modelinput_blocks70skip_connectionbias', 'model_ema.diffusion_modelinput_blocks71normweight', 'model_ema.diffusion_modelinput_blocks71normbias', 'model_ema.diffusion_modelinput_blocks71proj_inweight', 'model_ema.diffusion_modelinput_blocks71proj_inbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks71proj_outweight', 'model_ema.diffusion_modelinput_blocks71proj_outbias', 'model_ema.diffusion_modelinput_blocks80in_layers0weight', 'model_ema.diffusion_modelinput_blocks80in_layers0bias', 'model_ema.diffusion_modelinput_blocks80in_layers2weight', 'model_ema.diffusion_modelinput_blocks80in_layers2bias', 'model_ema.diffusion_modelinput_blocks80emb_layers1weight', 'model_ema.diffusion_modelinput_blocks80emb_layers1bias', 'model_ema.diffusion_modelinput_blocks80out_layers0weight', 'model_ema.diffusion_modelinput_blocks80out_layers0bias', 'model_ema.diffusion_modelinput_blocks80out_layers3weight', 'model_ema.diffusion_modelinput_blocks80out_layers3bias', 'model_ema.diffusion_modelinput_blocks81normweight', 'model_ema.diffusion_modelinput_blocks81normbias', 'model_ema.diffusion_modelinput_blocks81proj_inweight', 'model_ema.diffusion_modelinput_blocks81proj_inbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks81proj_outweight', 'model_ema.diffusion_modelinput_blocks81proj_outbias', 'model_ema.diffusion_modelinput_blocks90opweight', 'model_ema.diffusion_modelinput_blocks90opbias', 'model_ema.diffusion_modelinput_blocks100in_layers0weight', 'model_ema.diffusion_modelinput_blocks100in_layers0bias', 'model_ema.diffusion_modelinput_blocks100in_layers2weight', 'model_ema.diffusion_modelinput_blocks100in_layers2bias', 'model_ema.diffusion_modelinput_blocks100emb_layers1weight', 'model_ema.diffusion_modelinput_blocks100emb_layers1bias', 'model_ema.diffusion_modelinput_blocks100out_layers0weight', 'model_ema.diffusion_modelinput_blocks100out_layers0bias', 'model_ema.diffusion_modelinput_blocks100out_layers3weight', 'model_ema.diffusion_modelinput_blocks100out_layers3bias', 'model_ema.diffusion_modelinput_blocks110in_layers0weight', 'model_ema.diffusion_modelinput_blocks110in_layers0bias', 'model_ema.diffusion_modelinput_blocks110in_layers2weight', 'model_ema.diffusion_modelinput_blocks110in_layers2bias', 'model_ema.diffusion_modelinput_blocks110emb_layers1weight', 'model_ema.diffusion_modelinput_blocks110emb_layers1bias', 'model_ema.diffusion_modelinput_blocks110out_layers0weight', 'model_ema.diffusion_modelinput_blocks110out_layers0bias', 'model_ema.diffusion_modelinput_blocks110out_layers3weight', 'model_ema.diffusion_modelinput_blocks110out_layers3bias', 'model_ema.diffusion_modelmiddle_block0in_layers0weight', 'model_ema.diffusion_modelmiddle_block0in_layers0bias', 'model_ema.diffusion_modelmiddle_block0in_layers2weight', 'model_ema.diffusion_modelmiddle_block0in_layers2bias', 'model_ema.diffusion_modelmiddle_block0emb_layers1weight', 'model_ema.diffusion_modelmiddle_block0emb_layers1bias', 'model_ema.diffusion_modelmiddle_block0out_layers0weight', 'model_ema.diffusion_modelmiddle_block0out_layers0bias', 'model_ema.diffusion_modelmiddle_block0out_layers3weight', 'model_ema.diffusion_modelmiddle_block0out_layers3bias', 'model_ema.diffusion_modelmiddle_block1normweight', 'model_ema.diffusion_modelmiddle_block1normbias', 'model_ema.diffusion_modelmiddle_block1proj_inweight', 'model_ema.diffusion_modelmiddle_block1proj_inbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0vae_projweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0vae_projbias', 'model_ema.diffusion_modelmiddle_block1proj_outweight', 'model_ema.diffusion_modelmiddle_block1proj_outbias', 'model_ema.diffusion_modelmiddle_block2in_layers0weight', 'model_ema.diffusion_modelmiddle_block2in_layers0bias', 'model_ema.diffusion_modelmiddle_block2in_layers2weight', 'model_ema.diffusion_modelmiddle_block2in_layers2bias', 'model_ema.diffusion_modelmiddle_block2emb_layers1weight', 'model_ema.diffusion_modelmiddle_block2emb_layers1bias', 'model_ema.diffusion_modelmiddle_block2out_layers0weight', 'model_ema.diffusion_modelmiddle_block2out_layers0bias', 'model_ema.diffusion_modelmiddle_block2out_layers3weight', 'model_ema.diffusion_modelmiddle_block2out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00in_layers0weight', 'model_ema.diffusion_modeloutput_blocks00in_layers0bias', 'model_ema.diffusion_modeloutput_blocks00in_layers2weight', 'model_ema.diffusion_modeloutput_blocks00in_layers2bias', 'model_ema.diffusion_modeloutput_blocks00emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks00emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks00out_layers0weight', 'model_ema.diffusion_modeloutput_blocks00out_layers0bias', 'model_ema.diffusion_modeloutput_blocks00out_layers3weight', 'model_ema.diffusion_modeloutput_blocks00out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks00skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks10in_layers0weight', 'model_ema.diffusion_modeloutput_blocks10in_layers0bias', 'model_ema.diffusion_modeloutput_blocks10in_layers2weight', 'model_ema.diffusion_modeloutput_blocks10in_layers2bias', 'model_ema.diffusion_modeloutput_blocks10emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks10emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks10out_layers0weight', 'model_ema.diffusion_modeloutput_blocks10out_layers0bias', 'model_ema.diffusion_modeloutput_blocks10out_layers3weight', 'model_ema.diffusion_modeloutput_blocks10out_layers3bias', 'model_ema.diffusion_modeloutput_blocks10skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks10skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks20in_layers0weight', 'model_ema.diffusion_modeloutput_blocks20in_layers0bias', 'model_ema.diffusion_modeloutput_blocks20in_layers2weight', 'model_ema.diffusion_modeloutput_blocks20in_layers2bias', 'model_ema.diffusion_modeloutput_blocks20emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks20emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks20out_layers0weight', 'model_ema.diffusion_modeloutput_blocks20out_layers0bias', 'model_ema.diffusion_modeloutput_blocks20out_layers3weight', 'model_ema.diffusion_modeloutput_blocks20out_layers3bias', 'model_ema.diffusion_modeloutput_blocks20skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks20skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks21convweight', 'model_ema.diffusion_modeloutput_blocks21convbias', 'model_ema.diffusion_modeloutput_blocks30in_layers0weight', 'model_ema.diffusion_modeloutput_blocks30in_layers0bias', 'model_ema.diffusion_modeloutput_blocks30in_layers2weight', 'model_ema.diffusion_modeloutput_blocks30in_layers2bias', 'model_ema.diffusion_modeloutput_blocks30emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks30emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks30out_layers0weight', 'model_ema.diffusion_modeloutput_blocks30out_layers0bias', 'model_ema.diffusion_modeloutput_blocks30out_layers3weight', 'model_ema.diffusion_modeloutput_blocks30out_layers3bias', 'model_ema.diffusion_modeloutput_blocks30skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks30skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks31normweight', 'model_ema.diffusion_modeloutput_blocks31normbias', 'model_ema.diffusion_modeloutput_blocks31proj_inweight', 'model_ema.diffusion_modeloutput_blocks31proj_inbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks31proj_outweight', 'model_ema.diffusion_modeloutput_blocks31proj_outbias', 'model_ema.diffusion_modeloutput_blocks40in_layers0weight', 'model_ema.diffusion_modeloutput_blocks40in_layers0bias', 'model_ema.diffusion_modeloutput_blocks40in_layers2weight', 'model_ema.diffusion_modeloutput_blocks40in_layers2bias', 'model_ema.diffusion_modeloutput_blocks40emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks40emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks40out_layers0weight', 'model_ema.diffusion_modeloutput_blocks40out_layers0bias', 'model_ema.diffusion_modeloutput_blocks40out_layers3weight', 'model_ema.diffusion_modeloutput_blocks40out_layers3bias', 'model_ema.diffusion_modeloutput_blocks40skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks40skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks41normweight', 'model_ema.diffusion_modeloutput_blocks41normbias', 'model_ema.diffusion_modeloutput_blocks41proj_inweight', 'model_ema.diffusion_modeloutput_blocks41proj_inbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks41proj_outweight', 'model_ema.diffusion_modeloutput_blocks41proj_outbias', 'model_ema.diffusion_modeloutput_blocks50in_layers0weight', 'model_ema.diffusion_modeloutput_blocks50in_layers0bias', 'model_ema.diffusion_modeloutput_blocks50in_layers2weight', 'model_ema.diffusion_modeloutput_blocks50in_layers2bias', 'model_ema.diffusion_modeloutput_blocks50emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks50emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks50out_layers0weight', 'model_ema.diffusion_modeloutput_blocks50out_layers0bias', 'model_ema.diffusion_modeloutput_blocks50out_layers3weight', 'model_ema.diffusion_modeloutput_blocks50out_layers3bias', 'model_ema.diffusion_modeloutput_blocks50skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks50skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks51normweight', 'model_ema.diffusion_modeloutput_blocks51normbias', 'model_ema.diffusion_modeloutput_blocks51proj_inweight', 'model_ema.diffusion_modeloutput_blocks51proj_inbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks51proj_outweight', 'model_ema.diffusion_modeloutput_blocks51proj_outbias', 'model_ema.diffusion_modeloutput_blocks52convweight', 'model_ema.diffusion_modeloutput_blocks52convbias', 'model_ema.diffusion_modeloutput_blocks60in_layers0weight', 'model_ema.diffusion_modeloutput_blocks60in_layers0bias', 'model_ema.diffusion_modeloutput_blocks60in_layers2weight', 'model_ema.diffusion_modeloutput_blocks60in_layers2bias', 'model_ema.diffusion_modeloutput_blocks60emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks60emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks60out_layers0weight', 'model_ema.diffusion_modeloutput_blocks60out_layers0bias', 'model_ema.diffusion_modeloutput_blocks60out_layers3weight', 'model_ema.diffusion_modeloutput_blocks60out_layers3bias', 'model_ema.diffusion_modeloutput_blocks60skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks60skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks61normweight', 'model_ema.diffusion_modeloutput_blocks61normbias', 'model_ema.diffusion_modeloutput_blocks61proj_inweight', 'model_ema.diffusion_modeloutput_blocks61proj_inbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks61proj_outweight', 'model_ema.diffusion_modeloutput_blocks61proj_outbias', 'model_ema.diffusion_modeloutput_blocks70in_layers0weight', 'model_ema.diffusion_modeloutput_blocks70in_layers0bias', 'model_ema.diffusion_modeloutput_blocks70in_layers2weight', 'model_ema.diffusion_modeloutput_blocks70in_layers2bias', 'model_ema.diffusion_modeloutput_blocks70emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks70emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks70out_layers0weight', 'model_ema.diffusion_modeloutput_blocks70out_layers0bias', 'model_ema.diffusion_modeloutput_blocks70out_layers3weight', 'model_ema.diffusion_modeloutput_blocks70out_layers3bias', 'model_ema.diffusion_modeloutput_blocks70skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks70skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks71normweight', 'model_ema.diffusion_modeloutput_blocks71normbias', 'model_ema.diffusion_modeloutput_blocks71proj_inweight', 'model_ema.diffusion_modeloutput_blocks71proj_inbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks71proj_outweight', 'model_ema.diffusion_modeloutput_blocks71proj_outbias', 'model_ema.diffusion_modeloutput_blocks80in_layers0weight', 'model_ema.diffusion_modeloutput_blocks80in_layers0bias', 'model_ema.diffusion_modeloutput_blocks80in_layers2weight', 'model_ema.diffusion_modeloutput_blocks80in_layers2bias', 'model_ema.diffusion_modeloutput_blocks80emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks80emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks80out_layers0weight', 'model_ema.diffusion_modeloutput_blocks80out_layers0bias', 'model_ema.diffusion_modeloutput_blocks80out_layers3weight', 'model_ema.diffusion_modeloutput_blocks80out_layers3bias', 'model_ema.diffusion_modeloutput_blocks80skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks80skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks81normweight', 'model_ema.diffusion_modeloutput_blocks81normbias', 'model_ema.diffusion_modeloutput_blocks81proj_inweight', 'model_ema.diffusion_modeloutput_blocks81proj_inbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks81proj_outweight', 'model_ema.diffusion_modeloutput_blocks81proj_outbias', 'model_ema.diffusion_modeloutput_blocks82convweight', 'model_ema.diffusion_modeloutput_blocks82convbias', 'model_ema.diffusion_modeloutput_blocks90in_layers0weight', 'model_ema.diffusion_modeloutput_blocks90in_layers0bias', 'model_ema.diffusion_modeloutput_blocks90in_layers2weight', 'model_ema.diffusion_modeloutput_blocks90in_layers2bias', 'model_ema.diffusion_modeloutput_blocks90emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks90emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks90out_layers0weight', 'model_ema.diffusion_modeloutput_blocks90out_layers0bias', 'model_ema.diffusion_modeloutput_blocks90out_layers3weight', 'model_ema.diffusion_modeloutput_blocks90out_layers3bias', 'model_ema.diffusion_modeloutput_blocks90skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks90skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks91normweight', 'model_ema.diffusion_modeloutput_blocks91normbias', 'model_ema.diffusion_modeloutput_blocks91proj_inweight', 'model_ema.diffusion_modeloutput_blocks91proj_inbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks91proj_outweight', 'model_ema.diffusion_modeloutput_blocks91proj_outbias', 'model_ema.diffusion_modeloutput_blocks100in_layers0weight', 'model_ema.diffusion_modeloutput_blocks100in_layers0bias', 'model_ema.diffusion_modeloutput_blocks100in_layers2weight', 'model_ema.diffusion_modeloutput_blocks100in_layers2bias', 'model_ema.diffusion_modeloutput_blocks100emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks100emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks100out_layers0weight', 'model_ema.diffusion_modeloutput_blocks100out_layers0bias', 'model_ema.diffusion_modeloutput_blocks100out_layers3weight', 'model_ema.diffusion_modeloutput_blocks100out_layers3bias', 'model_ema.diffusion_modeloutput_blocks100skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks100skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks101normweight', 'model_ema.diffusion_modeloutput_blocks101normbias', 'model_ema.diffusion_modeloutput_blocks101proj_inweight', 'model_ema.diffusion_modeloutput_blocks101proj_inbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks101proj_outweight', 'model_ema.diffusion_modeloutput_blocks101proj_outbias', 'model_ema.diffusion_modeloutput_blocks110in_layers0weight', 'model_ema.diffusion_modeloutput_blocks110in_layers0bias', 'model_ema.diffusion_modeloutput_blocks110in_layers2weight', 'model_ema.diffusion_modeloutput_blocks110in_layers2bias', 'model_ema.diffusion_modeloutput_blocks110emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks110emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks110out_layers0weight', 'model_ema.diffusion_modeloutput_blocks110out_layers0bias', 'model_ema.diffusion_modeloutput_blocks110out_layers3weight', 'model_ema.diffusion_modeloutput_blocks110out_layers3bias', 'model_ema.diffusion_modeloutput_blocks110skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks110skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks111normweight', 'model_ema.diffusion_modeloutput_blocks111normbias', 'model_ema.diffusion_modeloutput_blocks111proj_inweight', 'model_ema.diffusion_modeloutput_blocks111proj_inbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks111proj_outweight', 'model_ema.diffusion_modeloutput_blocks111proj_outbias', 'model_ema.diffusion_modelout0weight', 'model_ema.diffusion_modelout0bias', 'model_ema.diffusion_modelout2weight', 'model_ema.diffusion_modelout2bias', 'model_ema.diffusion_modelpose_net0weight', 'model_ema.diffusion_modelpose_net0bias', 'model_ema.diffusion_modelpose_net2weight', 'model_ema.diffusion_modelpose_net2bias', 'model.diffusion_model.pose_net.0.weight', 'model.diffusion_model.pose_net.0.bias', 'model.diffusion_model.pose_net.2.weight', 'model.diffusion_model.pose_net.2.bias', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'cond_stage_model.transformer.text_model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "# Load Model (Training)\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "# state_dict = load_state_dict(\"ckpt/toss.ckpt\")\n",
    "\n",
    "# 1. Load the raw checkpoint\n",
    "state_dict = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)\n",
    "if \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "# 2. Fix the naming mismatch caused by PEFT\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    # If the key belongs to the diffusion model, add the PEFT prefix\n",
    "    if k.startswith(\"model.diffusion_model.\") and \"pose_net\" not in k:\n",
    "        new_key = k.replace(\"model.diffusion_model.\", \"model.diffusion_model.base_model.model.\")\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# 3. Load into your model\n",
    "m, u = model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# print(f\"Missing keys (should be only pose_net and lora): {m}\")\n",
    "\n",
    "# keys_to_remove = [\n",
    "#     k for k in state_dict.keys()\n",
    "#     if \"pose_net\" in k\n",
    "# ]\n",
    "\n",
    "# for k in keys_to_remove:\n",
    "#     print(\"Removing:\", k)\n",
    "#     del state_dict[k]\n",
    "\n",
    "# m, u = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"Missing keys:\", m)\n",
    "print(\"Unexpected keys:\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd3684a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.automatic_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf8eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 32\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.out.0.weight\n",
      "model.diffusion_model.base_model.model.out.0.bias\n",
      "model.diffusion_model.base_model.model.out.2.weight\n",
      "model.diffusion_model.base_model.model.out.2.bias\n",
      "model.diffusion_model.base_model.model.pose_net.0.weight\n",
      "model.diffusion_model.base_model.model.pose_net.0.bias\n",
      "model.diffusion_model.base_model.model.pose_net.2.weight\n",
      "model.diffusion_model.base_model.model.pose_net.2.bias\n"
     ]
    }
   ],
   "source": [
    "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", len(trainable))\n",
    "for n in trainable:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f19d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rotation_matrix_to_euler(R):\n",
    "    \"\"\"\n",
    "    Extract pitch (x-rotation) and yaw (y-rotation) from a 3x3 rotation matrix.\n",
    "    Returns angles in radians.\n",
    "    \n",
    "    Assumes rotation order: R = Ry(yaw) @ Rx(pitch) @ Rz(roll)\n",
    "    \"\"\"\n",
    "    # Clamp to avoid numerical issues with asin\n",
    "    sy = np.clip(R[0, 2], -1.0, 1.0)\n",
    "    yaw = np.arcsin(sy)\n",
    "    \n",
    "    # Check for gimbal lock\n",
    "    if np.abs(sy) < 0.99999:\n",
    "        pitch = np.arctan2(-R[1, 2], R[2, 2])\n",
    "    else:\n",
    "        pitch = np.arctan2(R[2, 1], R[1, 1])\n",
    "    \n",
    "    return pitch, yaw\n",
    "\n",
    "\n",
    "def pose_matrix_to_toss_format(pose_4x4):\n",
    "    \"\"\"\n",
    "    Convert a 4x4 pose matrix to TOSS format: [pitch, yaw, distance]\n",
    "    \n",
    "    Args:\n",
    "        pose_4x4: 4x4 transformation matrix (camera-to-world or world-to-camera)\n",
    "    \n",
    "    Returns:\n",
    "        [pitch, yaw, distance] as expected by TOSS pose_enc=\"vae\" or \"freq\"\n",
    "    \"\"\"\n",
    "    R = pose_4x4[:3, :3]  # 3x3 rotation\n",
    "    t = pose_4x4[:3, 3]   # translation vector\n",
    "    \n",
    "    pitch, yaw = rotation_matrix_to_euler(R)\n",
    "    \n",
    "    # Distance: typically the Z component or the norm of translation\n",
    "    # Adjust based on your coordinate system\n",
    "    distance = np.linalg.norm(t)  # or t[2] if Z is the depth axis\n",
    "    \n",
    "    return np.array([pitch, yaw, distance], dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_relative_pose(src_pose_4x4, tgt_pose_4x4):\n",
    "    \"\"\"\n",
    "    Compute relative pose from source to target view.\n",
    "    Returns [delta_pitch, delta_yaw, delta_distance] in radians.\n",
    "    \"\"\"\n",
    "    src_pitch, src_yaw = rotation_matrix_to_euler(src_pose_4x4[:3, :3])\n",
    "    tgt_pitch, tgt_yaw = rotation_matrix_to_euler(tgt_pose_4x4[:3, :3])\n",
    "    \n",
    "    src_dist = np.linalg.norm(src_pose_4x4[:3, 3])\n",
    "    tgt_dist = np.linalg.norm(tgt_pose_4x4[:3, 3])\n",
    "    \n",
    "    delta_pitch = tgt_pitch - src_pitch\n",
    "    delta_yaw = tgt_yaw - src_yaw\n",
    "    delta_distance = tgt_dist - src_dist\n",
    "    \n",
    "    return np.array([delta_pitch, delta_yaw, delta_distance], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf57482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16)\n"
     ]
    }
   ],
   "source": [
    "poses = np.load('poses.npy')\n",
    "print(poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0546922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def rotation_matrix_to_euler(R):\n",
    "    \"\"\"\n",
    "    Extract pitch (x-rotation) and yaw (y-rotation) from a 3x3 rotation matrix.\n",
    "    Returns angles in RADIANS.\n",
    "    \"\"\"\n",
    "    # Clamp to avoid numerical issues with asin\n",
    "    sy = np.clip(R[0, 2], -1.0, 1.0)\n",
    "    yaw = np.arcsin(sy)  # Returns radians [-π/2, π/2]\n",
    "    \n",
    "    # Check for gimbal lock\n",
    "    if np.abs(sy) < 0.99999:\n",
    "        pitch = np.arctan2(-R[1, 2], R[2, 2])  # Returns radians\n",
    "    else:\n",
    "        pitch = np.arctan2(R[2, 1], R[1, 1])\n",
    "    \n",
    "    return pitch, yaw\n",
    "\n",
    "\n",
    "def compute_relative_pose(src_pose_4x4, tgt_pose_4x4):\n",
    "    \"\"\"\n",
    "    Compute relative pose from source to target view.\n",
    "    Returns [delta_pitch, delta_yaw, delta_distance] in RADIANS.\n",
    "    \"\"\"\n",
    "    src_pitch, src_yaw = rotation_matrix_to_euler(src_pose_4x4[:3, :3])\n",
    "    tgt_pitch, tgt_yaw = rotation_matrix_to_euler(tgt_pose_4x4[:3, :3])\n",
    "    \n",
    "    src_dist = np.linalg.norm(src_pose_4x4[:3, 3])\n",
    "    tgt_dist = np.linalg.norm(tgt_pose_4x4[:3, 3])\n",
    "    \n",
    "    delta_pitch = tgt_pitch - src_pitch\n",
    "    delta_yaw = tgt_yaw - src_yaw\n",
    "    delta_distance = tgt_dist - src_dist\n",
    "    \n",
    "    return np.array([delta_pitch, delta_yaw, delta_distance], dtype=np.float32)\n",
    "\n",
    "\n",
    "class Portrait4dDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, src_view_idx=9, skip_identity=True):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.src_view_idx = src_view_idx\n",
    "        self.skip_identity = skip_identity\n",
    "        \n",
    "        self.subjects = [d for d in os.listdir(root_dir) \n",
    "                        if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.samples = []\n",
    "        \n",
    "        skipped = 0\n",
    "        for sub in self.subjects:\n",
    "            sub_path = os.path.join(root_dir, sub)\n",
    "            poses_path = os.path.join(sub_path, 'poses.npy')\n",
    "            \n",
    "            if not os.path.exists(poses_path):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            views = sorted([f for f in os.listdir(sub_path) \n",
    "                          if f.endswith('.jpg') and not f.startswith('src')])\n",
    "\n",
    "            for view_file in views:\n",
    "                view_id = view_file.split('.')[0]\n",
    "                view_index = int(view_id)\n",
    "                \n",
    "                if self.skip_identity and view_index == self.src_view_idx:\n",
    "                    continue\n",
    "                \n",
    "                mask_file = f\"{view_id}_mask.png\"\n",
    "                if os.path.exists(os.path.join(sub_path, mask_file)):\n",
    "                    self.samples.append({\n",
    "                        'sub_path': sub_path,\n",
    "                        'view_file': view_file,\n",
    "                        'mask_file': mask_file,\n",
    "                        'view_index': view_index\n",
    "                    })\n",
    "        \n",
    "        print(f\"[Dataset] Loaded {len(self.samples)} samples, skipped {skipped} incomplete subjects\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        sub_path = sample['sub_path']\n",
    "        \n",
    "        # Use center view (00009.jpg) as source for pose consistency\n",
    "        src_path = os.path.join(sub_path, f\"{self.src_view_idx:05d}.jpg\")\n",
    "        src = Image.open(src_path).convert(\"RGB\")\n",
    "\n",
    "        img_path = os.path.join(sub_path, sample['view_file'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        mask_path = os.path.join(sub_path, sample['mask_file'])\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Load and reshape poses\n",
    "        poses = np.load(os.path.join(sub_path, 'poses.npy'))\n",
    "        if poses.ndim == 2 and poses.shape[1] == 16:\n",
    "            poses = poses.reshape(-1, 4, 4)\n",
    "        \n",
    "        # Compute relative pose (returns RADIANS)\n",
    "        delta_pose = compute_relative_pose(\n",
    "            poses[self.src_view_idx], \n",
    "            poses[sample['view_index']]\n",
    "        )\n",
    "        \n",
    "        # DEBUG: Verify units are radians\n",
    "        # Expected: delta_yaw should be in range [-0.35, 0.35] for ±20° rotation\n",
    "        assert abs(delta_pose[1]) < 1.0, f\"Yaw {delta_pose[1]} seems too large - check units!\"\n",
    "        \n",
    "        delta_pose = torch.from_numpy(delta_pose).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            src = self.transform(src)\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        mask_transform = T.Compose([T.Resize((256, 256)), T.ToTensor()])\n",
    "        mask = mask_transform(mask)\n",
    "    \n",
    "        return {\n",
    "            \"jpg\": image,\n",
    "            \"hint\": src,\n",
    "            'mask': mask,\n",
    "            \"delta_pose\": delta_pose,\n",
    "            \"subject_id\": os.path.basename(sub_path),\n",
    "            \"txt\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82cd3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portrait4D Dataset Class\n",
    "\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# root\n",
    "    # 251514-296055\n",
    "        # src_img.png\n",
    "        # 00000.jpg-00020.jpg # tgt\n",
    "        # 00000_mask.png-00020.png # mask\n",
    "        # poses.npy # pose\n",
    "        \n",
    "        # src_img\n",
    "        # tgt_img\n",
    "        # mask\n",
    "        # delta_pose\n",
    "        # txt\n",
    "import os\n",
    "\n",
    "class Portrait4dDataset:\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # self.root = Path(root)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.subjects = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.samples = []\n",
    "        \n",
    "        for sub in self.subjects:\n",
    "            sub_path = os.path.join(root_dir, sub)\n",
    "            views = sorted([f for f in os.listdir(sub_path) if f.endswith('.jpg') and not f.startswith('src')])\n",
    "\n",
    "            for view_file in views:\n",
    "                view_id = view_file.split('.')[0]\n",
    "                mask_file = f\"{view_id}_mask.png\"\n",
    "                \n",
    "                if os.path.exists(os.path.join(sub_path, mask_file)):\n",
    "                    self.samples.append({\n",
    "                        'sub_path': sub_path,\n",
    "                        'view_file': view_file,\n",
    "                        'mask_file': mask_file,\n",
    "                        'view_index': int(view_id)\n",
    "                    })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        sub_path = sample['sub_path']\n",
    "        src_idx = 9\n",
    "        \n",
    "        # Load Source Image\n",
    "        # src_path = os.path.join(sub_path, \"src.jpg\")\n",
    "        src_path = os.path.join(sub_path, \"00009.jpg\")\n",
    "        src = Image.open(src_path).convert(\"RGB\")\n",
    "\n",
    "        # Load View Image\n",
    "        img_path = os.path.join(sub_path, sample['view_file'])\n",
    "        target = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load Mask\n",
    "        mask_path = os.path.join(sub_path, sample['mask_file'])\n",
    "        mask = Image.open(mask_path).convert(\"L\") # 1-channel grayscale\n",
    "        \n",
    "        # Load Pose\n",
    "        poses = np.load(os.path.join(sub_path, 'poses.npy'))\n",
    "\n",
    "        if poses.ndim == 2 and poses.shape[1] == 16:\n",
    "            poses = poses.reshape(-1, 4, 4)\n",
    "\n",
    "        src_pose = poses[src_idx]\n",
    "        \n",
    "        # Target: The pose of the current view (000xx.jpg)\n",
    "        tgt_pose = poses[sample['view_index']]\n",
    "        \n",
    "        # Delta: The movement required to get from 'hint' to 'target'\n",
    "        delta_pose = compute_relative_pose(src_pose, tgt_pose)\n",
    "        delta_pose = torch.from_numpy(delta_pose).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            src = self.transform(src)\n",
    "            target = self.transform(target)\n",
    "            mask_transform = T.Compose([\n",
    "                T.Resize((256, 256)),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "            mask = mask_transform(mask)\n",
    "    \n",
    "        return {\n",
    "            \"jpg\": target, # target pose image\n",
    "            \"hint\": src, # src image\n",
    "            'mask': mask, # mask of target pose image\n",
    "            \"delta_pose\": delta_pose, # delta pose of target - src\n",
    "            \"subject_id\": os.path.basename(sub_path),\n",
    "            \"txt\": \"\"\n",
    "        }\n",
    "    \n",
    "# Target pose 별로 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2488003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import dataloader\n",
    "\n",
    "# # Debug: Verify pose extraction is working correctly\n",
    "# # Run this BEFORE training to validate your data\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from torchvision import transforms as T\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.Resize((256, 256)),\n",
    "#     T.ToTensor(),\n",
    "#     # T.Normalize([0.5], [0.5])\n",
    "# ])\n",
    "\n",
    "# dataset = Portrait4dDataset(\n",
    "#     root_dir=\"datasets/portrait4d\", transform=transform\n",
    "# )\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# # Load one subject's poses to verify\n",
    "# sample_subject = dataset.subjects[0]\n",
    "# sample_path = os.path.join(dataset.root_dir, sample_subject, 'poses.npy')\n",
    "# poses = np.load(sample_path)\n",
    "\n",
    "# print(f\"Raw poses shape: {poses.shape}\")\n",
    "\n",
    "# # Reshape if flattened (N, 16) -> (N, 4, 4)\n",
    "# if poses.ndim == 2 and poses.shape[1] == 16:\n",
    "#     poses = poses.reshape(-1, 4, 4)\n",
    "#     print(f\"Reshaped to: {poses.shape}\")\n",
    "\n",
    "# print(f\"Number of views: {poses.shape[0]}\")\n",
    "\n",
    "# # Print one pose matrix to verify it looks like a valid transformation matrix\n",
    "# print(f\"\\nSample pose matrix (view 0):\\n{poses[0]}\")\n",
    "\n",
    "# # Check a few poses\n",
    "# src_idx = 9\n",
    "# for tgt_idx in [0, 5, 10, 15]:\n",
    "#     if tgt_idx < poses.shape[0]:\n",
    "#         src_pose = poses[src_idx]\n",
    "#         tgt_pose = poses[tgt_idx]\n",
    "        \n",
    "#         delta = compute_relative_pose(src_pose, tgt_pose)\n",
    "#         print(f\"\\nSource view {src_idx} -> Target view {tgt_idx}:\")\n",
    "#         print(f\"  delta_pose: {delta}\")\n",
    "#         print(f\"  delta_pitch: {np.degrees(delta[0]):.2f}°\")\n",
    "#         print(f\"  delta_yaw:   {np.degrees(delta[1]):.2f}°\")\n",
    "#         print(f\"  delta_dist:  {delta[2]:.4f}\")\n",
    "\n",
    "# # Verify a sample from dataloader\n",
    "# sample_batch = next(iter(dataloader))\n",
    "# print(f\"\\n--- Sample batch ---\")\n",
    "# print(f\"jpg shape: {sample_batch['jpg'].shape}\")\n",
    "# print(f\"hint shape: {sample_batch['hint'].shape}\")\n",
    "# print(f\"delta_pose shape: {sample_batch['delta_pose'].shape}\")\n",
    "# print(f\"delta_pose sample values (radians): {sample_batch['delta_pose'][0]}\")\n",
    "# print(f\"delta_pose sample values (degrees): pitch={np.degrees(sample_batch['delta_pose'][0, 0].item()):.2f}°, yaw={np.degrees(sample_batch['delta_pose'][0, 1].item()):.2f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c534043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory checkpoints/v2/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 863 M \n",
      "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "346 K     Trainable params\n",
      "1.1 B     Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,282.303 Total estimated model params size (MB)\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight, shape=torch.Size([16, 1280])\n",
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, shape=torch.Size([1280, 16])\n",
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight, shape=torch.Size([16, 1280])\n",
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, shape=torch.Size([1280, 16])\n",
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight, shape=torch.Size([16, 1280])\n",
      "[OPT] LoRA param: base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, shape=torch.Size([1280, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight, shape=torch.Size([16, 320])\n",
      "[OPT] LoRA param: base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight, shape=torch.Size([320, 16])\n",
      "[OPT] Other param: base_model.model.out.0.weight, shape=torch.Size([320])\n",
      "[OPT] Other param: base_model.model.out.0.bias, shape=torch.Size([320])\n",
      "[OPT] Other param: base_model.model.out.2.weight, shape=torch.Size([4, 320, 3, 3])\n",
      "[OPT] Other param: base_model.model.out.2.bias, shape=torch.Size([4])\n",
      "[OPT] pose_net param: base_model.model.pose_net.0.weight, shape=torch.Size([320, 51])\n",
      "[OPT] pose_net param: base_model.model.pose_net.0.bias, shape=torch.Size([320])\n",
      "[OPT] pose_net param: base_model.model.pose_net.2.weight, shape=torch.Size([320, 320])\n",
      "[OPT] pose_net param: base_model.model.pose_net.2.bias, shape=torch.Size([320])\n",
      "\n",
      "[OPT] Summary:\n",
      "  LoRA params: 24\n",
      "  pose_net params: 4\n",
      "  Other trainable params: 4\n",
      "  Total params in optimizer: 32\n",
      "[OPT] pose_net learning rate: 1.0000000000000002e-06 (0.1x of LoRA lr: 1e-05)\n",
      "Epoch 0:   0%|          | 0/500 [00:00<?, ?it/s] [TRAIN] Training step 0, verifying LoRA setup...\n",
      "[TRAIN] Found 108 LoRA modules in forward path\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: mean=1.388656e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: mean=1.247160e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: mean=1.399977e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: mean=1.249710e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: mean=1.401294e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: mean=1.249253e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: mean=2.787191e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: mean=1.251075e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: mean=2.799085e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: mean=1.249810e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: mean=2.790681e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: mean=1.254343e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: mean=2.753513e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: mean=1.254607e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: mean=2.836674e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: mean=1.265909e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: mean=2.840257e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: mean=1.254317e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight: mean=2.757403e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight: mean=1.260176e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight: mean=2.736355e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight: mean=1.262021e-03, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight: mean=2.813244e-02, requires_grad=True\n",
      "[TRAIN] base_model.model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight: mean=1.259528e-03, requires_grad=True\n",
      "[TRAIN] Active adapters: ['default']\n",
      "[DEBUG] Registered forward hook on base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q\n",
      "[DEBUG] Registered backward hook on lora_B.weight\n",
      "get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796223\n",
      "  lora_A(x) mean: 0.431551\n",
      "  lora_B(lora_A(x)) mean: 0.002455\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002455\n",
      "  output mean: 0.462969\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797961\n",
      "  lora_A(x) mean: 0.452861\n",
      "  lora_B(lora_A(x)) mean: 0.002600\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002600\n",
      "  output mean: 0.458521\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798020\n",
      "  lora_A(x) mean: 0.458141\n",
      "  lora_B(lora_A(x)) mean: 0.002625\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002625\n",
      "  output mean: 0.457864\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797759\n",
      "  lora_A(x) mean: 0.460923\n",
      "  lora_B(lora_A(x)) mean: 0.002640\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002640\n",
      "  output mean: 0.457555\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797546\n",
      "  lora_A(x) mean: 0.462062\n",
      "  lora_B(lora_A(x)) mean: 0.002648\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002648\n",
      "  output mean: 0.457372\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797408\n",
      "  lora_A(x) mean: 0.462843\n",
      "  lora_B(lora_A(x)) mean: 0.002652\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002652\n",
      "  output mean: 0.457380\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797313\n",
      "  lora_A(x) mean: 0.463237\n",
      "  lora_B(lora_A(x)) mean: 0.002654\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002654\n",
      "  output mean: 0.457459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797229\n",
      "  lora_A(x) mean: 0.463238\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457570\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797152\n",
      "  lora_A(x) mean: 0.463121\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457696\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797119\n",
      "  lora_A(x) mean: 0.463050\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457773\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797090\n",
      "  lora_A(x) mean: 0.463015\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797064\n",
      "  lora_A(x) mean: 0.462901\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457892\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797046\n",
      "  lora_A(x) mean: 0.462830\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797030\n",
      "  lora_A(x) mean: 0.462742\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457954\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797015\n",
      "  lora_A(x) mean: 0.462697\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.457984\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797009\n",
      "  lora_A(x) mean: 0.462616\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.458014\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796998\n",
      "  lora_A(x) mean: 0.462573\n",
      "  lora_B(lora_A(x)) mean: 0.002655\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002655\n",
      "  output mean: 0.458023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  80%|████████  | 16/20 [00:00<00:00, 28.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796994\n",
      "  lora_A(x) mean: 0.462490\n",
      "  lora_B(lora_A(x)) mean: 0.002654\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002654\n",
      "  output mean: 0.458053\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796989\n",
      "  lora_A(x) mean: 0.462491\n",
      "  lora_B(lora_A(x)) mean: 0.002654\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002654\n",
      "  output mean: 0.458052\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796986\n",
      "  lora_A(x) mean: 0.462469\n",
      "  lora_B(lora_A(x)) mean: 0.002654\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002654\n",
      "  output mean: 0.458066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796985\n",
      "  lora_A(x) mean: 0.462438\n",
      "  lora_B(lora_A(x)) mean: 0.002654\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002654\n",
      "  output mean: 0.458070\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799555\n",
      "  lora_A(x) mean: 0.448457\n",
      "  lora_B(lora_A(x)) mean: 0.002535\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002535\n",
      "  output mean: 0.461092\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799525\n",
      "  lora_A(x) mean: 0.453475\n",
      "  lora_B(lora_A(x)) mean: 0.002565\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002565\n",
      "  output mean: 0.460157\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799465\n",
      "  lora_A(x) mean: 0.456943\n",
      "  lora_B(lora_A(x)) mean: 0.002583\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002583\n",
      "  output mean: 0.459682\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799391\n",
      "  lora_A(x) mean: 0.459227\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.459386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799234\n",
      "  lora_A(x) mean: 0.460518\n",
      "  lora_B(lora_A(x)) mean: 0.002601\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002601\n",
      "  output mean: 0.459227\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799085\n",
      "  lora_A(x) mean: 0.461053\n",
      "  lora_B(lora_A(x)) mean: 0.002605\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002605\n",
      "  output mean: 0.459126\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798958\n",
      "  lora_A(x) mean: 0.461248\n",
      "  lora_B(lora_A(x)) mean: 0.002607\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002607\n",
      "  output mean: 0.459063\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  input mean: 0.798885\n",
      "  lora_A(x) mean: 0.461386\n",
      "  lora_B(lora_A(x)) mean: 0.002609\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002609\n",
      "  output mean: 0.458997\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798855\n",
      "  lora_A(x) mean: 0.461429\n",
      "  lora_B(lora_A(x)) mean: 0.002609\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002609\n",
      "  output mean: 0.458940\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798845\n",
      "  lora_A(x) mean: 0.461382\n",
      "  lora_B(lora_A(x)) mean: 0.002610\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002610\n",
      "  output mean: 0.458896\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798853\n",
      "  lora_A(x) mean: 0.461358\n",
      "  lora_B(lora_A(x)) mean: 0.002610\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002610\n",
      "  output mean: 0.458876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798854\n",
      "  lora_A(x) mean: 0.461303\n",
      "  lora_B(lora_A(x)) mean: 0.002610\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002610\n",
      "  output mean: 0.458853\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798845\n",
      "  lora_A(x) mean: 0.461204\n",
      "  lora_B(lora_A(x)) mean: 0.002610\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002610\n",
      "  output mean: 0.458853\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798845\n",
      "  lora_A(x) mean: 0.461133\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798848\n",
      "  lora_A(x) mean: 0.461095\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458831\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798845\n",
      "  lora_A(x) mean: 0.461016\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458818\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798843\n",
      "  lora_A(x) mean: 0.460968\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458819\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798840\n",
      "  lora_A(x) mean: 0.460976\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 30.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798838\n",
      "  lora_A(x) mean: 0.460917\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458805\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798840\n",
      "  lora_A(x) mean: 0.460903\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.458810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798120\n",
      "  lora_A(x) mean: 0.481890\n",
      "  lora_B(lora_A(x)) mean: 0.002723\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002723\n",
      "  output mean: 0.464044\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798938\n",
      "  lora_A(x) mean: 0.487289\n",
      "  lora_B(lora_A(x)) mean: 0.002755\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002755\n",
      "  output mean: 0.462943\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799362\n",
      "  lora_A(x) mean: 0.490159\n",
      "  lora_B(lora_A(x)) mean: 0.002772\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002772\n",
      "  output mean: 0.462256\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799711\n",
      "  lora_A(x) mean: 0.490887\n",
      "  lora_B(lora_A(x)) mean: 0.002781\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002781\n",
      "  output mean: 0.461903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799821\n",
      "  lora_A(x) mean: 0.491134\n",
      "  lora_B(lora_A(x)) mean: 0.002785\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002785\n",
      "  output mean: 0.461642\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799885\n",
      "  lora_A(x) mean: 0.490772\n",
      "  lora_B(lora_A(x)) mean: 0.002786\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002786\n",
      "  output mean: 0.461458\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799888\n",
      "  lora_A(x) mean: 0.490285\n",
      "  lora_B(lora_A(x)) mean: 0.002786\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002786\n",
      "  output mean: 0.461280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799895\n",
      "  lora_A(x) mean: 0.489665\n",
      "  lora_B(lora_A(x)) mean: 0.002785\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002785\n",
      "  output mean: 0.461160\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799903\n",
      "  lora_A(x) mean: 0.489069\n",
      "  lora_B(lora_A(x)) mean: 0.002784\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002784\n",
      "  output mean: 0.461066\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799911\n",
      "  lora_A(x) mean: 0.488529\n",
      "  lora_B(lora_A(x)) mean: 0.002783\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002783\n",
      "  output mean: 0.460997\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799902\n",
      "  lora_A(x) mean: 0.488136\n",
      "  lora_B(lora_A(x)) mean: 0.002783\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002783\n",
      "  output mean: 0.460966\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799899\n",
      "  lora_A(x) mean: 0.487780\n",
      "  lora_B(lora_A(x)) mean: 0.002782\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002782\n",
      "  output mean: 0.460930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799883\n",
      "  lora_A(x) mean: 0.487604\n",
      "  lora_B(lora_A(x)) mean: 0.002781\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002781\n",
      "  output mean: 0.460899\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799873\n",
      "  lora_A(x) mean: 0.487434\n",
      "  lora_B(lora_A(x)) mean: 0.002781\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002781\n",
      "  output mean: 0.460887\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799874\n",
      "  lora_A(x) mean: 0.487328\n",
      "  lora_B(lora_A(x)) mean: 0.002780\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002780\n",
      "  output mean: 0.460857\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799868\n",
      "  lora_A(x) mean: 0.487195\n",
      "  lora_B(lora_A(x)) mean: 0.002780\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002780\n",
      "  output mean: 0.460844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799863\n",
      "  lora_A(x) mean: 0.487060\n",
      "  lora_B(lora_A(x)) mean: 0.002779\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002779\n",
      "  output mean: 0.460846\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799864\n",
      "  lora_A(x) mean: 0.487019\n",
      "  lora_B(lora_A(x)) mean: 0.002779\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002779\n",
      "  output mean: 0.460836\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799858\n",
      "  lora_A(x) mean: 0.486950\n",
      "  lora_B(lora_A(x)) mean: 0.002779\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002779\n",
      "  output mean: 0.460843\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799852\n",
      "  lora_A(x) mean: 0.486855\n",
      "  lora_B(lora_A(x)) mean: 0.002779\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002779\n",
      "  output mean: 0.460830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 30.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799570\n",
      "  lora_A(x) mean: 0.400902\n",
      "  lora_B(lora_A(x)) mean: 0.002302\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002302\n",
      "  output mean: 0.461306\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799974\n",
      "  lora_A(x) mean: 0.406218\n",
      "  lora_B(lora_A(x)) mean: 0.002338\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002338\n",
      "  output mean: 0.460640\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800188\n",
      "  lora_A(x) mean: 0.408562\n",
      "  lora_B(lora_A(x)) mean: 0.002363\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002363\n",
      "  output mean: 0.460240\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800349\n",
      "  lora_A(x) mean: 0.409728\n",
      "  lora_B(lora_A(x)) mean: 0.002381\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002381\n",
      "  output mean: 0.459946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800340\n",
      "  lora_A(x) mean: 0.410236\n",
      "  lora_B(lora_A(x)) mean: 0.002394\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002394\n",
      "  output mean: 0.459749\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800262\n",
      "  lora_A(x) mean: 0.410512\n",
      "  lora_B(lora_A(x)) mean: 0.002402\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002402\n",
      "  output mean: 0.459604\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800244\n",
      "  lora_A(x) mean: 0.410727\n",
      "  lora_B(lora_A(x)) mean: 0.002409\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002409\n",
      "  output mean: 0.459501\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800248\n",
      "  lora_A(x) mean: 0.410813\n",
      "  lora_B(lora_A(x)) mean: 0.002413\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002413\n",
      "  output mean: 0.459473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800247\n",
      "  lora_A(x) mean: 0.410921\n",
      "  lora_B(lora_A(x)) mean: 0.002416\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002416\n",
      "  output mean: 0.459414\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800239\n",
      "  lora_A(x) mean: 0.411053\n",
      "  lora_B(lora_A(x)) mean: 0.002419\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002419\n",
      "  output mean: 0.459408\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800222\n",
      "  lora_A(x) mean: 0.411086\n",
      "  lora_B(lora_A(x)) mean: 0.002421\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002421\n",
      "  output mean: 0.459395\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800216\n",
      "  lora_A(x) mean: 0.411114\n",
      "  lora_B(lora_A(x)) mean: 0.002423\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002423\n",
      "  output mean: 0.459387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800213\n",
      "  lora_A(x) mean: 0.411108\n",
      "  lora_B(lora_A(x)) mean: 0.002424\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002424\n",
      "  output mean: 0.459404\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800213\n",
      "  lora_A(x) mean: 0.411098\n",
      "  lora_B(lora_A(x)) mean: 0.002425\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002425\n",
      "  output mean: 0.459398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800212\n",
      "  lora_A(x) mean: 0.411091\n",
      "  lora_B(lora_A(x)) mean: 0.002426\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002426\n",
      "  output mean: 0.459394\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800215\n",
      "  lora_A(x) mean: 0.411083\n",
      "  lora_B(lora_A(x)) mean: 0.002427\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002427\n",
      "  output mean: 0.459392\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800211\n",
      "  lora_A(x) mean: 0.411062\n",
      "  lora_B(lora_A(x)) mean: 0.002427\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002427\n",
      "  output mean: 0.459390\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800213\n",
      "  lora_A(x) mean: 0.411030\n",
      "  lora_B(lora_A(x)) mean: 0.002427\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002427\n",
      "  output mean: 0.459395\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800215\n",
      "  lora_A(x) mean: 0.411060\n",
      "  lora_B(lora_A(x)) mean: 0.002428\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002428\n",
      "  output mean: 0.459389\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800212\n",
      "  lora_A(x) mean: 0.411045\n",
      "  lora_B(lora_A(x)) mean: 0.002428\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002428\n",
      "  output mean: 0.459388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 29.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VIS] Logged multiview predictions at step 0\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 0\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:   0%|          | 1/500 [00:04<41:11,  4.95s/it, loss=4.03, v_num=179, train_loss=4.030]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799183\n",
      "  lora_A(x) mean: 0.447990\n",
      "  lora_B(lora_A(x)) mean: 0.002528\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002528\n",
      "  output mean: 0.463480\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   0%|          | 2/500 [00:05<21:39,  2.61s/it, loss=4.05, v_num=179, train_loss=4.080]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798715\n",
      "  lora_A(x) mean: 0.453005\n",
      "  lora_B(lora_A(x)) mean: 0.002591\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002591\n",
      "  output mean: 0.462541\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   1%|          | 3/500 [00:05<15:04,  1.82s/it, loss=4.02, v_num=179, train_loss=3.960]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797051\n",
      "  lora_A(x) mean: 0.438011\n",
      "  lora_B(lora_A(x)) mean: 0.002507\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002507\n",
      "  output mean: 0.460699\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   1%|          | 4/500 [00:05<11:46,  1.42s/it, loss=4.01, v_num=179, train_loss=3.990]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798528\n",
      "  lora_A(x) mean: 0.457587\n",
      "  lora_B(lora_A(x)) mean: 0.002588\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002588\n",
      "  output mean: 0.461848\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   1%|          | 5/500 [00:05<09:47,  1.19s/it, loss=3.99, v_num=179, train_loss=3.890]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797954\n",
      "  lora_A(x) mean: 0.432680\n",
      "  lora_B(lora_A(x)) mean: 0.002501\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002501\n",
      "  output mean: 0.461523\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   1%|          | 6/500 [00:06<08:28,  1.03s/it, loss=3.98, v_num=179, train_loss=3.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798158\n",
      "  lora_A(x) mean: 0.438999\n",
      "  lora_B(lora_A(x)) mean: 0.002523\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002523\n",
      "  output mean: 0.461914\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   1%|▏         | 7/500 [00:06<07:31,  1.09it/s, loss=3.97, v_num=179, train_loss=3.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799211\n",
      "  lora_A(x) mean: 0.445044\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.461938\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   2%|▏         | 8/500 [00:06<06:48,  1.20it/s, loss=3.96, v_num=179, train_loss=3.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799022\n",
      "  lora_A(x) mean: 0.434247\n",
      "  lora_B(lora_A(x)) mean: 0.002444\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002444\n",
      "  output mean: 0.464747\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   2%|▏         | 9/500 [00:06<06:15,  1.31it/s, loss=3.95, v_num=179, train_loss=3.900]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798135\n",
      "  lora_A(x) mean: 0.429638\n",
      "  lora_B(lora_A(x)) mean: 0.002479\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002479\n",
      "  output mean: 0.462145\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   2%|▏         | 10/500 [00:07<05:48,  1.41it/s, loss=3.95, v_num=179, train_loss=3.960]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798234\n",
      "  lora_A(x) mean: 0.434415\n",
      "  lora_B(lora_A(x)) mean: 0.002452\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002452\n",
      "  output mean: 0.462025\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   2%|▏         | 11/500 [00:07<05:26,  1.50it/s, loss=3.94, v_num=179, train_loss=3.830]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798233\n",
      "  lora_A(x) mean: 0.448345\n",
      "  lora_B(lora_A(x)) mean: 0.002524\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002524\n",
      "  output mean: 0.461209\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   2%|▏         | 12/500 [00:07<05:08,  1.58it/s, loss=3.94, v_num=179, train_loss=3.900]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799353\n",
      "  lora_A(x) mean: 0.459367\n",
      "  lora_B(lora_A(x)) mean: 0.002608\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002608\n",
      "  output mean: 0.462012\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   3%|▎         | 13/500 [00:07<04:52,  1.66it/s, loss=3.93, v_num=179, train_loss=3.820]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799146\n",
      "  lora_A(x) mean: 0.444634\n",
      "  lora_B(lora_A(x)) mean: 0.002499\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002499\n",
      "  output mean: 0.464480\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   3%|▎         | 14/500 [00:08<04:39,  1.74it/s, loss=3.92, v_num=179, train_loss=3.860]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798175\n",
      "  lora_A(x) mean: 0.450774\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.463371\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   3%|▎         | 15/500 [00:08<04:27,  1.81it/s, loss=3.92, v_num=179, train_loss=3.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798158\n",
      "  lora_A(x) mean: 0.434388\n",
      "  lora_B(lora_A(x)) mean: 0.002498\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002498\n",
      "  output mean: 0.462652\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   3%|▎         | 16/500 [00:08<04:17,  1.88it/s, loss=3.91, v_num=179, train_loss=3.800]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798103\n",
      "  lora_A(x) mean: 0.441356\n",
      "  lora_B(lora_A(x)) mean: 0.002474\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002474\n",
      "  output mean: 0.463419\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   3%|▎         | 17/500 [00:08<04:08,  1.94it/s, loss=3.91, v_num=179, train_loss=3.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798531\n",
      "  lora_A(x) mean: 0.449107\n",
      "  lora_B(lora_A(x)) mean: 0.002592\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002592\n",
      "  output mean: 0.463326\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   4%|▎         | 18/500 [00:08<04:00,  2.00it/s, loss=3.91, v_num=179, train_loss=3.860]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798775\n",
      "  lora_A(x) mean: 0.450441\n",
      "  lora_B(lora_A(x)) mean: 0.002598\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002598\n",
      "  output mean: 0.460605\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   4%|▍         | 19/500 [00:09<03:53,  2.06it/s, loss=3.9, v_num=179, train_loss=3.760] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799696\n",
      "  lora_A(x) mean: 0.430826\n",
      "  lora_B(lora_A(x)) mean: 0.002469\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002469\n",
      "  output mean: 0.461886\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   4%|▍         | 20/500 [00:09<03:47,  2.11it/s, loss=3.9, v_num=179, train_loss=3.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797277\n",
      "  lora_A(x) mean: 0.455543\n",
      "  lora_B(lora_A(x)) mean: 0.002580\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002580\n",
      "  output mean: 0.462352\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   4%|▍         | 21/500 [00:09<03:41,  2.16it/s, loss=3.88, v_num=179, train_loss=3.790]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798318\n",
      "  lora_A(x) mean: 0.446926\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.463626\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   4%|▍         | 22/500 [00:09<03:35,  2.22it/s, loss=3.87, v_num=179, train_loss=3.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798868\n",
      "  lora_A(x) mean: 0.452786\n",
      "  lora_B(lora_A(x)) mean: 0.002546\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002546\n",
      "  output mean: 0.462581\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   5%|▍         | 23/500 [00:10<03:30,  2.26it/s, loss=3.86, v_num=179, train_loss=3.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797701\n",
      "  lora_A(x) mean: 0.426943\n",
      "  lora_B(lora_A(x)) mean: 0.002447\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002447\n",
      "  output mean: 0.461431\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   5%|▍         | 24/500 [00:10<03:26,  2.31it/s, loss=3.85, v_num=179, train_loss=3.780]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797704\n",
      "  lora_A(x) mean: 0.452339\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.462758\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   5%|▌         | 25/500 [00:10<03:22,  2.35it/s, loss=3.83, v_num=179, train_loss=3.640]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798271\n",
      "  lora_A(x) mean: 0.438042\n",
      "  lora_B(lora_A(x)) mean: 0.002480\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002480\n",
      "  output mean: 0.461258\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   5%|▌         | 26/500 [00:10<03:18,  2.39it/s, loss=3.83, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799609\n",
      "  lora_A(x) mean: 0.446171\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.461852\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   5%|▌         | 27/500 [00:11<03:14,  2.43it/s, loss=3.82, v_num=179, train_loss=3.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798253\n",
      "  lora_A(x) mean: 0.454952\n",
      "  lora_B(lora_A(x)) mean: 0.002565\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002565\n",
      "  output mean: 0.463656\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   6%|▌         | 28/500 [00:11<03:11,  2.47it/s, loss=3.81, v_num=179, train_loss=3.820]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799533\n",
      "  lora_A(x) mean: 0.437702\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.461666\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   6%|▌         | 29/500 [00:11<03:07,  2.51it/s, loss=3.8, v_num=179, train_loss=3.620] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798262\n",
      "  lora_A(x) mean: 0.446703\n",
      "  lora_B(lora_A(x)) mean: 0.002563\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002563\n",
      "  output mean: 0.463138\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   6%|▌         | 30/500 [00:11<03:04,  2.54it/s, loss=3.79, v_num=179, train_loss=3.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798249\n",
      "  lora_A(x) mean: 0.443165\n",
      "  lora_B(lora_A(x)) mean: 0.002468\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002468\n",
      "  output mean: 0.463080\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   6%|▌         | 31/500 [00:12<03:02,  2.57it/s, loss=3.79, v_num=179, train_loss=3.770]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798038\n",
      "  lora_A(x) mean: 0.434255\n",
      "  lora_B(lora_A(x)) mean: 0.002477\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002477\n",
      "  output mean: 0.462345\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   6%|▋         | 32/500 [00:12<02:59,  2.61it/s, loss=3.77, v_num=179, train_loss=3.600]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797764\n",
      "  lora_A(x) mean: 0.447783\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.461880\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   7%|▋         | 33/500 [00:12<02:56,  2.64it/s, loss=3.76, v_num=179, train_loss=3.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799521\n",
      "  lora_A(x) mean: 0.448578\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.464012\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   7%|▋         | 34/500 [00:12<02:54,  2.67it/s, loss=3.75, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797623\n",
      "  lora_A(x) mean: 0.443795\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.465438\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   7%|▋         | 35/500 [00:12<02:52,  2.70it/s, loss=3.74, v_num=179, train_loss=3.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797450\n",
      "  lora_A(x) mean: 0.437370\n",
      "  lora_B(lora_A(x)) mean: 0.002484\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002484\n",
      "  output mean: 0.463472\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   7%|▋         | 36/500 [00:13<02:50,  2.73it/s, loss=3.73, v_num=179, train_loss=3.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798135\n",
      "  lora_A(x) mean: 0.422903\n",
      "  lora_B(lora_A(x)) mean: 0.002420\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002420\n",
      "  output mean: 0.461443\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   7%|▋         | 37/500 [00:13<02:48,  2.75it/s, loss=3.72, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798888\n",
      "  lora_A(x) mean: 0.434651\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.463904\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   8%|▊         | 38/500 [00:13<02:46,  2.78it/s, loss=3.72, v_num=179, train_loss=3.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798915\n",
      "  lora_A(x) mean: 0.463166\n",
      "  lora_B(lora_A(x)) mean: 0.002608\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002608\n",
      "  output mean: 0.465105\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   8%|▊         | 39/500 [00:13<02:44,  2.80it/s, loss=3.72, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797065\n",
      "  lora_A(x) mean: 0.447134\n",
      "  lora_B(lora_A(x)) mean: 0.002520\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002520\n",
      "  output mean: 0.462892\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   8%|▊         | 40/500 [00:14<02:42,  2.83it/s, loss=3.7, v_num=179, train_loss=3.560] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800034\n",
      "  lora_A(x) mean: 0.446817\n",
      "  lora_B(lora_A(x)) mean: 0.002504\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002504\n",
      "  output mean: 0.463997\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   8%|▊         | 41/500 [00:14<02:41,  2.85it/s, loss=3.69, v_num=179, train_loss=3.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799056\n",
      "  lora_A(x) mean: 0.428993\n",
      "  lora_B(lora_A(x)) mean: 0.002456\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002456\n",
      "  output mean: 0.462384\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   8%|▊         | 42/500 [00:14<02:39,  2.87it/s, loss=3.69, v_num=179, train_loss=3.790]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798268\n",
      "  lora_A(x) mean: 0.433004\n",
      "  lora_B(lora_A(x)) mean: 0.002462\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002462\n",
      "  output mean: 0.462666\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   9%|▊         | 43/500 [00:14<02:37,  2.89it/s, loss=3.68, v_num=179, train_loss=3.560]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797979\n",
      "  lora_A(x) mean: 0.450218\n",
      "  lora_B(lora_A(x)) mean: 0.002539\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002539\n",
      "  output mean: 0.461288\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   9%|▉         | 44/500 [00:15<02:36,  2.92it/s, loss=3.67, v_num=179, train_loss=3.470]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797690\n",
      "  lora_A(x) mean: 0.442841\n",
      "  lora_B(lora_A(x)) mean: 0.002528\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002528\n",
      "  output mean: 0.461196\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   9%|▉         | 45/500 [00:15<02:34,  2.94it/s, loss=3.66, v_num=179, train_loss=3.480]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798329\n",
      "  lora_A(x) mean: 0.453849\n",
      "  lora_B(lora_A(x)) mean: 0.002608\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002608\n",
      "  output mean: 0.461641\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   9%|▉         | 46/500 [00:15<02:33,  2.96it/s, loss=3.65, v_num=179, train_loss=3.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796989\n",
      "  lora_A(x) mean: 0.438595\n",
      "  lora_B(lora_A(x)) mean: 0.002493\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002493\n",
      "  output mean: 0.463076\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:   9%|▉         | 47/500 [00:15<02:32,  2.98it/s, loss=3.64, v_num=179, train_loss=3.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798411\n",
      "  lora_A(x) mean: 0.456484\n",
      "  lora_B(lora_A(x)) mean: 0.002571\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002571\n",
      "  output mean: 0.464541\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  10%|▉         | 48/500 [00:15<02:30,  3.00it/s, loss=3.62, v_num=179, train_loss=3.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798809\n",
      "  lora_A(x) mean: 0.456355\n",
      "  lora_B(lora_A(x)) mean: 0.002571\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002571\n",
      "  output mean: 0.463270\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  10%|▉         | 49/500 [00:16<02:29,  3.02it/s, loss=3.62, v_num=179, train_loss=3.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799462\n",
      "  lora_A(x) mean: 0.444798\n",
      "  lora_B(lora_A(x)) mean: 0.002493\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002493\n",
      "  output mean: 0.463242\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  10%|█         | 50/500 [00:16<02:28,  3.04it/s, loss=3.61, v_num=179, train_loss=3.470]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797327\n",
      "  lora_A(x) mean: 0.445838\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.463964\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 50\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  10%|█         | 51/500 [00:16<02:27,  3.05it/s, loss=3.6, v_num=179, train_loss=3.580] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799845\n",
      "  lora_A(x) mean: 0.453789\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.464666\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  10%|█         | 52/500 [00:16<02:25,  3.07it/s, loss=3.6, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797396\n",
      "  lora_A(x) mean: 0.429512\n",
      "  lora_B(lora_A(x)) mean: 0.002446\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002446\n",
      "  output mean: 0.464600\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  11%|█         | 53/500 [00:17<02:24,  3.09it/s, loss=3.59, v_num=179, train_loss=3.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798155\n",
      "  lora_A(x) mean: 0.454812\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.463404\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  11%|█         | 54/500 [00:17<02:23,  3.10it/s, loss=3.59, v_num=179, train_loss=3.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797832\n",
      "  lora_A(x) mean: 0.436068\n",
      "  lora_B(lora_A(x)) mean: 0.002484\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002484\n",
      "  output mean: 0.461536\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  11%|█         | 55/500 [00:17<02:22,  3.12it/s, loss=3.57, v_num=179, train_loss=3.370]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797028\n",
      "  lora_A(x) mean: 0.425229\n",
      "  lora_B(lora_A(x)) mean: 0.002448\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002448\n",
      "  output mean: 0.463088\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  11%|█         | 56/500 [00:17<02:21,  3.13it/s, loss=3.56, v_num=179, train_loss=3.350]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798141\n",
      "  lora_A(x) mean: 0.437859\n",
      "  lora_B(lora_A(x)) mean: 0.002484\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002484\n",
      "  output mean: 0.462318\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  11%|█▏        | 57/500 [00:18<02:20,  3.15it/s, loss=3.54, v_num=179, train_loss=3.280]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798525\n",
      "  lora_A(x) mean: 0.440430\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.463696\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  12%|█▏        | 58/500 [00:18<02:19,  3.16it/s, loss=3.52, v_num=179, train_loss=3.480]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799085\n",
      "  lora_A(x) mean: 0.454834\n",
      "  lora_B(lora_A(x)) mean: 0.002557\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002557\n",
      "  output mean: 0.463271\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  12%|█▏        | 59/500 [00:18<02:18,  3.18it/s, loss=3.51, v_num=179, train_loss=3.560]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797631\n",
      "  lora_A(x) mean: 0.451998\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.463179\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  12%|█▏        | 60/500 [00:18<02:17,  3.19it/s, loss=3.5, v_num=179, train_loss=3.220] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798771\n",
      "  lora_A(x) mean: 0.430690\n",
      "  lora_B(lora_A(x)) mean: 0.002470\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002470\n",
      "  output mean: 0.464299\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  12%|█▏        | 61/500 [00:19<02:16,  3.20it/s, loss=3.49, v_num=179, train_loss=3.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797459\n",
      "  lora_A(x) mean: 0.438122\n",
      "  lora_B(lora_A(x)) mean: 0.002469\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002469\n",
      "  output mean: 0.461870\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  12%|█▏        | 62/500 [00:19<02:16,  3.22it/s, loss=3.47, v_num=179, train_loss=3.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796559\n",
      "  lora_A(x) mean: 0.451076\n",
      "  lora_B(lora_A(x)) mean: 0.002544\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002544\n",
      "  output mean: 0.462370\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  13%|█▎        | 63/500 [00:19<02:15,  3.23it/s, loss=3.46, v_num=179, train_loss=3.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797959\n",
      "  lora_A(x) mean: 0.444686\n",
      "  lora_B(lora_A(x)) mean: 0.002525\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002525\n",
      "  output mean: 0.462403\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  13%|█▎        | 64/500 [00:19<02:14,  3.24it/s, loss=3.45, v_num=179, train_loss=3.300]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798575\n",
      "  lora_A(x) mean: 0.440198\n",
      "  lora_B(lora_A(x)) mean: 0.002541\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002541\n",
      "  output mean: 0.463783\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  13%|█▎        | 65/500 [00:20<02:13,  3.25it/s, loss=3.45, v_num=179, train_loss=3.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799429\n",
      "  lora_A(x) mean: 0.468087\n",
      "  lora_B(lora_A(x)) mean: 0.002631\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002631\n",
      "  output mean: 0.464144\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  13%|█▎        | 66/500 [00:20<02:13,  3.26it/s, loss=3.46, v_num=179, train_loss=3.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797833\n",
      "  lora_A(x) mean: 0.455906\n",
      "  lora_B(lora_A(x)) mean: 0.002587\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002587\n",
      "  output mean: 0.464084\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  13%|█▎        | 67/500 [00:20<02:12,  3.27it/s, loss=3.45, v_num=179, train_loss=3.310]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800115\n",
      "  lora_A(x) mean: 0.456429\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.462410\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  14%|█▎        | 68/500 [00:20<02:11,  3.28it/s, loss=3.44, v_num=179, train_loss=3.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798049\n",
      "  lora_A(x) mean: 0.442614\n",
      "  lora_B(lora_A(x)) mean: 0.002524\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002524\n",
      "  output mean: 0.463598\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  14%|█▍        | 69/500 [00:20<02:10,  3.30it/s, loss=3.43, v_num=179, train_loss=3.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799950\n",
      "  lora_A(x) mean: 0.455581\n",
      "  lora_B(lora_A(x)) mean: 0.002548\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002548\n",
      "  output mean: 0.461355\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  14%|█▍        | 70/500 [00:21<02:10,  3.31it/s, loss=3.44, v_num=179, train_loss=3.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800204\n",
      "  lora_A(x) mean: 0.445843\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.461049\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  14%|█▍        | 71/500 [00:21<02:09,  3.32it/s, loss=3.43, v_num=179, train_loss=3.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798397\n",
      "  lora_A(x) mean: 0.457778\n",
      "  lora_B(lora_A(x)) mean: 0.002614\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002614\n",
      "  output mean: 0.459423\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  14%|█▍        | 72/500 [00:21<02:08,  3.33it/s, loss=3.41, v_num=179, train_loss=3.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797750\n",
      "  lora_A(x) mean: 0.433676\n",
      "  lora_B(lora_A(x)) mean: 0.002465\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002465\n",
      "  output mean: 0.462568\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  15%|█▍        | 73/500 [00:21<02:07,  3.34it/s, loss=3.4, v_num=179, train_loss=3.120] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797908\n",
      "  lora_A(x) mean: 0.461891\n",
      "  lora_B(lora_A(x)) mean: 0.002593\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002593\n",
      "  output mean: 0.464360\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  15%|█▍        | 74/500 [00:22<02:07,  3.35it/s, loss=3.39, v_num=179, train_loss=3.380]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799300\n",
      "  lora_A(x) mean: 0.441941\n",
      "  lora_B(lora_A(x)) mean: 0.002513\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002513\n",
      "  output mean: 0.461215\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  15%|█▌        | 75/500 [00:22<02:06,  3.36it/s, loss=3.38, v_num=179, train_loss=3.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797552\n",
      "  lora_A(x) mean: 0.439208\n",
      "  lora_B(lora_A(x)) mean: 0.002511\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002511\n",
      "  output mean: 0.461312\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  15%|█▌        | 76/500 [00:22<02:05,  3.37it/s, loss=3.36, v_num=179, train_loss=3.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798493\n",
      "  lora_A(x) mean: 0.432799\n",
      "  lora_B(lora_A(x)) mean: 0.002475\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002475\n",
      "  output mean: 0.459487\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  15%|█▌        | 77/500 [00:22<02:05,  3.38it/s, loss=3.37, v_num=179, train_loss=3.320]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797785\n",
      "  lora_A(x) mean: 0.454792\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.462041\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  16%|█▌        | 78/500 [00:23<02:04,  3.39it/s, loss=3.35, v_num=179, train_loss=3.200]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799252\n",
      "  lora_A(x) mean: 0.470707\n",
      "  lora_B(lora_A(x)) mean: 0.002652\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002652\n",
      "  output mean: 0.462666\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  16%|█▌        | 79/500 [00:23<02:03,  3.40it/s, loss=3.34, v_num=179, train_loss=3.340]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799330\n",
      "  lora_A(x) mean: 0.449487\n",
      "  lora_B(lora_A(x)) mean: 0.002578\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002578\n",
      "  output mean: 0.462495\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  16%|█▌        | 80/500 [00:23<02:03,  3.41it/s, loss=3.34, v_num=179, train_loss=3.130]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798187\n",
      "  lora_A(x) mean: 0.433518\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.464581\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  16%|█▌        | 81/500 [00:23<02:02,  3.41it/s, loss=3.33, v_num=179, train_loss=3.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798340\n",
      "  lora_A(x) mean: 0.455930\n",
      "  lora_B(lora_A(x)) mean: 0.002576\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002576\n",
      "  output mean: 0.461893\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  16%|█▋        | 82/500 [00:23<02:02,  3.42it/s, loss=3.33, v_num=179, train_loss=3.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799041\n",
      "  lora_A(x) mean: 0.452551\n",
      "  lora_B(lora_A(x)) mean: 0.002597\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002597\n",
      "  output mean: 0.461346\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  17%|█▋        | 83/500 [00:24<02:01,  3.43it/s, loss=3.32, v_num=179, train_loss=3.160]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798690\n",
      "  lora_A(x) mean: 0.458923\n",
      "  lora_B(lora_A(x)) mean: 0.002593\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002593\n",
      "  output mean: 0.465113\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  17%|█▋        | 84/500 [00:24<02:00,  3.44it/s, loss=3.32, v_num=179, train_loss=3.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798108\n",
      "  lora_A(x) mean: 0.450122\n",
      "  lora_B(lora_A(x)) mean: 0.002553\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002553\n",
      "  output mean: 0.461526\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  17%|█▋        | 85/500 [00:24<02:00,  3.45it/s, loss=3.31, v_num=179, train_loss=3.220]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800169\n",
      "  lora_A(x) mean: 0.424495\n",
      "  lora_B(lora_A(x)) mean: 0.002400\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002400\n",
      "  output mean: 0.463313\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  17%|█▋        | 86/500 [00:24<01:59,  3.46it/s, loss=3.28, v_num=179, train_loss=3.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798027\n",
      "  lora_A(x) mean: 0.448845\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.462139\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  17%|█▋        | 87/500 [00:25<01:59,  3.47it/s, loss=3.28, v_num=179, train_loss=3.230]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798124\n",
      "  lora_A(x) mean: 0.447613\n",
      "  lora_B(lora_A(x)) mean: 0.002534\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002534\n",
      "  output mean: 0.461739\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  18%|█▊        | 88/500 [00:25<01:58,  3.48it/s, loss=3.27, v_num=179, train_loss=3.300]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798564\n",
      "  lora_A(x) mean: 0.469158\n",
      "  lora_B(lora_A(x)) mean: 0.002639\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002639\n",
      "  output mean: 0.461154\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  18%|█▊        | 89/500 [00:25<01:57,  3.48it/s, loss=3.26, v_num=179, train_loss=3.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797123\n",
      "  lora_A(x) mean: 0.434392\n",
      "  lora_B(lora_A(x)) mean: 0.002498\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002498\n",
      "  output mean: 0.461608\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  18%|█▊        | 90/500 [00:25<01:57,  3.49it/s, loss=3.22, v_num=179, train_loss=2.900]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798833\n",
      "  lora_A(x) mean: 0.426572\n",
      "  lora_B(lora_A(x)) mean: 0.002438\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002438\n",
      "  output mean: 0.462666\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  18%|█▊        | 91/500 [00:26<01:56,  3.50it/s, loss=3.2, v_num=179, train_loss=3.050] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798031\n",
      "  lora_A(x) mean: 0.436735\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.464327\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  18%|█▊        | 92/500 [00:26<01:56,  3.51it/s, loss=3.18, v_num=179, train_loss=2.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799072\n",
      "  lora_A(x) mean: 0.448574\n",
      "  lora_B(lora_A(x)) mean: 0.002534\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002534\n",
      "  output mean: 0.464422\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  19%|█▊        | 93/500 [00:26<01:55,  3.51it/s, loss=3.19, v_num=179, train_loss=3.310]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799034\n",
      "  lora_A(x) mean: 0.448222\n",
      "  lora_B(lora_A(x)) mean: 0.002514\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002514\n",
      "  output mean: 0.461378\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  19%|█▉        | 94/500 [00:26<01:55,  3.52it/s, loss=3.18, v_num=179, train_loss=3.130]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798228\n",
      "  lora_A(x) mean: 0.435177\n",
      "  lora_B(lora_A(x)) mean: 0.002482\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002482\n",
      "  output mean: 0.461902\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  19%|█▉        | 95/500 [00:26<01:54,  3.52it/s, loss=3.17, v_num=179, train_loss=2.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796056\n",
      "  lora_A(x) mean: 0.452090\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.465116\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  19%|█▉        | 96/500 [00:27<01:54,  3.53it/s, loss=3.18, v_num=179, train_loss=3.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798096\n",
      "  lora_A(x) mean: 0.425707\n",
      "  lora_B(lora_A(x)) mean: 0.002430\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002430\n",
      "  output mean: 0.463244\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  19%|█▉        | 97/500 [00:27<01:53,  3.54it/s, loss=3.18, v_num=179, train_loss=3.230]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798249\n",
      "  lora_A(x) mean: 0.445945\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.461739\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  20%|█▉        | 98/500 [00:27<01:53,  3.55it/s, loss=3.18, v_num=179, train_loss=3.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799320\n",
      "  lora_A(x) mean: 0.435084\n",
      "  lora_B(lora_A(x)) mean: 0.002467\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002467\n",
      "  output mean: 0.460457\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  20%|█▉        | 99/500 [00:27<01:52,  3.56it/s, loss=3.15, v_num=179, train_loss=2.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798155\n",
      "  lora_A(x) mean: 0.453170\n",
      "  lora_B(lora_A(x)) mean: 0.002567\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002567\n",
      "  output mean: 0.463045\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  20%|██        | 100/500 [00:28<01:52,  3.56it/s, loss=3.14, v_num=179, train_loss=2.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798711\n",
      "  lora_A(x) mean: 0.438840\n",
      "  lora_B(lora_A(x)) mean: 0.002525\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002525\n",
      "  output mean: 0.461768\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 100\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  20%|██        | 101/500 [00:28<01:51,  3.57it/s, loss=3.14, v_num=179, train_loss=3.250]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797667\n",
      "  lora_A(x) mean: 0.462975\n",
      "  lora_B(lora_A(x)) mean: 0.002618\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002618\n",
      "  output mean: 0.461051\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  20%|██        | 102/500 [00:28<01:51,  3.57it/s, loss=3.12, v_num=179, train_loss=2.800]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798623\n",
      "  lora_A(x) mean: 0.433946\n",
      "  lora_B(lora_A(x)) mean: 0.002491\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002491\n",
      "  output mean: 0.462792\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  21%|██        | 103/500 [00:28<01:50,  3.58it/s, loss=3.1, v_num=179, train_loss=2.790] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798392\n",
      "  lora_A(x) mean: 0.452339\n",
      "  lora_B(lora_A(x)) mean: 0.002547\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002547\n",
      "  output mean: 0.462245\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  21%|██        | 104/500 [00:28<01:50,  3.59it/s, loss=3.07, v_num=179, train_loss=2.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798460\n",
      "  lora_A(x) mean: 0.436551\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.462024\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  21%|██        | 105/500 [00:29<01:49,  3.59it/s, loss=3.07, v_num=179, train_loss=3.140]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798475\n",
      "  lora_A(x) mean: 0.440880\n",
      "  lora_B(lora_A(x)) mean: 0.002509\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002509\n",
      "  output mean: 0.461849\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  21%|██        | 106/500 [00:29<01:49,  3.60it/s, loss=3.06, v_num=179, train_loss=3.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799471\n",
      "  lora_A(x) mean: 0.440244\n",
      "  lora_B(lora_A(x)) mean: 0.002512\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002512\n",
      "  output mean: 0.462112\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  21%|██▏       | 107/500 [00:29<01:48,  3.61it/s, loss=3.05, v_num=179, train_loss=3.070]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797909\n",
      "  lora_A(x) mean: 0.446086\n",
      "  lora_B(lora_A(x)) mean: 0.002536\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002536\n",
      "  output mean: 0.461811\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  22%|██▏       | 108/500 [00:29<01:48,  3.61it/s, loss=3.02, v_num=179, train_loss=2.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799863\n",
      "  lora_A(x) mean: 0.436077\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.461863\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  22%|██▏       | 109/500 [00:30<01:48,  3.62it/s, loss=3.03, v_num=179, train_loss=3.150]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797639\n",
      "  lora_A(x) mean: 0.438225\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.462461\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  22%|██▏       | 110/500 [00:30<01:47,  3.63it/s, loss=3.03, v_num=179, train_loss=2.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798167\n",
      "  lora_A(x) mean: 0.445729\n",
      "  lora_B(lora_A(x)) mean: 0.002572\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002572\n",
      "  output mean: 0.464579\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  22%|██▏       | 111/500 [00:30<01:47,  3.63it/s, loss=3.03, v_num=179, train_loss=3.050]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798954\n",
      "  lora_A(x) mean: 0.446857\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.462257\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  22%|██▏       | 112/500 [00:30<01:46,  3.64it/s, loss=3.03, v_num=179, train_loss=2.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798742\n",
      "  lora_A(x) mean: 0.442357\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.462086\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  23%|██▎       | 113/500 [00:31<01:46,  3.64it/s, loss=3.01, v_num=179, train_loss=2.910]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796918\n",
      "  lora_A(x) mean: 0.443282\n",
      "  lora_B(lora_A(x)) mean: 0.002532\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002532\n",
      "  output mean: 0.463024\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  23%|██▎       | 114/500 [00:31<01:45,  3.65it/s, loss=2.99, v_num=179, train_loss=2.680]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796949\n",
      "  lora_A(x) mean: 0.462763\n",
      "  lora_B(lora_A(x)) mean: 0.002626\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002626\n",
      "  output mean: 0.463657\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  23%|██▎       | 115/500 [00:31<01:45,  3.65it/s, loss=2.99, v_num=179, train_loss=2.970]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798073\n",
      "  lora_A(x) mean: 0.431500\n",
      "  lora_B(lora_A(x)) mean: 0.002425\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002425\n",
      "  output mean: 0.460877\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  23%|██▎       | 116/500 [00:31<01:44,  3.66it/s, loss=2.95, v_num=179, train_loss=2.720]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798329\n",
      "  lora_A(x) mean: 0.435337\n",
      "  lora_B(lora_A(x)) mean: 0.002467\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002467\n",
      "  output mean: 0.463721\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  23%|██▎       | 117/500 [00:31<01:44,  3.66it/s, loss=2.93, v_num=179, train_loss=2.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798145\n",
      "  lora_A(x) mean: 0.456041\n",
      "  lora_B(lora_A(x)) mean: 0.002577\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002577\n",
      "  output mean: 0.461588\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  24%|██▎       | 118/500 [00:32<01:44,  3.67it/s, loss=2.91, v_num=179, train_loss=2.810]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797933\n",
      "  lora_A(x) mean: 0.453870\n",
      "  lora_B(lora_A(x)) mean: 0.002548\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002548\n",
      "  output mean: 0.459940\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  24%|██▍       | 119/500 [00:32<01:43,  3.68it/s, loss=2.9, v_num=179, train_loss=2.580] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797452\n",
      "  lora_A(x) mean: 0.451754\n",
      "  lora_B(lora_A(x)) mean: 0.002556\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002556\n",
      "  output mean: 0.463876\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  24%|██▍       | 120/500 [00:32<01:43,  3.68it/s, loss=2.91, v_num=179, train_loss=3.250]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798191\n",
      "  lora_A(x) mean: 0.441318\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.463572\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  24%|██▍       | 121/500 [00:32<01:42,  3.68it/s, loss=2.89, v_num=179, train_loss=2.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798355\n",
      "  lora_A(x) mean: 0.440033\n",
      "  lora_B(lora_A(x)) mean: 0.002541\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002541\n",
      "  output mean: 0.462343\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  24%|██▍       | 122/500 [00:33<01:42,  3.69it/s, loss=2.88, v_num=179, train_loss=2.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798700\n",
      "  lora_A(x) mean: 0.453353\n",
      "  lora_B(lora_A(x)) mean: 0.002556\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002556\n",
      "  output mean: 0.461663\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  25%|██▍       | 123/500 [00:33<01:42,  3.69it/s, loss=2.9, v_num=179, train_loss=3.070] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799033\n",
      "  lora_A(x) mean: 0.438852\n",
      "  lora_B(lora_A(x)) mean: 0.002478\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002478\n",
      "  output mean: 0.461271\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  25%|██▍       | 124/500 [00:33<01:41,  3.70it/s, loss=2.88, v_num=179, train_loss=2.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797471\n",
      "  lora_A(x) mean: 0.443737\n",
      "  lora_B(lora_A(x)) mean: 0.002499\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002499\n",
      "  output mean: 0.462142\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  25%|██▌       | 125/500 [00:33<01:41,  3.70it/s, loss=2.88, v_num=179, train_loss=3.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797219\n",
      "  lora_A(x) mean: 0.455184\n",
      "  lora_B(lora_A(x)) mean: 0.002590\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002590\n",
      "  output mean: 0.463606\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  25%|██▌       | 126/500 [00:33<01:40,  3.71it/s, loss=2.88, v_num=179, train_loss=3.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798499\n",
      "  lora_A(x) mean: 0.452620\n",
      "  lora_B(lora_A(x)) mean: 0.002554\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002554\n",
      "  output mean: 0.461487\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  25%|██▌       | 127/500 [00:34<01:40,  3.71it/s, loss=2.88, v_num=179, train_loss=3.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797874\n",
      "  lora_A(x) mean: 0.464649\n",
      "  lora_B(lora_A(x)) mean: 0.002609\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002609\n",
      "  output mean: 0.462760\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  26%|██▌       | 128/500 [00:34<01:40,  3.72it/s, loss=2.88, v_num=179, train_loss=2.620]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799221\n",
      "  lora_A(x) mean: 0.442044\n",
      "  lora_B(lora_A(x)) mean: 0.002484\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002484\n",
      "  output mean: 0.462895\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  26%|██▌       | 129/500 [00:34<01:39,  3.72it/s, loss=2.86, v_num=179, train_loss=2.810]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799440\n",
      "  lora_A(x) mean: 0.449770\n",
      "  lora_B(lora_A(x)) mean: 0.002560\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002560\n",
      "  output mean: 0.462496\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  26%|██▌       | 130/500 [00:34<01:39,  3.73it/s, loss=2.88, v_num=179, train_loss=3.250]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798746\n",
      "  lora_A(x) mean: 0.441904\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.461431\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  26%|██▌       | 131/500 [00:35<01:38,  3.73it/s, loss=2.86, v_num=179, train_loss=2.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798763\n",
      "  lora_A(x) mean: 0.440884\n",
      "  lora_B(lora_A(x)) mean: 0.002507\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002507\n",
      "  output mean: 0.463199\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  26%|██▋       | 132/500 [00:35<01:38,  3.74it/s, loss=2.86, v_num=179, train_loss=2.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798964\n",
      "  lora_A(x) mean: 0.449454\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.461812\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  27%|██▋       | 133/500 [00:35<01:38,  3.74it/s, loss=2.88, v_num=179, train_loss=3.420]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798571\n",
      "  lora_A(x) mean: 0.454223\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.464203\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  27%|██▋       | 134/500 [00:35<01:37,  3.74it/s, loss=2.87, v_num=179, train_loss=2.520]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798103\n",
      "  lora_A(x) mean: 0.440738\n",
      "  lora_B(lora_A(x)) mean: 0.002507\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002507\n",
      "  output mean: 0.462766\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  27%|██▋       | 135/500 [00:35<01:37,  3.75it/s, loss=2.86, v_num=179, train_loss=2.770]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798893\n",
      "  lora_A(x) mean: 0.447807\n",
      "  lora_B(lora_A(x)) mean: 0.002534\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002534\n",
      "  output mean: 0.464348\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  27%|██▋       | 136/500 [00:36<01:36,  3.76it/s, loss=2.85, v_num=179, train_loss=2.380]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798525\n",
      "  lora_A(x) mean: 0.427412\n",
      "  lora_B(lora_A(x)) mean: 0.002461\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002461\n",
      "  output mean: 0.462928\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  27%|██▋       | 137/500 [00:36<01:36,  3.76it/s, loss=2.83, v_num=179, train_loss=2.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797055\n",
      "  lora_A(x) mean: 0.439931\n",
      "  lora_B(lora_A(x)) mean: 0.002518\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002518\n",
      "  output mean: 0.462555\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  28%|██▊       | 138/500 [00:36<01:36,  3.76it/s, loss=2.82, v_num=179, train_loss=2.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798576\n",
      "  lora_A(x) mean: 0.440365\n",
      "  lora_B(lora_A(x)) mean: 0.002466\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002466\n",
      "  output mean: 0.462579\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  28%|██▊       | 139/500 [00:36<01:35,  3.77it/s, loss=2.84, v_num=179, train_loss=3.120]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798639\n",
      "  lora_A(x) mean: 0.445318\n",
      "  lora_B(lora_A(x)) mean: 0.002517\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002517\n",
      "  output mean: 0.462026\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  28%|██▊       | 140/500 [00:37<01:35,  3.77it/s, loss=2.81, v_num=179, train_loss=2.490]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797454\n",
      "  lora_A(x) mean: 0.437546\n",
      "  lora_B(lora_A(x)) mean: 0.002509\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002509\n",
      "  output mean: 0.463241\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  28%|██▊       | 141/500 [00:37<01:35,  3.77it/s, loss=2.8, v_num=179, train_loss=2.730] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798126\n",
      "  lora_A(x) mean: 0.477263\n",
      "  lora_B(lora_A(x)) mean: 0.002684\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002684\n",
      "  output mean: 0.464457\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  28%|██▊       | 142/500 [00:37<01:34,  3.78it/s, loss=2.81, v_num=179, train_loss=2.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798113\n",
      "  lora_A(x) mean: 0.456763\n",
      "  lora_B(lora_A(x)) mean: 0.002569\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002569\n",
      "  output mean: 0.462216\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  29%|██▊       | 143/500 [00:37<01:34,  3.78it/s, loss=2.79, v_num=179, train_loss=2.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797904\n",
      "  lora_A(x) mean: 0.443547\n",
      "  lora_B(lora_A(x)) mean: 0.002499\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002499\n",
      "  output mean: 0.461629\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  29%|██▉       | 144/500 [00:38<01:34,  3.78it/s, loss=2.81, v_num=179, train_loss=3.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798617\n",
      "  lora_A(x) mean: 0.427554\n",
      "  lora_B(lora_A(x)) mean: 0.002446\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002446\n",
      "  output mean: 0.463814\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  29%|██▉       | 145/500 [00:38<01:33,  3.79it/s, loss=2.8, v_num=179, train_loss=2.830] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798343\n",
      "  lora_A(x) mean: 0.451659\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.462192\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  29%|██▉       | 146/500 [00:38<01:33,  3.79it/s, loss=2.78, v_num=179, train_loss=2.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798262\n",
      "  lora_A(x) mean: 0.443658\n",
      "  lora_B(lora_A(x)) mean: 0.002482\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002482\n",
      "  output mean: 0.462191\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  29%|██▉       | 147/500 [00:38<01:33,  3.79it/s, loss=2.75, v_num=179, train_loss=2.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796873\n",
      "  lora_A(x) mean: 0.427722\n",
      "  lora_B(lora_A(x)) mean: 0.002469\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002469\n",
      "  output mean: 0.462219\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  30%|██▉       | 148/500 [00:38<01:32,  3.80it/s, loss=2.74, v_num=179, train_loss=2.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799005\n",
      "  lora_A(x) mean: 0.457852\n",
      "  lora_B(lora_A(x)) mean: 0.002581\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002581\n",
      "  output mean: 0.461335\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  30%|██▉       | 149/500 [00:39<01:32,  3.80it/s, loss=2.73, v_num=179, train_loss=2.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798566\n",
      "  lora_A(x) mean: 0.455213\n",
      "  lora_B(lora_A(x)) mean: 0.002535\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002535\n",
      "  output mean: 0.464663\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  30%|███       | 150/500 [00:39<01:31,  3.80it/s, loss=2.72, v_num=179, train_loss=3.020]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799038\n",
      "  lora_A(x) mean: 0.432795\n",
      "  lora_B(lora_A(x)) mean: 0.002478\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002478\n",
      "  output mean: 0.463613\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 150\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  30%|███       | 151/500 [00:39<01:31,  3.81it/s, loss=2.73, v_num=179, train_loss=2.910]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798106\n",
      "  lora_A(x) mean: 0.445507\n",
      "  lora_B(lora_A(x)) mean: 0.002528\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002528\n",
      "  output mean: 0.461033\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  30%|███       | 152/500 [00:39<01:31,  3.81it/s, loss=2.71, v_num=179, train_loss=2.460]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798591\n",
      "  lora_A(x) mean: 0.449506\n",
      "  lora_B(lora_A(x)) mean: 0.002533\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002533\n",
      "  output mean: 0.462671\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  31%|███       | 153/500 [00:40<01:31,  3.81it/s, loss=2.68, v_num=179, train_loss=2.780]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799145\n",
      "  lora_A(x) mean: 0.447809\n",
      "  lora_B(lora_A(x)) mean: 0.002527\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002527\n",
      "  output mean: 0.463196\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  31%|███       | 154/500 [00:40<01:30,  3.81it/s, loss=2.69, v_num=179, train_loss=2.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798140\n",
      "  lora_A(x) mean: 0.457083\n",
      "  lora_B(lora_A(x)) mean: 0.002570\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002570\n",
      "  output mean: 0.461255\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  31%|███       | 155/500 [00:40<01:30,  3.82it/s, loss=2.67, v_num=179, train_loss=2.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798677\n",
      "  lora_A(x) mean: 0.437339\n",
      "  lora_B(lora_A(x)) mean: 0.002512\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002512\n",
      "  output mean: 0.461504\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  31%|███       | 156/500 [00:40<01:30,  3.82it/s, loss=2.69, v_num=179, train_loss=2.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798041\n",
      "  lora_A(x) mean: 0.444471\n",
      "  lora_B(lora_A(x)) mean: 0.002524\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002524\n",
      "  output mean: 0.459906\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  31%|███▏      | 157/500 [00:41<01:29,  3.82it/s, loss=2.67, v_num=179, train_loss=2.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797998\n",
      "  lora_A(x) mean: 0.430024\n",
      "  lora_B(lora_A(x)) mean: 0.002430\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002430\n",
      "  output mean: 0.464079\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  32%|███▏      | 158/500 [00:41<01:29,  3.83it/s, loss=2.67, v_num=179, train_loss=2.470]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798928\n",
      "  lora_A(x) mean: 0.457178\n",
      "  lora_B(lora_A(x)) mean: 0.002571\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002571\n",
      "  output mean: 0.462271\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  32%|███▏      | 159/500 [00:41<01:29,  3.83it/s, loss=2.65, v_num=179, train_loss=2.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798374\n",
      "  lora_A(x) mean: 0.435513\n",
      "  lora_B(lora_A(x)) mean: 0.002510\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002510\n",
      "  output mean: 0.463796\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  32%|███▏      | 160/500 [00:41<01:28,  3.83it/s, loss=2.64, v_num=179, train_loss=2.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796733\n",
      "  lora_A(x) mean: 0.431585\n",
      "  lora_B(lora_A(x)) mean: 0.002473\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002473\n",
      "  output mean: 0.462432\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  32%|███▏      | 161/500 [00:41<01:28,  3.84it/s, loss=2.63, v_num=179, train_loss=2.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798294\n",
      "  lora_A(x) mean: 0.417493\n",
      "  lora_B(lora_A(x)) mean: 0.002420\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002420\n",
      "  output mean: 0.461716\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  32%|███▏      | 162/500 [00:42<01:28,  3.84it/s, loss=2.6, v_num=179, train_loss=2.220] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798072\n",
      "  lora_A(x) mean: 0.445702\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.463450\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  33%|███▎      | 163/500 [00:42<01:27,  3.84it/s, loss=2.62, v_num=179, train_loss=3.040]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799771\n",
      "  lora_A(x) mean: 0.465276\n",
      "  lora_B(lora_A(x)) mean: 0.002679\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002679\n",
      "  output mean: 0.462947\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  33%|███▎      | 164/500 [00:42<01:27,  3.85it/s, loss=2.6, v_num=179, train_loss=2.500] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799302\n",
      "  lora_A(x) mean: 0.448617\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.462044\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  33%|███▎      | 165/500 [00:42<01:27,  3.85it/s, loss=2.59, v_num=179, train_loss=2.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797924\n",
      "  lora_A(x) mean: 0.437270\n",
      "  lora_B(lora_A(x)) mean: 0.002477\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002477\n",
      "  output mean: 0.463634\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  33%|███▎      | 166/500 [00:43<01:26,  3.85it/s, loss=2.58, v_num=179, train_loss=2.400]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798649\n",
      "  lora_A(x) mean: 0.426621\n",
      "  lora_B(lora_A(x)) mean: 0.002446\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002446\n",
      "  output mean: 0.462099\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  33%|███▎      | 167/500 [00:43<01:26,  3.85it/s, loss=2.58, v_num=179, train_loss=2.560]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798449\n",
      "  lora_A(x) mean: 0.462551\n",
      "  lora_B(lora_A(x)) mean: 0.002605\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002605\n",
      "  output mean: 0.460927\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  34%|███▎      | 168/500 [00:43<01:26,  3.86it/s, loss=2.58, v_num=179, train_loss=2.400]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798430\n",
      "  lora_A(x) mean: 0.445894\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.461853\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  34%|███▍      | 169/500 [00:43<01:25,  3.86it/s, loss=2.56, v_num=179, train_loss=2.290]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797729\n",
      "  lora_A(x) mean: 0.438383\n",
      "  lora_B(lora_A(x)) mean: 0.002515\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002515\n",
      "  output mean: 0.463100\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  34%|███▍      | 170/500 [00:44<01:25,  3.86it/s, loss=2.53, v_num=179, train_loss=2.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796733\n",
      "  lora_A(x) mean: 0.432837\n",
      "  lora_B(lora_A(x)) mean: 0.002471\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002471\n",
      "  output mean: 0.461819\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  34%|███▍      | 171/500 [00:44<01:25,  3.87it/s, loss=2.48, v_num=179, train_loss=2.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799334\n",
      "  lora_A(x) mean: 0.436691\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.461693\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  34%|███▍      | 172/500 [00:44<01:24,  3.87it/s, loss=2.5, v_num=179, train_loss=2.860] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797361\n",
      "  lora_A(x) mean: 0.433225\n",
      "  lora_B(lora_A(x)) mean: 0.002464\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002464\n",
      "  output mean: 0.460676\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  35%|███▍      | 173/500 [00:44<01:24,  3.87it/s, loss=2.47, v_num=179, train_loss=2.160]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798837\n",
      "  lora_A(x) mean: 0.436363\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.462636\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  35%|███▍      | 174/500 [00:44<01:24,  3.87it/s, loss=2.47, v_num=179, train_loss=2.620]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799403\n",
      "  lora_A(x) mean: 0.441945\n",
      "  lora_B(lora_A(x)) mean: 0.002547\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002547\n",
      "  output mean: 0.463604\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  35%|███▌      | 175/500 [00:45<01:23,  3.88it/s, loss=2.46, v_num=179, train_loss=2.180]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798269\n",
      "  lora_A(x) mean: 0.436279\n",
      "  lora_B(lora_A(x)) mean: 0.002485\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002485\n",
      "  output mean: 0.461542\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  35%|███▌      | 176/500 [00:45<01:23,  3.88it/s, loss=2.45, v_num=179, train_loss=2.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797071\n",
      "  lora_A(x) mean: 0.451678\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.463062\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  35%|███▌      | 177/500 [00:45<01:23,  3.88it/s, loss=2.48, v_num=179, train_loss=2.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798964\n",
      "  lora_A(x) mean: 0.459379\n",
      "  lora_B(lora_A(x)) mean: 0.002628\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002628\n",
      "  output mean: 0.462292\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  36%|███▌      | 178/500 [00:45<01:22,  3.88it/s, loss=2.5, v_num=179, train_loss=2.890] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797657\n",
      "  lora_A(x) mean: 0.448103\n",
      "  lora_B(lora_A(x)) mean: 0.002527\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002527\n",
      "  output mean: 0.462273\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  36%|███▌      | 179/500 [00:46<01:22,  3.89it/s, loss=2.51, v_num=179, train_loss=2.940]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798215\n",
      "  lora_A(x) mean: 0.461916\n",
      "  lora_B(lora_A(x)) mean: 0.002606\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002606\n",
      "  output mean: 0.463309\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  36%|███▌      | 180/500 [00:46<01:22,  3.89it/s, loss=2.51, v_num=179, train_loss=2.270]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800388\n",
      "  lora_A(x) mean: 0.437754\n",
      "  lora_B(lora_A(x)) mean: 0.002517\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002517\n",
      "  output mean: 0.460761\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  36%|███▌      | 181/500 [00:46<01:21,  3.89it/s, loss=2.49, v_num=179, train_loss=2.230]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800526\n",
      "  lora_A(x) mean: 0.426635\n",
      "  lora_B(lora_A(x)) mean: 0.002432\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002432\n",
      "  output mean: 0.463710\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  36%|███▋      | 182/500 [00:46<01:21,  3.89it/s, loss=2.52, v_num=179, train_loss=2.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798904\n",
      "  lora_A(x) mean: 0.433094\n",
      "  lora_B(lora_A(x)) mean: 0.002476\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002476\n",
      "  output mean: 0.461630\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  37%|███▋      | 183/500 [00:46<01:21,  3.90it/s, loss=2.47, v_num=179, train_loss=2.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797972\n",
      "  lora_A(x) mean: 0.439550\n",
      "  lora_B(lora_A(x)) mean: 0.002510\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002510\n",
      "  output mean: 0.463093\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  37%|███▋      | 184/500 [00:47<01:21,  3.90it/s, loss=2.46, v_num=179, train_loss=2.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798056\n",
      "  lora_A(x) mean: 0.452216\n",
      "  lora_B(lora_A(x)) mean: 0.002535\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002535\n",
      "  output mean: 0.461703\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  37%|███▋      | 185/500 [00:47<01:20,  3.90it/s, loss=2.44, v_num=179, train_loss=2.220]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797261\n",
      "  lora_A(x) mean: 0.444516\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.463481\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  37%|███▋      | 186/500 [00:47<01:20,  3.90it/s, loss=2.44, v_num=179, train_loss=2.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798535\n",
      "  lora_A(x) mean: 0.450203\n",
      "  lora_B(lora_A(x)) mean: 0.002539\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002539\n",
      "  output mean: 0.463551\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  37%|███▋      | 187/500 [00:47<01:20,  3.91it/s, loss=2.46, v_num=179, train_loss=2.800]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799169\n",
      "  lora_A(x) mean: 0.435485\n",
      "  lora_B(lora_A(x)) mean: 0.002456\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002456\n",
      "  output mean: 0.461614\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  38%|███▊      | 188/500 [00:48<01:19,  3.91it/s, loss=2.45, v_num=179, train_loss=2.210]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798396\n",
      "  lora_A(x) mean: 0.448841\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.463230\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  38%|███▊      | 189/500 [00:48<01:19,  3.91it/s, loss=2.47, v_num=179, train_loss=2.720]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798158\n",
      "  lora_A(x) mean: 0.435143\n",
      "  lora_B(lora_A(x)) mean: 0.002452\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002452\n",
      "  output mean: 0.464055\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  38%|███▊      | 190/500 [00:48<01:19,  3.91it/s, loss=2.49, v_num=179, train_loss=2.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798110\n",
      "  lora_A(x) mean: 0.434606\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.460158\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  38%|███▊      | 191/500 [00:48<01:18,  3.92it/s, loss=2.49, v_num=179, train_loss=1.970]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798678\n",
      "  lora_A(x) mean: 0.447603\n",
      "  lora_B(lora_A(x)) mean: 0.002529\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002529\n",
      "  output mean: 0.461075\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  38%|███▊      | 192/500 [00:48<01:18,  3.92it/s, loss=2.48, v_num=179, train_loss=2.720]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799111\n",
      "  lora_A(x) mean: 0.460220\n",
      "  lora_B(lora_A(x)) mean: 0.002614\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002614\n",
      "  output mean: 0.461385\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  39%|███▊      | 193/500 [00:49<01:18,  3.92it/s, loss=2.5, v_num=179, train_loss=2.490] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798958\n",
      "  lora_A(x) mean: 0.427920\n",
      "  lora_B(lora_A(x)) mean: 0.002453\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002453\n",
      "  output mean: 0.463683\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  39%|███▉      | 194/500 [00:49<01:17,  3.92it/s, loss=2.52, v_num=179, train_loss=3.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797841\n",
      "  lora_A(x) mean: 0.431625\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.463081\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  39%|███▉      | 195/500 [00:49<01:17,  3.93it/s, loss=2.51, v_num=179, train_loss=1.860]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798203\n",
      "  lora_A(x) mean: 0.436243\n",
      "  lora_B(lora_A(x)) mean: 0.002469\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002469\n",
      "  output mean: 0.462388\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  39%|███▉      | 196/500 [00:49<01:17,  3.93it/s, loss=2.51, v_num=179, train_loss=2.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799509\n",
      "  lora_A(x) mean: 0.437123\n",
      "  lora_B(lora_A(x)) mean: 0.002481\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002481\n",
      "  output mean: 0.461828\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  39%|███▉      | 197/500 [00:50<01:17,  3.93it/s, loss=2.52, v_num=179, train_loss=2.990]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797624\n",
      "  lora_A(x) mean: 0.438735\n",
      "  lora_B(lora_A(x)) mean: 0.002492\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002492\n",
      "  output mean: 0.462924\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  40%|███▉      | 198/500 [00:50<01:16,  3.93it/s, loss=2.48, v_num=179, train_loss=1.970]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796806\n",
      "  lora_A(x) mean: 0.435212\n",
      "  lora_B(lora_A(x)) mean: 0.002451\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002451\n",
      "  output mean: 0.463239\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  40%|███▉      | 199/500 [00:50<01:16,  3.93it/s, loss=2.44, v_num=179, train_loss=2.270]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797780\n",
      "  lora_A(x) mean: 0.462176\n",
      "  lora_B(lora_A(x)) mean: 0.002616\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002616\n",
      "  output mean: 0.462647\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  40%|████      | 200/500 [00:50<01:16,  3.94it/s, loss=2.42, v_num=179, train_loss=1.890]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798121\n",
      "  lora_A(x) mean: 0.438471\n",
      "  lora_B(lora_A(x)) mean: 0.002451\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002451\n",
      "  output mean: 0.463528\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 200\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  40%|████      | 201/500 [00:51<01:15,  3.94it/s, loss=2.43, v_num=179, train_loss=2.370]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797625\n",
      "  lora_A(x) mean: 0.444218\n",
      "  lora_B(lora_A(x)) mean: 0.002543\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002543\n",
      "  output mean: 0.464156\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  40%|████      | 202/500 [00:51<01:15,  3.94it/s, loss=2.41, v_num=179, train_loss=2.240]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797825\n",
      "  lora_A(x) mean: 0.444824\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.464613\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  41%|████      | 203/500 [00:51<01:15,  3.94it/s, loss=2.42, v_num=179, train_loss=2.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797155\n",
      "  lora_A(x) mean: 0.443902\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.462026\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  41%|████      | 204/500 [00:51<01:15,  3.94it/s, loss=2.43, v_num=179, train_loss=2.380]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797847\n",
      "  lora_A(x) mean: 0.423981\n",
      "  lora_B(lora_A(x)) mean: 0.002423\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002423\n",
      "  output mean: 0.462874\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  41%|████      | 205/500 [00:51<01:14,  3.94it/s, loss=2.42, v_num=179, train_loss=2.060]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798052\n",
      "  lora_A(x) mean: 0.433477\n",
      "  lora_B(lora_A(x)) mean: 0.002474\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002474\n",
      "  output mean: 0.463480\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  41%|████      | 206/500 [00:52<01:14,  3.95it/s, loss=2.42, v_num=179, train_loss=2.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798334\n",
      "  lora_A(x) mean: 0.445543\n",
      "  lora_B(lora_A(x)) mean: 0.002496\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002496\n",
      "  output mean: 0.463787\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  41%|████▏     | 207/500 [00:52<01:14,  3.95it/s, loss=2.42, v_num=179, train_loss=2.870]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797699\n",
      "  lora_A(x) mean: 0.450110\n",
      "  lora_B(lora_A(x)) mean: 0.002548\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002548\n",
      "  output mean: 0.461222\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  42%|████▏     | 208/500 [00:52<01:13,  3.95it/s, loss=2.41, v_num=179, train_loss=1.880]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798783\n",
      "  lora_A(x) mean: 0.426914\n",
      "  lora_B(lora_A(x)) mean: 0.002405\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002405\n",
      "  output mean: 0.461583\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  42%|████▏     | 209/500 [00:52<01:13,  3.95it/s, loss=2.39, v_num=179, train_loss=2.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797576\n",
      "  lora_A(x) mean: 0.425754\n",
      "  lora_B(lora_A(x)) mean: 0.002435\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002435\n",
      "  output mean: 0.463784\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  42%|████▏     | 210/500 [00:53<01:13,  3.95it/s, loss=2.38, v_num=179, train_loss=2.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798350\n",
      "  lora_A(x) mean: 0.419590\n",
      "  lora_B(lora_A(x)) mean: 0.002395\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002395\n",
      "  output mean: 0.459956\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  42%|████▏     | 211/500 [00:53<01:13,  3.95it/s, loss=2.41, v_num=179, train_loss=2.470]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797579\n",
      "  lora_A(x) mean: 0.441867\n",
      "  lora_B(lora_A(x)) mean: 0.002512\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002512\n",
      "  output mean: 0.461804\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  42%|████▏     | 212/500 [00:53<01:12,  3.95it/s, loss=2.35, v_num=179, train_loss=1.600]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798995\n",
      "  lora_A(x) mean: 0.440337\n",
      "  lora_B(lora_A(x)) mean: 0.002492\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002492\n",
      "  output mean: 0.462869\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  43%|████▎     | 213/500 [00:53<01:12,  3.96it/s, loss=2.33, v_num=179, train_loss=2.020]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798675\n",
      "  lora_A(x) mean: 0.463605\n",
      "  lora_B(lora_A(x)) mean: 0.002590\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002590\n",
      "  output mean: 0.462624\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  43%|████▎     | 214/500 [00:54<01:12,  3.96it/s, loss=2.28, v_num=179, train_loss=2.230]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798086\n",
      "  lora_A(x) mean: 0.447824\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.462681\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  43%|████▎     | 215/500 [00:54<01:11,  3.96it/s, loss=2.27, v_num=179, train_loss=1.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799026\n",
      "  lora_A(x) mean: 0.435857\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.462469\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  43%|████▎     | 216/500 [00:54<01:11,  3.96it/s, loss=2.26, v_num=179, train_loss=2.280]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799183\n",
      "  lora_A(x) mean: 0.454614\n",
      "  lora_B(lora_A(x)) mean: 0.002605\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002605\n",
      "  output mean: 0.462495\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  43%|████▎     | 217/500 [00:54<01:11,  3.96it/s, loss=2.19, v_num=179, train_loss=1.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798561\n",
      "  lora_A(x) mean: 0.455063\n",
      "  lora_B(lora_A(x)) mean: 0.002546\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002546\n",
      "  output mean: 0.460722\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  44%|████▎     | 218/500 [00:55<01:11,  3.96it/s, loss=2.21, v_num=179, train_loss=2.290]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797652\n",
      "  lora_A(x) mean: 0.438744\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.462029\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  44%|████▍     | 219/500 [00:55<01:10,  3.96it/s, loss=2.19, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798349\n",
      "  lora_A(x) mean: 0.449881\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.461792\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  44%|████▍     | 220/500 [00:55<01:10,  3.97it/s, loss=2.22, v_num=179, train_loss=2.380]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798224\n",
      "  lora_A(x) mean: 0.458318\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.461164\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  44%|████▍     | 221/500 [00:55<01:10,  3.97it/s, loss=2.2, v_num=179, train_loss=2.110] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798354\n",
      "  lora_A(x) mean: 0.430894\n",
      "  lora_B(lora_A(x)) mean: 0.002471\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002471\n",
      "  output mean: 0.462782\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  44%|████▍     | 222/500 [00:55<01:10,  3.97it/s, loss=2.2, v_num=179, train_loss=2.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799206\n",
      "  lora_A(x) mean: 0.442119\n",
      "  lora_B(lora_A(x)) mean: 0.002509\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002509\n",
      "  output mean: 0.461495\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  45%|████▍     | 223/500 [00:56<01:09,  3.97it/s, loss=2.18, v_num=179, train_loss=2.120]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797018\n",
      "  lora_A(x) mean: 0.456065\n",
      "  lora_B(lora_A(x)) mean: 0.002581\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002581\n",
      "  output mean: 0.461900\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  45%|████▍     | 224/500 [00:56<01:09,  3.97it/s, loss=2.17, v_num=179, train_loss=2.030]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797473\n",
      "  lora_A(x) mean: 0.460175\n",
      "  lora_B(lora_A(x)) mean: 0.002617\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002617\n",
      "  output mean: 0.462572\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  45%|████▌     | 225/500 [00:56<01:09,  3.97it/s, loss=2.19, v_num=179, train_loss=2.520]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798884\n",
      "  lora_A(x) mean: 0.444147\n",
      "  lora_B(lora_A(x)) mean: 0.002510\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002510\n",
      "  output mean: 0.462568\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  45%|████▌     | 226/500 [00:56<01:08,  3.98it/s, loss=2.2, v_num=179, train_loss=2.780] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799180\n",
      "  lora_A(x) mean: 0.445104\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.464234\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  45%|████▌     | 227/500 [00:57<01:08,  3.98it/s, loss=2.19, v_num=179, train_loss=2.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798284\n",
      "  lora_A(x) mean: 0.450796\n",
      "  lora_B(lora_A(x)) mean: 0.002575\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002575\n",
      "  output mean: 0.461879\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  46%|████▌     | 228/500 [00:57<01:08,  3.98it/s, loss=2.21, v_num=179, train_loss=2.180]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798612\n",
      "  lora_A(x) mean: 0.451723\n",
      "  lora_B(lora_A(x)) mean: 0.002580\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002580\n",
      "  output mean: 0.463491\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  46%|████▌     | 229/500 [00:57<01:08,  3.98it/s, loss=2.2, v_num=179, train_loss=2.170] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798158\n",
      "  lora_A(x) mean: 0.445677\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.461470\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  46%|████▌     | 230/500 [00:57<01:07,  3.98it/s, loss=2.16, v_num=179, train_loss=1.950]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798042\n",
      "  lora_A(x) mean: 0.451786\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.461621\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  46%|████▌     | 231/500 [00:57<01:07,  3.98it/s, loss=2.13, v_num=179, train_loss=1.940]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799110\n",
      "  lora_A(x) mean: 0.431851\n",
      "  lora_B(lora_A(x)) mean: 0.002456\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002456\n",
      "  output mean: 0.463282\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  46%|████▋     | 232/500 [00:58<01:07,  3.99it/s, loss=2.17, v_num=179, train_loss=2.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797321\n",
      "  lora_A(x) mean: 0.444923\n",
      "  lora_B(lora_A(x)) mean: 0.002501\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002501\n",
      "  output mean: 0.462088\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  47%|████▋     | 233/500 [00:58<01:06,  3.99it/s, loss=2.17, v_num=179, train_loss=1.880]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798701\n",
      "  lora_A(x) mean: 0.439248\n",
      "  lora_B(lora_A(x)) mean: 0.002501\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002501\n",
      "  output mean: 0.463209\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  47%|████▋     | 234/500 [00:58<01:06,  3.99it/s, loss=2.19, v_num=179, train_loss=2.690]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798136\n",
      "  lora_A(x) mean: 0.446881\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.461146\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  47%|████▋     | 235/500 [00:58<01:06,  3.99it/s, loss=2.2, v_num=179, train_loss=1.960] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799292\n",
      "  lora_A(x) mean: 0.433408\n",
      "  lora_B(lora_A(x)) mean: 0.002464\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002464\n",
      "  output mean: 0.462734\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  47%|████▋     | 236/500 [00:59<01:06,  3.99it/s, loss=2.19, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799140\n",
      "  lora_A(x) mean: 0.446597\n",
      "  lora_B(lora_A(x)) mean: 0.002516\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002516\n",
      "  output mean: 0.463734\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  47%|████▋     | 237/500 [00:59<01:05,  3.99it/s, loss=2.21, v_num=179, train_loss=2.120]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797642\n",
      "  lora_A(x) mean: 0.451717\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.464450\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  48%|████▊     | 238/500 [00:59<01:05,  3.99it/s, loss=2.2, v_num=179, train_loss=2.110] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798307\n",
      "  lora_A(x) mean: 0.411696\n",
      "  lora_B(lora_A(x)) mean: 0.002386\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002386\n",
      "  output mean: 0.463332\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  48%|████▊     | 239/500 [00:59<01:05,  4.00it/s, loss=2.22, v_num=179, train_loss=2.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797715\n",
      "  lora_A(x) mean: 0.451179\n",
      "  lora_B(lora_A(x)) mean: 0.002565\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002565\n",
      "  output mean: 0.462081\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  48%|████▊     | 240/500 [01:00<01:05,  4.00it/s, loss=2.22, v_num=179, train_loss=2.240]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797366\n",
      "  lora_A(x) mean: 0.462678\n",
      "  lora_B(lora_A(x)) mean: 0.002571\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002571\n",
      "  output mean: 0.463008\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  48%|████▊     | 241/500 [01:00<01:04,  4.00it/s, loss=2.23, v_num=179, train_loss=2.420]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798190\n",
      "  lora_A(x) mean: 0.449821\n",
      "  lora_B(lora_A(x)) mean: 0.002603\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002603\n",
      "  output mean: 0.464145\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  48%|████▊     | 242/500 [01:00<01:04,  4.00it/s, loss=2.26, v_num=179, train_loss=2.630]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799356\n",
      "  lora_A(x) mean: 0.446595\n",
      "  lora_B(lora_A(x)) mean: 0.002526\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002526\n",
      "  output mean: 0.462086\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  49%|████▊     | 243/500 [01:00<01:04,  4.00it/s, loss=2.25, v_num=179, train_loss=2.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797871\n",
      "  lora_A(x) mean: 0.457672\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.464468\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  49%|████▉     | 244/500 [01:00<01:03,  4.00it/s, loss=2.28, v_num=179, train_loss=2.600]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797324\n",
      "  lora_A(x) mean: 0.441070\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.462584\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  49%|████▉     | 245/500 [01:01<01:03,  4.00it/s, loss=2.23, v_num=179, train_loss=1.440]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799441\n",
      "  lora_A(x) mean: 0.451473\n",
      "  lora_B(lora_A(x)) mean: 0.002575\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002575\n",
      "  output mean: 0.462408\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  49%|████▉     | 246/500 [01:01<01:03,  4.00it/s, loss=2.21, v_num=179, train_loss=2.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797509\n",
      "  lora_A(x) mean: 0.443710\n",
      "  lora_B(lora_A(x)) mean: 0.002504\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002504\n",
      "  output mean: 0.460658\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  49%|████▉     | 247/500 [01:01<01:03,  4.00it/s, loss=2.19, v_num=179, train_loss=2.380]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798895\n",
      "  lora_A(x) mean: 0.456671\n",
      "  lora_B(lora_A(x)) mean: 0.002597\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002597\n",
      "  output mean: 0.463772\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  50%|████▉     | 248/500 [01:01<01:02,  4.00it/s, loss=2.2, v_num=179, train_loss=2.370] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799282\n",
      "  lora_A(x) mean: 0.453192\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.462715\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  50%|████▉     | 249/500 [01:02<01:02,  4.01it/s, loss=2.19, v_num=179, train_loss=1.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797972\n",
      "  lora_A(x) mean: 0.457050\n",
      "  lora_B(lora_A(x)) mean: 0.002628\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002628\n",
      "  output mean: 0.464198\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  50%|█████     | 250/500 [01:02<01:02,  4.01it/s, loss=2.18, v_num=179, train_loss=1.800]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798656\n",
      "  lora_A(x) mean: 0.445007\n",
      "  lora_B(lora_A(x)) mean: 0.002582\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002582\n",
      "  output mean: 0.464275\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 250\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  50%|█████     | 251/500 [01:02<01:02,  4.01it/s, loss=2.19, v_num=179, train_loss=2.050]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797053\n",
      "  lora_A(x) mean: 0.415244\n",
      "  lora_B(lora_A(x)) mean: 0.002359\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002359\n",
      "  output mean: 0.459623\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  50%|█████     | 252/500 [01:02<01:01,  4.01it/s, loss=2.15, v_num=179, train_loss=1.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798895\n",
      "  lora_A(x) mean: 0.444925\n",
      "  lora_B(lora_A(x)) mean: 0.002494\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002494\n",
      "  output mean: 0.464250\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  51%|█████     | 253/500 [01:03<01:01,  4.01it/s, loss=2.17, v_num=179, train_loss=2.420]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797435\n",
      "  lora_A(x) mean: 0.444591\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.462276\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  51%|█████     | 254/500 [01:03<01:01,  4.01it/s, loss=2.16, v_num=179, train_loss=2.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798480\n",
      "  lora_A(x) mean: 0.450283\n",
      "  lora_B(lora_A(x)) mean: 0.002564\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002564\n",
      "  output mean: 0.461705\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  51%|█████     | 255/500 [01:03<01:01,  4.01it/s, loss=2.19, v_num=179, train_loss=2.480]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797715\n",
      "  lora_A(x) mean: 0.448403\n",
      "  lora_B(lora_A(x)) mean: 0.002553\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002553\n",
      "  output mean: 0.461199\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  51%|█████     | 256/500 [01:03<01:00,  4.01it/s, loss=2.23, v_num=179, train_loss=2.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798485\n",
      "  lora_A(x) mean: 0.436334\n",
      "  lora_B(lora_A(x)) mean: 0.002491\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002491\n",
      "  output mean: 0.462267\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  51%|█████▏    | 257/500 [01:04<01:00,  4.01it/s, loss=2.22, v_num=179, train_loss=1.990]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797459\n",
      "  lora_A(x) mean: 0.460446\n",
      "  lora_B(lora_A(x)) mean: 0.002598\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002598\n",
      "  output mean: 0.461766\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  52%|█████▏    | 258/500 [01:04<01:00,  4.01it/s, loss=2.23, v_num=179, train_loss=2.180]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797501\n",
      "  lora_A(x) mean: 0.436390\n",
      "  lora_B(lora_A(x)) mean: 0.002465\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002465\n",
      "  output mean: 0.461648\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  52%|█████▏    | 259/500 [01:04<01:00,  4.01it/s, loss=2.21, v_num=179, train_loss=2.060]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799044\n",
      "  lora_A(x) mean: 0.453903\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.461292\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  52%|█████▏    | 260/500 [01:04<00:59,  4.01it/s, loss=2.17, v_num=179, train_loss=1.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798344\n",
      "  lora_A(x) mean: 0.443832\n",
      "  lora_B(lora_A(x)) mean: 0.002533\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002533\n",
      "  output mean: 0.459350\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  52%|█████▏    | 261/500 [01:05<00:59,  4.01it/s, loss=2.13, v_num=179, train_loss=1.530]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797904\n",
      "  lora_A(x) mean: 0.433555\n",
      "  lora_B(lora_A(x)) mean: 0.002472\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002472\n",
      "  output mean: 0.464187\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  52%|█████▏    | 262/500 [01:05<00:59,  4.02it/s, loss=2.09, v_num=179, train_loss=1.910]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797437\n",
      "  lora_A(x) mean: 0.445260\n",
      "  lora_B(lora_A(x)) mean: 0.002515\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002515\n",
      "  output mean: 0.463778\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  53%|█████▎    | 263/500 [01:05<00:59,  4.02it/s, loss=2.09, v_num=179, train_loss=1.940]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799358\n",
      "  lora_A(x) mean: 0.451551\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.461206\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  53%|█████▎    | 264/500 [01:05<00:58,  4.02it/s, loss=2.06, v_num=179, train_loss=2.050]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799896\n",
      "  lora_A(x) mean: 0.446831\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.461496\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  53%|█████▎    | 265/500 [01:05<00:58,  4.02it/s, loss=2.1, v_num=179, train_loss=2.210] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799236\n",
      "  lora_A(x) mean: 0.439334\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.462205\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  53%|█████▎    | 266/500 [01:06<00:58,  4.02it/s, loss=2.07, v_num=179, train_loss=1.690]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797416\n",
      "  lora_A(x) mean: 0.433283\n",
      "  lora_B(lora_A(x)) mean: 0.002490\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002490\n",
      "  output mean: 0.462709\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  53%|█████▎    | 267/500 [01:06<00:57,  4.02it/s, loss=2.04, v_num=179, train_loss=1.780]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796200\n",
      "  lora_A(x) mean: 0.447296\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.461560\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  54%|█████▎    | 268/500 [01:06<00:57,  4.02it/s, loss=2.01, v_num=179, train_loss=1.880]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797466\n",
      "  lora_A(x) mean: 0.442560\n",
      "  lora_B(lora_A(x)) mean: 0.002535\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002535\n",
      "  output mean: 0.461496\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  54%|█████▍    | 269/500 [01:06<00:57,  4.02it/s, loss=2, v_num=179, train_loss=1.620]   get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797898\n",
      "  lora_A(x) mean: 0.434257\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.462526\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  54%|█████▍    | 270/500 [01:07<00:57,  4.02it/s, loss=1.99, v_num=179, train_loss=1.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798511\n",
      "  lora_A(x) mean: 0.445876\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.460115\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  54%|█████▍    | 271/500 [01:07<00:56,  4.02it/s, loss=1.98, v_num=179, train_loss=1.900]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798573\n",
      "  lora_A(x) mean: 0.438670\n",
      "  lora_B(lora_A(x)) mean: 0.002498\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002498\n",
      "  output mean: 0.462679\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  54%|█████▍    | 272/500 [01:07<00:56,  4.02it/s, loss=2.02, v_num=179, train_loss=2.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796934\n",
      "  lora_A(x) mean: 0.446412\n",
      "  lora_B(lora_A(x)) mean: 0.002586\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002586\n",
      "  output mean: 0.462412\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  55%|█████▍    | 273/500 [01:07<00:56,  4.02it/s, loss=1.96, v_num=179, train_loss=1.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797753\n",
      "  lora_A(x) mean: 0.451920\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.462099\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  55%|█████▍    | 274/500 [01:08<00:56,  4.03it/s, loss=1.92, v_num=179, train_loss=1.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797311\n",
      "  lora_A(x) mean: 0.459618\n",
      "  lora_B(lora_A(x)) mean: 0.002598\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002598\n",
      "  output mean: 0.459978\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  55%|█████▌    | 275/500 [01:08<00:55,  4.03it/s, loss=1.88, v_num=179, train_loss=1.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797607\n",
      "  lora_A(x) mean: 0.453436\n",
      "  lora_B(lora_A(x)) mean: 0.002553\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002553\n",
      "  output mean: 0.460830\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  55%|█████▌    | 276/500 [01:08<00:55,  4.03it/s, loss=1.81, v_num=179, train_loss=1.210]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797315\n",
      "  lora_A(x) mean: 0.454419\n",
      "  lora_B(lora_A(x)) mean: 0.002567\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002567\n",
      "  output mean: 0.464740\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  55%|█████▌    | 277/500 [01:08<00:55,  4.03it/s, loss=1.79, v_num=179, train_loss=1.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797735\n",
      "  lora_A(x) mean: 0.436321\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.462518\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  56%|█████▌    | 278/500 [01:09<00:55,  4.03it/s, loss=1.74, v_num=179, train_loss=1.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798762\n",
      "  lora_A(x) mean: 0.438174\n",
      "  lora_B(lora_A(x)) mean: 0.002440\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002440\n",
      "  output mean: 0.462158\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  56%|█████▌    | 279/500 [01:09<00:54,  4.03it/s, loss=1.77, v_num=179, train_loss=2.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796894\n",
      "  lora_A(x) mean: 0.454534\n",
      "  lora_B(lora_A(x)) mean: 0.002606\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002606\n",
      "  output mean: 0.461195\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  56%|█████▌    | 280/500 [01:09<00:54,  4.03it/s, loss=1.75, v_num=179, train_loss=1.240]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798584\n",
      "  lora_A(x) mean: 0.447165\n",
      "  lora_B(lora_A(x)) mean: 0.002513\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002513\n",
      "  output mean: 0.460629\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  56%|█████▌    | 281/500 [01:09<00:54,  4.03it/s, loss=1.8, v_num=179, train_loss=2.400] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799060\n",
      "  lora_A(x) mean: 0.451404\n",
      "  lora_B(lora_A(x)) mean: 0.002573\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002573\n",
      "  output mean: 0.461328\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  56%|█████▋    | 282/500 [01:09<00:54,  4.03it/s, loss=1.8, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798356\n",
      "  lora_A(x) mean: 0.448424\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.463015\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  57%|█████▋    | 283/500 [01:10<00:53,  4.03it/s, loss=1.81, v_num=179, train_loss=2.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797974\n",
      "  lora_A(x) mean: 0.430920\n",
      "  lora_B(lora_A(x)) mean: 0.002441\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002441\n",
      "  output mean: 0.462885\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  57%|█████▋    | 284/500 [01:10<00:53,  4.04it/s, loss=1.81, v_num=179, train_loss=1.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797477\n",
      "  lora_A(x) mean: 0.447093\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.461993\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  57%|█████▋    | 285/500 [01:10<00:53,  4.04it/s, loss=1.79, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797972\n",
      "  lora_A(x) mean: 0.462185\n",
      "  lora_B(lora_A(x)) mean: 0.002615\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002615\n",
      "  output mean: 0.464043\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  57%|█████▋    | 286/500 [01:10<00:53,  4.04it/s, loss=1.77, v_num=179, train_loss=1.290]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798896\n",
      "  lora_A(x) mean: 0.443985\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.462025\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  57%|█████▋    | 287/500 [01:11<00:52,  4.04it/s, loss=1.75, v_num=179, train_loss=1.370]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797499\n",
      "  lora_A(x) mean: 0.459161\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.463310\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  58%|█████▊    | 288/500 [01:11<00:52,  4.04it/s, loss=1.74, v_num=179, train_loss=1.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797371\n",
      "  lora_A(x) mean: 0.451397\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.462223\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  58%|█████▊    | 289/500 [01:11<00:52,  4.04it/s, loss=1.72, v_num=179, train_loss=1.320]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797947\n",
      "  lora_A(x) mean: 0.454085\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.460900\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  58%|█████▊    | 290/500 [01:11<00:51,  4.04it/s, loss=1.74, v_num=179, train_loss=2.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798228\n",
      "  lora_A(x) mean: 0.438654\n",
      "  lora_B(lora_A(x)) mean: 0.002518\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002518\n",
      "  output mean: 0.462776\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  58%|█████▊    | 291/500 [01:12<00:51,  4.04it/s, loss=1.75, v_num=179, train_loss=2.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798479\n",
      "  lora_A(x) mean: 0.441959\n",
      "  lora_B(lora_A(x)) mean: 0.002479\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002479\n",
      "  output mean: 0.461512\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  58%|█████▊    | 292/500 [01:12<00:51,  4.04it/s, loss=1.7, v_num=179, train_loss=1.330] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798744\n",
      "  lora_A(x) mean: 0.438520\n",
      "  lora_B(lora_A(x)) mean: 0.002493\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002493\n",
      "  output mean: 0.462114\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  59%|█████▊    | 293/500 [01:12<00:51,  4.04it/s, loss=1.72, v_num=179, train_loss=1.830]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798899\n",
      "  lora_A(x) mean: 0.414164\n",
      "  lora_B(lora_A(x)) mean: 0.002351\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002351\n",
      "  output mean: 0.463814\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  59%|█████▉    | 294/500 [01:12<00:50,  4.04it/s, loss=1.71, v_num=179, train_loss=1.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797175\n",
      "  lora_A(x) mean: 0.439558\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.461761\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  59%|█████▉    | 295/500 [01:12<00:50,  4.04it/s, loss=1.71, v_num=179, train_loss=1.690]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798367\n",
      "  lora_A(x) mean: 0.425054\n",
      "  lora_B(lora_A(x)) mean: 0.002417\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002417\n",
      "  output mean: 0.462887\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  59%|█████▉    | 296/500 [01:13<00:50,  4.04it/s, loss=1.71, v_num=179, train_loss=1.120]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798493\n",
      "  lora_A(x) mean: 0.434893\n",
      "  lora_B(lora_A(x)) mean: 0.002486\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002486\n",
      "  output mean: 0.461361\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  59%|█████▉    | 297/500 [01:13<00:50,  4.04it/s, loss=1.68, v_num=179, train_loss=1.190]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797477\n",
      "  lora_A(x) mean: 0.471174\n",
      "  lora_B(lora_A(x)) mean: 0.002657\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002657\n",
      "  output mean: 0.462931\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  60%|█████▉    | 298/500 [01:13<00:49,  4.04it/s, loss=1.7, v_num=179, train_loss=1.540] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797762\n",
      "  lora_A(x) mean: 0.453892\n",
      "  lora_B(lora_A(x)) mean: 0.002592\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002592\n",
      "  output mean: 0.462953\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  60%|█████▉    | 299/500 [01:13<00:49,  4.05it/s, loss=1.66, v_num=179, train_loss=1.940]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797735\n",
      "  lora_A(x) mean: 0.433572\n",
      "  lora_B(lora_A(x)) mean: 0.002476\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002476\n",
      "  output mean: 0.461327\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  60%|██████    | 300/500 [01:14<00:49,  4.05it/s, loss=1.67, v_num=179, train_loss=1.410]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797592\n",
      "  lora_A(x) mean: 0.448539\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.463316\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 300\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  60%|██████    | 301/500 [01:14<00:49,  4.05it/s, loss=1.61, v_num=179, train_loss=1.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799254\n",
      "  lora_A(x) mean: 0.447736\n",
      "  lora_B(lora_A(x)) mean: 0.002532\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002532\n",
      "  output mean: 0.463636\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  60%|██████    | 302/500 [01:14<00:48,  4.05it/s, loss=1.62, v_num=179, train_loss=2.160]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797974\n",
      "  lora_A(x) mean: 0.447351\n",
      "  lora_B(lora_A(x)) mean: 0.002546\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002546\n",
      "  output mean: 0.461589\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  61%|██████    | 303/500 [01:14<00:48,  4.05it/s, loss=1.57, v_num=179, train_loss=1.220]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797824\n",
      "  lora_A(x) mean: 0.426318\n",
      "  lora_B(lora_A(x)) mean: 0.002429\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002429\n",
      "  output mean: 0.463455\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  61%|██████    | 304/500 [01:15<00:48,  4.05it/s, loss=1.54, v_num=179, train_loss=1.350]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799367\n",
      "  lora_A(x) mean: 0.433230\n",
      "  lora_B(lora_A(x)) mean: 0.002470\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002470\n",
      "  output mean: 0.461614\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  61%|██████    | 305/500 [01:15<00:48,  4.05it/s, loss=1.54, v_num=179, train_loss=1.940]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797911\n",
      "  lora_A(x) mean: 0.439768\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.463088\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  61%|██████    | 306/500 [01:15<00:47,  4.05it/s, loss=1.56, v_num=179, train_loss=1.580]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798926\n",
      "  lora_A(x) mean: 0.463592\n",
      "  lora_B(lora_A(x)) mean: 0.002611\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002611\n",
      "  output mean: 0.462825\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  61%|██████▏   | 307/500 [01:15<00:47,  4.05it/s, loss=1.58, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797279\n",
      "  lora_A(x) mean: 0.446085\n",
      "  lora_B(lora_A(x)) mean: 0.002525\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002525\n",
      "  output mean: 0.462637\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  62%|██████▏   | 308/500 [01:16<00:47,  4.05it/s, loss=1.57, v_num=179, train_loss=1.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797907\n",
      "  lora_A(x) mean: 0.451968\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.463452\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  62%|██████▏   | 309/500 [01:16<00:47,  4.05it/s, loss=1.57, v_num=179, train_loss=1.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797497\n",
      "  lora_A(x) mean: 0.442432\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.461709\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  62%|██████▏   | 310/500 [01:16<00:46,  4.05it/s, loss=1.54, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797994\n",
      "  lora_A(x) mean: 0.441402\n",
      "  lora_B(lora_A(x)) mean: 0.002519\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002519\n",
      "  output mean: 0.459818\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  62%|██████▏   | 311/500 [01:16<00:46,  4.05it/s, loss=1.52, v_num=179, train_loss=1.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797549\n",
      "  lora_A(x) mean: 0.441528\n",
      "  lora_B(lora_A(x)) mean: 0.002529\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002529\n",
      "  output mean: 0.461058\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  62%|██████▏   | 312/500 [01:16<00:46,  4.05it/s, loss=1.53, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798153\n",
      "  lora_A(x) mean: 0.456440\n",
      "  lora_B(lora_A(x)) mean: 0.002557\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002557\n",
      "  output mean: 0.462802\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  63%|██████▎   | 313/500 [01:17<00:46,  4.05it/s, loss=1.56, v_num=179, train_loss=2.520]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798266\n",
      "  lora_A(x) mean: 0.453201\n",
      "  lora_B(lora_A(x)) mean: 0.002569\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002569\n",
      "  output mean: 0.463079\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  63%|██████▎   | 314/500 [01:17<00:45,  4.05it/s, loss=1.59, v_num=179, train_loss=1.820]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798714\n",
      "  lora_A(x) mean: 0.448260\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.461406\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  63%|██████▎   | 315/500 [01:17<00:45,  4.05it/s, loss=1.59, v_num=179, train_loss=1.670]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798360\n",
      "  lora_A(x) mean: 0.438190\n",
      "  lora_B(lora_A(x)) mean: 0.002530\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002530\n",
      "  output mean: 0.463658\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  63%|██████▎   | 316/500 [01:17<00:45,  4.05it/s, loss=1.61, v_num=179, train_loss=1.590]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798371\n",
      "  lora_A(x) mean: 0.435245\n",
      "  lora_B(lora_A(x)) mean: 0.002527\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002527\n",
      "  output mean: 0.462720\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  63%|██████▎   | 317/500 [01:18<00:45,  4.05it/s, loss=1.62, v_num=179, train_loss=1.340]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797755\n",
      "  lora_A(x) mean: 0.451540\n",
      "  lora_B(lora_A(x)) mean: 0.002556\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002556\n",
      "  output mean: 0.461483\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  64%|██████▎   | 318/500 [01:18<00:44,  4.05it/s, loss=1.6, v_num=179, train_loss=1.270] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797462\n",
      "  lora_A(x) mean: 0.450332\n",
      "  lora_B(lora_A(x)) mean: 0.002569\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002569\n",
      "  output mean: 0.462733\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  64%|██████▍   | 319/500 [01:18<00:44,  4.06it/s, loss=1.57, v_num=179, train_loss=1.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797868\n",
      "  lora_A(x) mean: 0.428493\n",
      "  lora_B(lora_A(x)) mean: 0.002465\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002465\n",
      "  output mean: 0.462486\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  64%|██████▍   | 320/500 [01:18<00:44,  4.06it/s, loss=1.59, v_num=179, train_loss=1.810]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797248\n",
      "  lora_A(x) mean: 0.447750\n",
      "  lora_B(lora_A(x)) mean: 0.002517\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002517\n",
      "  output mean: 0.463311\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  64%|██████▍   | 321/500 [01:19<00:44,  4.06it/s, loss=1.62, v_num=179, train_loss=1.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797801\n",
      "  lora_A(x) mean: 0.439097\n",
      "  lora_B(lora_A(x)) mean: 0.002498\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002498\n",
      "  output mean: 0.461401\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  64%|██████▍   | 322/500 [01:19<00:43,  4.06it/s, loss=1.57, v_num=179, train_loss=1.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798782\n",
      "  lora_A(x) mean: 0.452589\n",
      "  lora_B(lora_A(x)) mean: 0.002562\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002562\n",
      "  output mean: 0.462963\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  65%|██████▍   | 323/500 [01:19<00:43,  4.06it/s, loss=1.58, v_num=179, train_loss=1.490]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798559\n",
      "  lora_A(x) mean: 0.442167\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.465393\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  65%|██████▍   | 324/500 [01:19<00:43,  4.06it/s, loss=1.6, v_num=179, train_loss=1.740] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798318\n",
      "  lora_A(x) mean: 0.435991\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.463256\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  65%|██████▌   | 325/500 [01:20<00:43,  4.06it/s, loss=1.58, v_num=179, train_loss=1.440]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798376\n",
      "  lora_A(x) mean: 0.454703\n",
      "  lora_B(lora_A(x)) mean: 0.002617\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002617\n",
      "  output mean: 0.460804\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  65%|██████▌   | 326/500 [01:20<00:42,  4.06it/s, loss=1.56, v_num=179, train_loss=1.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797458\n",
      "  lora_A(x) mean: 0.440808\n",
      "  lora_B(lora_A(x)) mean: 0.002484\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002484\n",
      "  output mean: 0.463227\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  65%|██████▌   | 327/500 [01:20<00:42,  4.06it/s, loss=1.56, v_num=179, train_loss=1.740]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797921\n",
      "  lora_A(x) mean: 0.455756\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.462644\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  66%|██████▌   | 328/500 [01:20<00:42,  4.06it/s, loss=1.58, v_num=179, train_loss=1.700]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798135\n",
      "  lora_A(x) mean: 0.433472\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.460672\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  66%|██████▌   | 329/500 [01:21<00:42,  4.06it/s, loss=1.59, v_num=179, train_loss=1.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798930\n",
      "  lora_A(x) mean: 0.435980\n",
      "  lora_B(lora_A(x)) mean: 0.002479\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002479\n",
      "  output mean: 0.461460\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  66%|██████▌   | 330/500 [01:21<00:41,  4.06it/s, loss=1.61, v_num=179, train_loss=1.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798471\n",
      "  lora_A(x) mean: 0.449560\n",
      "  lora_B(lora_A(x)) mean: 0.002547\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002547\n",
      "  output mean: 0.460739\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  66%|██████▌   | 331/500 [01:21<00:41,  4.06it/s, loss=1.61, v_num=179, train_loss=1.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797936\n",
      "  lora_A(x) mean: 0.435100\n",
      "  lora_B(lora_A(x)) mean: 0.002494\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002494\n",
      "  output mean: 0.463292\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  66%|██████▋   | 332/500 [01:21<00:41,  4.06it/s, loss=1.6, v_num=179, train_loss=1.300] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798410\n",
      "  lora_A(x) mean: 0.423859\n",
      "  lora_B(lora_A(x)) mean: 0.002448\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002448\n",
      "  output mean: 0.464179\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  67%|██████▋   | 333/500 [01:21<00:41,  4.06it/s, loss=1.6, v_num=179, train_loss=2.530]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799627\n",
      "  lora_A(x) mean: 0.452260\n",
      "  lora_B(lora_A(x)) mean: 0.002554\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002554\n",
      "  output mean: 0.463605\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  67%|██████▋   | 334/500 [01:22<00:40,  4.06it/s, loss=1.59, v_num=179, train_loss=1.670]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798522\n",
      "  lora_A(x) mean: 0.438687\n",
      "  lora_B(lora_A(x)) mean: 0.002517\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002517\n",
      "  output mean: 0.463002\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  67%|██████▋   | 335/500 [01:22<00:40,  4.06it/s, loss=1.61, v_num=179, train_loss=2.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798361\n",
      "  lora_A(x) mean: 0.450816\n",
      "  lora_B(lora_A(x)) mean: 0.002618\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002618\n",
      "  output mean: 0.461253\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  67%|██████▋   | 336/500 [01:22<00:40,  4.07it/s, loss=1.59, v_num=179, train_loss=1.040]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797158\n",
      "  lora_A(x) mean: 0.429222\n",
      "  lora_B(lora_A(x)) mean: 0.002456\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002456\n",
      "  output mean: 0.462240\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  67%|██████▋   | 337/500 [01:22<00:40,  4.07it/s, loss=1.58, v_num=179, train_loss=1.250]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797785\n",
      "  lora_A(x) mean: 0.438889\n",
      "  lora_B(lora_A(x)) mean: 0.002532\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002532\n",
      "  output mean: 0.463625\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  68%|██████▊   | 338/500 [01:23<00:39,  4.07it/s, loss=1.59, v_num=179, train_loss=1.500]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797954\n",
      "  lora_A(x) mean: 0.439896\n",
      "  lora_B(lora_A(x)) mean: 0.002504\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002504\n",
      "  output mean: 0.462829\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  68%|██████▊   | 339/500 [01:23<00:39,  4.07it/s, loss=1.6, v_num=179, train_loss=1.500] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797390\n",
      "  lora_A(x) mean: 0.449833\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.461703\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  68%|██████▊   | 340/500 [01:23<00:39,  4.07it/s, loss=1.56, v_num=179, train_loss=0.990]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799250\n",
      "  lora_A(x) mean: 0.455088\n",
      "  lora_B(lora_A(x)) mean: 0.002614\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002614\n",
      "  output mean: 0.461820\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  68%|██████▊   | 341/500 [01:23<00:39,  4.07it/s, loss=1.55, v_num=179, train_loss=1.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797899\n",
      "  lora_A(x) mean: 0.460195\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.461217\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  68%|██████▊   | 342/500 [01:24<00:38,  4.07it/s, loss=1.55, v_num=179, train_loss=1.210]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799448\n",
      "  lora_A(x) mean: 0.446913\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.464901\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  69%|██████▊   | 343/500 [01:24<00:38,  4.07it/s, loss=1.63, v_num=179, train_loss=3.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797704\n",
      "  lora_A(x) mean: 0.431248\n",
      "  lora_B(lora_A(x)) mean: 0.002462\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002462\n",
      "  output mean: 0.464166\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  69%|██████▉   | 344/500 [01:24<00:38,  4.07it/s, loss=1.69, v_num=179, train_loss=2.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798073\n",
      "  lora_A(x) mean: 0.441686\n",
      "  lora_B(lora_A(x)) mean: 0.002541\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002541\n",
      "  output mean: 0.462410\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  69%|██████▉   | 345/500 [01:24<00:38,  4.07it/s, loss=1.69, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798514\n",
      "  lora_A(x) mean: 0.437243\n",
      "  lora_B(lora_A(x)) mean: 0.002461\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002461\n",
      "  output mean: 0.463731\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  69%|██████▉   | 346/500 [01:25<00:37,  4.07it/s, loss=1.72, v_num=179, train_loss=1.960]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799557\n",
      "  lora_A(x) mean: 0.441191\n",
      "  lora_B(lora_A(x)) mean: 0.002497\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002497\n",
      "  output mean: 0.463606\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  69%|██████▉   | 347/500 [01:25<00:37,  4.07it/s, loss=1.73, v_num=179, train_loss=1.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798794\n",
      "  lora_A(x) mean: 0.446352\n",
      "  lora_B(lora_A(x)) mean: 0.002580\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002580\n",
      "  output mean: 0.462970\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  70%|██████▉   | 348/500 [01:25<00:37,  4.07it/s, loss=1.75, v_num=179, train_loss=2.160]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797828\n",
      "  lora_A(x) mean: 0.439640\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.462167\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  70%|██████▉   | 349/500 [01:25<00:37,  4.07it/s, loss=1.76, v_num=179, train_loss=1.850]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798191\n",
      "  lora_A(x) mean: 0.440328\n",
      "  lora_B(lora_A(x)) mean: 0.002523\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002523\n",
      "  output mean: 0.464576\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  70%|███████   | 350/500 [01:25<00:36,  4.07it/s, loss=1.77, v_num=179, train_loss=2.060]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798959\n",
      "  lora_A(x) mean: 0.430862\n",
      "  lora_B(lora_A(x)) mean: 0.002450\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002450\n",
      "  output mean: 0.463355\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 350\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  70%|███████   | 351/500 [01:26<00:36,  4.07it/s, loss=1.77, v_num=179, train_loss=1.490]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799518\n",
      "  lora_A(x) mean: 0.438183\n",
      "  lora_B(lora_A(x)) mean: 0.002489\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002489\n",
      "  output mean: 0.460372\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  70%|███████   | 352/500 [01:26<00:36,  4.07it/s, loss=1.77, v_num=179, train_loss=1.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.800126\n",
      "  lora_A(x) mean: 0.437350\n",
      "  lora_B(lora_A(x)) mean: 0.002477\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002477\n",
      "  output mean: 0.461492\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  71%|███████   | 353/500 [01:26<00:36,  4.07it/s, loss=1.7, v_num=179, train_loss=1.230] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798500\n",
      "  lora_A(x) mean: 0.441318\n",
      "  lora_B(lora_A(x)) mean: 0.002478\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002478\n",
      "  output mean: 0.462925\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  71%|███████   | 354/500 [01:26<00:35,  4.07it/s, loss=1.67, v_num=179, train_loss=1.020]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798215\n",
      "  lora_A(x) mean: 0.437468\n",
      "  lora_B(lora_A(x)) mean: 0.002506\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002506\n",
      "  output mean: 0.462895\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  71%|███████   | 355/500 [01:27<00:35,  4.07it/s, loss=1.63, v_num=179, train_loss=1.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798918\n",
      "  lora_A(x) mean: 0.455841\n",
      "  lora_B(lora_A(x)) mean: 0.002554\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002554\n",
      "  output mean: 0.463844\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  71%|███████   | 356/500 [01:27<00:35,  4.07it/s, loss=1.66, v_num=179, train_loss=1.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798302\n",
      "  lora_A(x) mean: 0.438812\n",
      "  lora_B(lora_A(x)) mean: 0.002485\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002485\n",
      "  output mean: 0.462848\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  71%|███████▏  | 357/500 [01:27<00:35,  4.08it/s, loss=1.69, v_num=179, train_loss=1.800]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799000\n",
      "  lora_A(x) mean: 0.452572\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.463008\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  72%|███████▏  | 358/500 [01:27<00:34,  4.08it/s, loss=1.68, v_num=179, train_loss=1.250]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798967\n",
      "  lora_A(x) mean: 0.454612\n",
      "  lora_B(lora_A(x)) mean: 0.002564\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002564\n",
      "  output mean: 0.462179\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  72%|███████▏  | 359/500 [01:28<00:34,  4.08it/s, loss=1.67, v_num=179, train_loss=1.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797839\n",
      "  lora_A(x) mean: 0.424792\n",
      "  lora_B(lora_A(x)) mean: 0.002431\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002431\n",
      "  output mean: 0.463098\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  72%|███████▏  | 360/500 [01:28<00:34,  4.08it/s, loss=1.66, v_num=179, train_loss=0.815]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799010\n",
      "  lora_A(x) mean: 0.439644\n",
      "  lora_B(lora_A(x)) mean: 0.002510\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002510\n",
      "  output mean: 0.461087\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  72%|███████▏  | 361/500 [01:28<00:34,  4.08it/s, loss=1.66, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798813\n",
      "  lora_A(x) mean: 0.438948\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.462224\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  72%|███████▏  | 362/500 [01:28<00:33,  4.08it/s, loss=1.66, v_num=179, train_loss=1.190]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799847\n",
      "  lora_A(x) mean: 0.421589\n",
      "  lora_B(lora_A(x)) mean: 0.002404\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002404\n",
      "  output mean: 0.462445\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  73%|███████▎  | 363/500 [01:29<00:33,  4.08it/s, loss=1.6, v_num=179, train_loss=1.830] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797673\n",
      "  lora_A(x) mean: 0.432634\n",
      "  lora_B(lora_A(x)) mean: 0.002457\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002457\n",
      "  output mean: 0.462442\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  73%|███████▎  | 364/500 [01:29<00:33,  4.08it/s, loss=1.52, v_num=179, train_loss=1.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797745\n",
      "  lora_A(x) mean: 0.444782\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.464111\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  73%|███████▎  | 365/500 [01:29<00:33,  4.08it/s, loss=1.52, v_num=179, train_loss=1.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797899\n",
      "  lora_A(x) mean: 0.451193\n",
      "  lora_B(lora_A(x)) mean: 0.002602\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002602\n",
      "  output mean: 0.462146\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  73%|███████▎  | 366/500 [01:29<00:32,  4.08it/s, loss=1.49, v_num=179, train_loss=1.350]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798277\n",
      "  lora_A(x) mean: 0.438475\n",
      "  lora_B(lora_A(x)) mean: 0.002516\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002516\n",
      "  output mean: 0.465118\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  73%|███████▎  | 367/500 [01:29<00:32,  4.08it/s, loss=1.47, v_num=179, train_loss=1.360]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798755\n",
      "  lora_A(x) mean: 0.446789\n",
      "  lora_B(lora_A(x)) mean: 0.002564\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002564\n",
      "  output mean: 0.461232\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  74%|███████▎  | 368/500 [01:30<00:32,  4.08it/s, loss=1.4, v_num=179, train_loss=0.828] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797861\n",
      "  lora_A(x) mean: 0.424441\n",
      "  lora_B(lora_A(x)) mean: 0.002428\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002428\n",
      "  output mean: 0.461533\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  74%|███████▍  | 369/500 [01:30<00:32,  4.08it/s, loss=1.38, v_num=179, train_loss=1.460]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798535\n",
      "  lora_A(x) mean: 0.456825\n",
      "  lora_B(lora_A(x)) mean: 0.002576\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002576\n",
      "  output mean: 0.465618\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  74%|███████▍  | 370/500 [01:30<00:31,  4.08it/s, loss=1.36, v_num=179, train_loss=1.690]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797643\n",
      "  lora_A(x) mean: 0.453763\n",
      "  lora_B(lora_A(x)) mean: 0.002572\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002572\n",
      "  output mean: 0.463369\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  74%|███████▍  | 371/500 [01:30<00:31,  4.08it/s, loss=1.36, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799340\n",
      "  lora_A(x) mean: 0.432699\n",
      "  lora_B(lora_A(x)) mean: 0.002483\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002483\n",
      "  output mean: 0.461906\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  74%|███████▍  | 372/500 [01:31<00:31,  4.08it/s, loss=1.37, v_num=179, train_loss=1.490]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798418\n",
      "  lora_A(x) mean: 0.455083\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.464175\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  75%|███████▍  | 373/500 [01:31<00:31,  4.08it/s, loss=1.36, v_num=179, train_loss=1.040]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798628\n",
      "  lora_A(x) mean: 0.452578\n",
      "  lora_B(lora_A(x)) mean: 0.002534\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002534\n",
      "  output mean: 0.463161\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  75%|███████▍  | 374/500 [01:31<00:30,  4.08it/s, loss=1.41, v_num=179, train_loss=2.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798307\n",
      "  lora_A(x) mean: 0.439924\n",
      "  lora_B(lora_A(x)) mean: 0.002501\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002501\n",
      "  output mean: 0.462899\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  75%|███████▌  | 375/500 [01:31<00:30,  4.08it/s, loss=1.43, v_num=179, train_loss=1.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798881\n",
      "  lora_A(x) mean: 0.439022\n",
      "  lora_B(lora_A(x)) mean: 0.002481\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002481\n",
      "  output mean: 0.462473\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  75%|███████▌  | 376/500 [01:32<00:30,  4.08it/s, loss=1.43, v_num=179, train_loss=1.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797365\n",
      "  lora_A(x) mean: 0.448807\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.460502\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  75%|███████▌  | 377/500 [01:32<00:30,  4.08it/s, loss=1.38, v_num=179, train_loss=0.878]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797697\n",
      "  lora_A(x) mean: 0.436800\n",
      "  lora_B(lora_A(x)) mean: 0.002496\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002496\n",
      "  output mean: 0.462866\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  76%|███████▌  | 378/500 [01:32<00:29,  4.08it/s, loss=1.37, v_num=179, train_loss=0.987]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799324\n",
      "  lora_A(x) mean: 0.441354\n",
      "  lora_B(lora_A(x)) mean: 0.002469\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002469\n",
      "  output mean: 0.464023\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  76%|███████▌  | 379/500 [01:32<00:29,  4.09it/s, loss=1.39, v_num=179, train_loss=1.760]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797353\n",
      "  lora_A(x) mean: 0.438553\n",
      "  lora_B(lora_A(x)) mean: 0.002495\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002495\n",
      "  output mean: 0.460301\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  76%|███████▌  | 380/500 [01:33<00:29,  4.09it/s, loss=1.41, v_num=179, train_loss=1.200]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797525\n",
      "  lora_A(x) mean: 0.440223\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.464152\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  76%|███████▌  | 381/500 [01:33<00:29,  4.09it/s, loss=1.4, v_num=179, train_loss=1.350] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797821\n",
      "  lora_A(x) mean: 0.431598\n",
      "  lora_B(lora_A(x)) mean: 0.002476\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002476\n",
      "  output mean: 0.463140\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  76%|███████▋  | 382/500 [01:33<00:28,  4.09it/s, loss=1.42, v_num=179, train_loss=1.520]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798150\n",
      "  lora_A(x) mean: 0.432220\n",
      "  lora_B(lora_A(x)) mean: 0.002492\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002492\n",
      "  output mean: 0.463083\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  77%|███████▋  | 383/500 [01:33<00:28,  4.09it/s, loss=1.42, v_num=179, train_loss=1.760]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797992\n",
      "  lora_A(x) mean: 0.454404\n",
      "  lora_B(lora_A(x)) mean: 0.002594\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002594\n",
      "  output mean: 0.462783\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  77%|███████▋  | 384/500 [01:33<00:28,  4.09it/s, loss=1.44, v_num=179, train_loss=1.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798475\n",
      "  lora_A(x) mean: 0.433627\n",
      "  lora_B(lora_A(x)) mean: 0.002472\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002472\n",
      "  output mean: 0.462237\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  77%|███████▋  | 385/500 [01:34<00:28,  4.09it/s, loss=1.48, v_num=179, train_loss=2.310]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799113\n",
      "  lora_A(x) mean: 0.461080\n",
      "  lora_B(lora_A(x)) mean: 0.002590\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002590\n",
      "  output mean: 0.463654\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  77%|███████▋  | 386/500 [01:34<00:27,  4.09it/s, loss=1.48, v_num=179, train_loss=1.340]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799862\n",
      "  lora_A(x) mean: 0.446493\n",
      "  lora_B(lora_A(x)) mean: 0.002549\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002549\n",
      "  output mean: 0.463397\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  77%|███████▋  | 387/500 [01:34<00:27,  4.09it/s, loss=1.49, v_num=179, train_loss=1.520]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797980\n",
      "  lora_A(x) mean: 0.439982\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.461938\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  78%|███████▊  | 388/500 [01:34<00:27,  4.09it/s, loss=1.5, v_num=179, train_loss=1.000] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798222\n",
      "  lora_A(x) mean: 0.443517\n",
      "  lora_B(lora_A(x)) mean: 0.002519\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002519\n",
      "  output mean: 0.463643\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  78%|███████▊  | 389/500 [01:35<00:27,  4.09it/s, loss=1.52, v_num=179, train_loss=1.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798662\n",
      "  lora_A(x) mean: 0.438724\n",
      "  lora_B(lora_A(x)) mean: 0.002467\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002467\n",
      "  output mean: 0.463027\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  78%|███████▊  | 390/500 [01:35<00:26,  4.09it/s, loss=1.5, v_num=179, train_loss=1.340] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798025\n",
      "  lora_A(x) mean: 0.461675\n",
      "  lora_B(lora_A(x)) mean: 0.002640\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002640\n",
      "  output mean: 0.462912\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  78%|███████▊  | 391/500 [01:35<00:26,  4.09it/s, loss=1.52, v_num=179, train_loss=1.710]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798357\n",
      "  lora_A(x) mean: 0.431262\n",
      "  lora_B(lora_A(x)) mean: 0.002468\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002468\n",
      "  output mean: 0.461274\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  78%|███████▊  | 392/500 [01:35<00:26,  4.09it/s, loss=1.52, v_num=179, train_loss=1.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798628\n",
      "  lora_A(x) mean: 0.419787\n",
      "  lora_B(lora_A(x)) mean: 0.002402\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002402\n",
      "  output mean: 0.463089\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  79%|███████▊  | 393/500 [01:36<00:26,  4.09it/s, loss=1.53, v_num=179, train_loss=1.180]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798964\n",
      "  lora_A(x) mean: 0.444163\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.463538\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  79%|███████▉  | 394/500 [01:36<00:25,  4.09it/s, loss=1.46, v_num=179, train_loss=0.707]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798028\n",
      "  lora_A(x) mean: 0.434408\n",
      "  lora_B(lora_A(x)) mean: 0.002455\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002455\n",
      "  output mean: 0.462761\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  79%|███████▉  | 395/500 [01:36<00:25,  4.09it/s, loss=1.42, v_num=179, train_loss=0.945]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797191\n",
      "  lora_A(x) mean: 0.448256\n",
      "  lora_B(lora_A(x)) mean: 0.002527\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002527\n",
      "  output mean: 0.461236\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  79%|███████▉  | 396/500 [01:36<00:25,  4.09it/s, loss=1.38, v_num=179, train_loss=0.705]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798499\n",
      "  lora_A(x) mean: 0.439147\n",
      "  lora_B(lora_A(x)) mean: 0.002553\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002553\n",
      "  output mean: 0.461699\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  79%|███████▉  | 397/500 [01:37<00:25,  4.09it/s, loss=1.4, v_num=179, train_loss=1.380] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799519\n",
      "  lora_A(x) mean: 0.448019\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.462777\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  80%|███████▉  | 398/500 [01:37<00:24,  4.09it/s, loss=1.45, v_num=179, train_loss=1.960]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797805\n",
      "  lora_A(x) mean: 0.452150\n",
      "  lora_B(lora_A(x)) mean: 0.002577\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002577\n",
      "  output mean: 0.464007\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  80%|███████▉  | 399/500 [01:37<00:24,  4.09it/s, loss=1.46, v_num=179, train_loss=1.920]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797787\n",
      "  lora_A(x) mean: 0.430085\n",
      "  lora_B(lora_A(x)) mean: 0.002399\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002399\n",
      "  output mean: 0.461569\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  80%|████████  | 400/500 [01:37<00:24,  4.09it/s, loss=1.43, v_num=179, train_loss=0.577]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798240\n",
      "  lora_A(x) mean: 0.475243\n",
      "  lora_B(lora_A(x)) mean: 0.002681\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002681\n",
      "  output mean: 0.462614\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 400\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  80%|████████  | 401/500 [01:37<00:24,  4.09it/s, loss=1.41, v_num=179, train_loss=1.040]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797544\n",
      "  lora_A(x) mean: 0.445138\n",
      "  lora_B(lora_A(x)) mean: 0.002533\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002533\n",
      "  output mean: 0.462936\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  80%|████████  | 402/500 [01:38<00:23,  4.09it/s, loss=1.38, v_num=179, train_loss=0.951]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799238\n",
      "  lora_A(x) mean: 0.444584\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.462659\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  81%|████████  | 403/500 [01:38<00:23,  4.09it/s, loss=1.38, v_num=179, train_loss=1.670]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798069\n",
      "  lora_A(x) mean: 0.449111\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.460197\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  81%|████████  | 404/500 [01:38<00:23,  4.09it/s, loss=1.32, v_num=179, train_loss=0.725]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798325\n",
      "  lora_A(x) mean: 0.459117\n",
      "  lora_B(lora_A(x)) mean: 0.002566\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002566\n",
      "  output mean: 0.460095\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  81%|████████  | 405/500 [01:38<00:23,  4.09it/s, loss=1.26, v_num=179, train_loss=0.958]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797140\n",
      "  lora_A(x) mean: 0.446458\n",
      "  lora_B(lora_A(x)) mean: 0.002560\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002560\n",
      "  output mean: 0.463400\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  81%|████████  | 406/500 [01:39<00:22,  4.09it/s, loss=1.22, v_num=179, train_loss=0.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796385\n",
      "  lora_A(x) mean: 0.448456\n",
      "  lora_B(lora_A(x)) mean: 0.002544\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002544\n",
      "  output mean: 0.463185\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  81%|████████▏ | 407/500 [01:39<00:22,  4.09it/s, loss=1.24, v_num=179, train_loss=1.910]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798011\n",
      "  lora_A(x) mean: 0.447249\n",
      "  lora_B(lora_A(x)) mean: 0.002532\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002532\n",
      "  output mean: 0.464428\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  82%|████████▏ | 408/500 [01:39<00:22,  4.10it/s, loss=1.27, v_num=179, train_loss=1.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798092\n",
      "  lora_A(x) mean: 0.456877\n",
      "  lora_B(lora_A(x)) mean: 0.002561\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002561\n",
      "  output mean: 0.462281\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  82%|████████▏ | 409/500 [01:39<00:22,  4.10it/s, loss=1.25, v_num=179, train_loss=1.340]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799576\n",
      "  lora_A(x) mean: 0.446478\n",
      "  lora_B(lora_A(x)) mean: 0.002515\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002515\n",
      "  output mean: 0.461223\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  82%|████████▏ | 410/500 [01:40<00:21,  4.10it/s, loss=1.24, v_num=179, train_loss=1.330]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799512\n",
      "  lora_A(x) mean: 0.447501\n",
      "  lora_B(lora_A(x)) mean: 0.002566\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002566\n",
      "  output mean: 0.462776\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  82%|████████▏ | 411/500 [01:40<00:21,  4.10it/s, loss=1.21, v_num=179, train_loss=1.050]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797169\n",
      "  lora_A(x) mean: 0.447543\n",
      "  lora_B(lora_A(x)) mean: 0.002519\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002519\n",
      "  output mean: 0.461382\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  82%|████████▏ | 412/500 [01:40<00:21,  4.10it/s, loss=1.2, v_num=179, train_loss=1.220] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798831\n",
      "  lora_A(x) mean: 0.449855\n",
      "  lora_B(lora_A(x)) mean: 0.002563\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002563\n",
      "  output mean: 0.460535\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  83%|████████▎ | 413/500 [01:40<00:21,  4.10it/s, loss=1.19, v_num=179, train_loss=1.130]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797467\n",
      "  lora_A(x) mean: 0.443830\n",
      "  lora_B(lora_A(x)) mean: 0.002520\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002520\n",
      "  output mean: 0.460282\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  83%|████████▎ | 414/500 [01:41<00:20,  4.10it/s, loss=1.24, v_num=179, train_loss=1.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798191\n",
      "  lora_A(x) mean: 0.440562\n",
      "  lora_B(lora_A(x)) mean: 0.002520\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002520\n",
      "  output mean: 0.462382\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  83%|████████▎ | 415/500 [01:41<00:20,  4.10it/s, loss=1.28, v_num=179, train_loss=1.780]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798059\n",
      "  lora_A(x) mean: 0.443464\n",
      "  lora_B(lora_A(x)) mean: 0.002528\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002528\n",
      "  output mean: 0.463614\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  83%|████████▎ | 416/500 [01:41<00:20,  4.10it/s, loss=1.33, v_num=179, train_loss=1.680]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798065\n",
      "  lora_A(x) mean: 0.441371\n",
      "  lora_B(lora_A(x)) mean: 0.002495\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002495\n",
      "  output mean: 0.462658\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  83%|████████▎ | 417/500 [01:41<00:20,  4.10it/s, loss=1.33, v_num=179, train_loss=1.390]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799298\n",
      "  lora_A(x) mean: 0.457316\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.460999\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  84%|████████▎ | 418/500 [01:41<00:20,  4.10it/s, loss=1.33, v_num=179, train_loss=1.860]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797743\n",
      "  lora_A(x) mean: 0.452221\n",
      "  lora_B(lora_A(x)) mean: 0.002565\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002565\n",
      "  output mean: 0.462421\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  84%|████████▍ | 419/500 [01:42<00:19,  4.10it/s, loss=1.28, v_num=179, train_loss=0.889]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799237\n",
      "  lora_A(x) mean: 0.433824\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.462661\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  84%|████████▍ | 420/500 [01:42<00:19,  4.10it/s, loss=1.31, v_num=179, train_loss=1.130]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798764\n",
      "  lora_A(x) mean: 0.430453\n",
      "  lora_B(lora_A(x)) mean: 0.002451\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002451\n",
      "  output mean: 0.464790\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  84%|████████▍ | 421/500 [01:42<00:19,  4.10it/s, loss=1.33, v_num=179, train_loss=1.610]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798650\n",
      "  lora_A(x) mean: 0.447007\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.462705\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  84%|████████▍ | 422/500 [01:42<00:19,  4.10it/s, loss=1.36, v_num=179, train_loss=1.560]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798269\n",
      "  lora_A(x) mean: 0.438222\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.461501\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  85%|████████▍ | 423/500 [01:43<00:18,  4.10it/s, loss=1.33, v_num=179, train_loss=0.913]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797207\n",
      "  lora_A(x) mean: 0.448528\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.460761\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  85%|████████▍ | 424/500 [01:43<00:18,  4.10it/s, loss=1.32, v_num=179, train_loss=0.677]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798768\n",
      "  lora_A(x) mean: 0.438341\n",
      "  lora_B(lora_A(x)) mean: 0.002533\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002533\n",
      "  output mean: 0.462596\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  85%|████████▌ | 425/500 [01:43<00:18,  4.10it/s, loss=1.3, v_num=179, train_loss=0.536] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798618\n",
      "  lora_A(x) mean: 0.444679\n",
      "  lora_B(lora_A(x)) mean: 0.002549\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002549\n",
      "  output mean: 0.462943\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  85%|████████▌ | 426/500 [01:43<00:18,  4.10it/s, loss=1.33, v_num=179, train_loss=1.270]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797908\n",
      "  lora_A(x) mean: 0.459921\n",
      "  lora_B(lora_A(x)) mean: 0.002621\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002621\n",
      "  output mean: 0.462120\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  85%|████████▌ | 427/500 [01:44<00:17,  4.10it/s, loss=1.28, v_num=179, train_loss=0.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797692\n",
      "  lora_A(x) mean: 0.469889\n",
      "  lora_B(lora_A(x)) mean: 0.002664\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002664\n",
      "  output mean: 0.461536\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  86%|████████▌ | 428/500 [01:44<00:17,  4.10it/s, loss=1.25, v_num=179, train_loss=1.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798573\n",
      "  lora_A(x) mean: 0.455455\n",
      "  lora_B(lora_A(x)) mean: 0.002543\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002543\n",
      "  output mean: 0.461586\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  86%|████████▌ | 429/500 [01:44<00:17,  4.10it/s, loss=1.26, v_num=179, train_loss=1.690]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797312\n",
      "  lora_A(x) mean: 0.442496\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.464781\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  86%|████████▌ | 430/500 [01:44<00:17,  4.10it/s, loss=1.3, v_num=179, train_loss=2.000] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797231\n",
      "  lora_A(x) mean: 0.447106\n",
      "  lora_B(lora_A(x)) mean: 0.002556\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002556\n",
      "  output mean: 0.462833\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  86%|████████▌ | 431/500 [01:45<00:16,  4.10it/s, loss=1.3, v_num=179, train_loss=1.150]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798356\n",
      "  lora_A(x) mean: 0.440221\n",
      "  lora_B(lora_A(x)) mean: 0.002506\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002506\n",
      "  output mean: 0.462213\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  86%|████████▋ | 432/500 [01:45<00:16,  4.10it/s, loss=1.34, v_num=179, train_loss=2.000]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798386\n",
      "  lora_A(x) mean: 0.455267\n",
      "  lora_B(lora_A(x)) mean: 0.002557\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002557\n",
      "  output mean: 0.463314\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  87%|████████▋ | 433/500 [01:45<00:16,  4.10it/s, loss=1.34, v_num=179, train_loss=1.060]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797306\n",
      "  lora_A(x) mean: 0.427573\n",
      "  lora_B(lora_A(x)) mean: 0.002437\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002437\n",
      "  output mean: 0.459904\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  87%|████████▋ | 434/500 [01:45<00:16,  4.10it/s, loss=1.38, v_num=179, train_loss=2.540]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797826\n",
      "  lora_A(x) mean: 0.450376\n",
      "  lora_B(lora_A(x)) mean: 0.002588\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002588\n",
      "  output mean: 0.462958\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  87%|████████▋ | 435/500 [01:46<00:15,  4.10it/s, loss=1.38, v_num=179, train_loss=1.760]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798928\n",
      "  lora_A(x) mean: 0.457104\n",
      "  lora_B(lora_A(x)) mean: 0.002596\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002596\n",
      "  output mean: 0.463860\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  87%|████████▋ | 436/500 [01:46<00:15,  4.10it/s, loss=1.37, v_num=179, train_loss=1.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798205\n",
      "  lora_A(x) mean: 0.425516\n",
      "  lora_B(lora_A(x)) mean: 0.002442\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002442\n",
      "  output mean: 0.462540\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  87%|████████▋ | 437/500 [01:46<00:15,  4.10it/s, loss=1.41, v_num=179, train_loss=2.280]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798088\n",
      "  lora_A(x) mean: 0.445226\n",
      "  lora_B(lora_A(x)) mean: 0.002544\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002544\n",
      "  output mean: 0.461369\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  88%|████████▊ | 438/500 [01:46<00:15,  4.10it/s, loss=1.35, v_num=179, train_loss=0.588]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797540\n",
      "  lora_A(x) mean: 0.438799\n",
      "  lora_B(lora_A(x)) mean: 0.002504\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002504\n",
      "  output mean: 0.462542\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  88%|████████▊ | 439/500 [01:46<00:14,  4.10it/s, loss=1.33, v_num=179, train_loss=0.640]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797584\n",
      "  lora_A(x) mean: 0.436098\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.463833\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  88%|████████▊ | 440/500 [01:47<00:14,  4.11it/s, loss=1.3, v_num=179, train_loss=0.543] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798176\n",
      "  lora_A(x) mean: 0.430523\n",
      "  lora_B(lora_A(x)) mean: 0.002447\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002447\n",
      "  output mean: 0.460486\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  88%|████████▊ | 441/500 [01:47<00:14,  4.11it/s, loss=1.26, v_num=179, train_loss=0.668]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797552\n",
      "  lora_A(x) mean: 0.446132\n",
      "  lora_B(lora_A(x)) mean: 0.002570\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002570\n",
      "  output mean: 0.462618\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  88%|████████▊ | 442/500 [01:47<00:14,  4.11it/s, loss=1.22, v_num=179, train_loss=0.837]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797873\n",
      "  lora_A(x) mean: 0.451191\n",
      "  lora_B(lora_A(x)) mean: 0.002576\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002576\n",
      "  output mean: 0.462648\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  89%|████████▊ | 443/500 [01:47<00:13,  4.11it/s, loss=1.28, v_num=179, train_loss=2.180]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798243\n",
      "  lora_A(x) mean: 0.443757\n",
      "  lora_B(lora_A(x)) mean: 0.002474\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002474\n",
      "  output mean: 0.462906\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  89%|████████▉ | 444/500 [01:48<00:13,  4.11it/s, loss=1.36, v_num=179, train_loss=2.280]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799647\n",
      "  lora_A(x) mean: 0.438819\n",
      "  lora_B(lora_A(x)) mean: 0.002510\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002510\n",
      "  output mean: 0.461384\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  89%|████████▉ | 445/500 [01:48<00:13,  4.11it/s, loss=1.39, v_num=179, train_loss=1.100]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799447\n",
      "  lora_A(x) mean: 0.430040\n",
      "  lora_B(lora_A(x)) mean: 0.002424\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002424\n",
      "  output mean: 0.462748\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  89%|████████▉ | 446/500 [01:48<00:13,  4.11it/s, loss=1.37, v_num=179, train_loss=0.765]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797955\n",
      "  lora_A(x) mean: 0.434624\n",
      "  lora_B(lora_A(x)) mean: 0.002483\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002483\n",
      "  output mean: 0.460893\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  89%|████████▉ | 447/500 [01:48<00:12,  4.11it/s, loss=1.36, v_num=179, train_loss=0.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798924\n",
      "  lora_A(x) mean: 0.434581\n",
      "  lora_B(lora_A(x)) mean: 0.002432\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002432\n",
      "  output mean: 0.462426\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  90%|████████▉ | 448/500 [01:49<00:12,  4.11it/s, loss=1.37, v_num=179, train_loss=1.140]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799745\n",
      "  lora_A(x) mean: 0.455807\n",
      "  lora_B(lora_A(x)) mean: 0.002577\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002577\n",
      "  output mean: 0.460719\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  90%|████████▉ | 449/500 [01:49<00:12,  4.11it/s, loss=1.32, v_num=179, train_loss=0.589]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798750\n",
      "  lora_A(x) mean: 0.436678\n",
      "  lora_B(lora_A(x)) mean: 0.002514\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002514\n",
      "  output mean: 0.459864\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  90%|█████████ | 450/500 [01:49<00:12,  4.11it/s, loss=1.28, v_num=179, train_loss=1.190]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797630\n",
      "  lora_A(x) mean: 0.454027\n",
      "  lora_B(lora_A(x)) mean: 0.002573\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002573\n",
      "  output mean: 0.461696\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 450\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 0:  90%|█████████ | 451/500 [01:49<00:11,  4.11it/s, loss=1.25, v_num=179, train_loss=0.619]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798470\n",
      "  lora_A(x) mean: 0.458616\n",
      "  lora_B(lora_A(x)) mean: 0.002601\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002601\n",
      "  output mean: 0.461495\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  90%|█████████ | 452/500 [01:50<00:11,  4.11it/s, loss=1.18, v_num=179, train_loss=0.689]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799135\n",
      "  lora_A(x) mean: 0.436393\n",
      "  lora_B(lora_A(x)) mean: 0.002436\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002436\n",
      "  output mean: 0.462718\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  91%|█████████ | 453/500 [01:50<00:11,  4.11it/s, loss=1.23, v_num=179, train_loss=2.010]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798805\n",
      "  lora_A(x) mean: 0.442383\n",
      "  lora_B(lora_A(x)) mean: 0.002482\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002482\n",
      "  output mean: 0.462962\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  91%|█████████ | 454/500 [01:50<00:11,  4.11it/s, loss=1.24, v_num=179, train_loss=2.770]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798869\n",
      "  lora_A(x) mean: 0.449809\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.462931\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  91%|█████████ | 455/500 [01:50<00:10,  4.11it/s, loss=1.19, v_num=179, train_loss=0.704]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797816\n",
      "  lora_A(x) mean: 0.428018\n",
      "  lora_B(lora_A(x)) mean: 0.002438\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002438\n",
      "  output mean: 0.462809\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  91%|█████████ | 456/500 [01:50<00:10,  4.11it/s, loss=1.2, v_num=179, train_loss=1.580] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798334\n",
      "  lora_A(x) mean: 0.442533\n",
      "  lora_B(lora_A(x)) mean: 0.002519\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002519\n",
      "  output mean: 0.463181\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  91%|█████████▏| 457/500 [01:51<00:10,  4.11it/s, loss=1.13, v_num=179, train_loss=0.996]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798943\n",
      "  lora_A(x) mean: 0.454566\n",
      "  lora_B(lora_A(x)) mean: 0.002596\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002596\n",
      "  output mean: 0.462251\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  92%|█████████▏| 458/500 [01:51<00:10,  4.11it/s, loss=1.13, v_num=179, train_loss=0.615]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798292\n",
      "  lora_A(x) mean: 0.445266\n",
      "  lora_B(lora_A(x)) mean: 0.002546\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002546\n",
      "  output mean: 0.462910\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  92%|█████████▏| 459/500 [01:51<00:09,  4.11it/s, loss=1.13, v_num=179, train_loss=0.504]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797008\n",
      "  lora_A(x) mean: 0.443987\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.463795\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  92%|█████████▏| 460/500 [01:51<00:09,  4.11it/s, loss=1.17, v_num=179, train_loss=1.510]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799359\n",
      "  lora_A(x) mean: 0.421371\n",
      "  lora_B(lora_A(x)) mean: 0.002387\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002387\n",
      "  output mean: 0.461586\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  92%|█████████▏| 461/500 [01:52<00:09,  4.11it/s, loss=1.17, v_num=179, train_loss=0.644]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798352\n",
      "  lora_A(x) mean: 0.434938\n",
      "  lora_B(lora_A(x)) mean: 0.002499\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002499\n",
      "  output mean: 0.461634\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  92%|█████████▏| 462/500 [01:52<00:09,  4.11it/s, loss=1.22, v_num=179, train_loss=1.750]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799575\n",
      "  lora_A(x) mean: 0.446922\n",
      "  lora_B(lora_A(x)) mean: 0.002550\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002550\n",
      "  output mean: 0.462159\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  93%|█████████▎| 463/500 [01:52<00:09,  4.11it/s, loss=1.19, v_num=179, train_loss=1.600]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([7.9100e-11, 9.9461e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798342\n",
      "  lora_A(x) mean: 0.443025\n",
      "  lora_B(lora_A(x)) mean: 0.002543\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002543\n",
      "  output mean: 0.462428\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  93%|█████████▎| 464/500 [01:52<00:08,  4.11it/s, loss=1.16, v_num=179, train_loss=1.620]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.7646e-10, 2.3198e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798561\n",
      "  lora_A(x) mean: 0.449253\n",
      "  lora_B(lora_A(x)) mean: 0.002572\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002572\n",
      "  output mean: 0.465159\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  93%|█████████▎| 465/500 [01:53<00:08,  4.11it/s, loss=1.17, v_num=179, train_loss=1.430]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798828\n",
      "  lora_A(x) mean: 0.425065\n",
      "  lora_B(lora_A(x)) mean: 0.002390\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002390\n",
      "  output mean: 0.461214\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  93%|█████████▎| 466/500 [01:53<00:08,  4.11it/s, loss=1.18, v_num=179, train_loss=0.994]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798915\n",
      "  lora_A(x) mean: 0.440907\n",
      "  lora_B(lora_A(x)) mean: 0.002459\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002459\n",
      "  output mean: 0.462324\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  93%|█████████▎| 467/500 [01:53<00:08,  4.11it/s, loss=1.18, v_num=179, train_loss=0.560]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797734\n",
      "  lora_A(x) mean: 0.434099\n",
      "  lora_B(lora_A(x)) mean: 0.002462\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002462\n",
      "  output mean: 0.463010\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  94%|█████████▎| 468/500 [01:53<00:07,  4.11it/s, loss=1.23, v_num=179, train_loss=2.280]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798830\n",
      "  lora_A(x) mean: 0.451529\n",
      "  lora_B(lora_A(x)) mean: 0.002589\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002589\n",
      "  output mean: 0.462923\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  94%|█████████▍| 469/500 [01:54<00:07,  4.11it/s, loss=1.27, v_num=179, train_loss=1.310]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797765\n",
      "  lora_A(x) mean: 0.453069\n",
      "  lora_B(lora_A(x)) mean: 0.002559\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002559\n",
      "  output mean: 0.461301\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  94%|█████████▍| 470/500 [01:54<00:07,  4.11it/s, loss=1.27, v_num=179, train_loss=1.210]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798211\n",
      "  lora_A(x) mean: 0.458181\n",
      "  lora_B(lora_A(x)) mean: 0.002609\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002609\n",
      "  output mean: 0.460585\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  94%|█████████▍| 471/500 [01:54<00:07,  4.11it/s, loss=1.29, v_num=179, train_loss=0.998]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797902\n",
      "  lora_A(x) mean: 0.439664\n",
      "  lora_B(lora_A(x)) mean: 0.002505\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002505\n",
      "  output mean: 0.461706\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  94%|█████████▍| 472/500 [01:54<00:06,  4.11it/s, loss=1.29, v_num=179, train_loss=0.755]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798902\n",
      "  lora_A(x) mean: 0.446399\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.463380\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  95%|█████████▍| 473/500 [01:55<00:06,  4.11it/s, loss=1.23, v_num=179, train_loss=0.880]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798118\n",
      "  lora_A(x) mean: 0.442239\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.463929\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  95%|█████████▍| 474/500 [01:55<00:06,  4.11it/s, loss=1.18, v_num=179, train_loss=1.730]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798167\n",
      "  lora_A(x) mean: 0.437736\n",
      "  lora_B(lora_A(x)) mean: 0.002516\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002516\n",
      "  output mean: 0.462152\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  95%|█████████▌| 475/500 [01:55<00:06,  4.11it/s, loss=1.2, v_num=179, train_loss=0.962] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799447\n",
      "  lora_A(x) mean: 0.457781\n",
      "  lora_B(lora_A(x)) mean: 0.002630\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002630\n",
      "  output mean: 0.462934\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  95%|█████████▌| 476/500 [01:55<00:05,  4.11it/s, loss=1.2, v_num=179, train_loss=1.680]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799569\n",
      "  lora_A(x) mean: 0.451308\n",
      "  lora_B(lora_A(x)) mean: 0.002554\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002554\n",
      "  output mean: 0.464723\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  95%|█████████▌| 477/500 [01:55<00:05,  4.11it/s, loss=1.19, v_num=179, train_loss=0.840]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.795936\n",
      "  lora_A(x) mean: 0.431942\n",
      "  lora_B(lora_A(x)) mean: 0.002472\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002472\n",
      "  output mean: 0.463841\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  96%|█████████▌| 478/500 [01:56<00:05,  4.11it/s, loss=1.22, v_num=179, train_loss=1.090]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798304\n",
      "  lora_A(x) mean: 0.451459\n",
      "  lora_B(lora_A(x)) mean: 0.002564\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002564\n",
      "  output mean: 0.461312\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  96%|█████████▌| 479/500 [01:56<00:05,  4.11it/s, loss=1.31, v_num=179, train_loss=2.470]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797998\n",
      "  lora_A(x) mean: 0.438077\n",
      "  lora_B(lora_A(x)) mean: 0.002528\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002528\n",
      "  output mean: 0.462652\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  96%|█████████▌| 480/500 [01:56<00:04,  4.11it/s, loss=1.31, v_num=179, train_loss=1.490]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798693\n",
      "  lora_A(x) mean: 0.459953\n",
      "  lora_B(lora_A(x)) mean: 0.002625\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002625\n",
      "  output mean: 0.460433\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  96%|█████████▌| 481/500 [01:56<00:04,  4.11it/s, loss=1.33, v_num=179, train_loss=0.963]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798232\n",
      "  lora_A(x) mean: 0.462092\n",
      "  lora_B(lora_A(x)) mean: 0.002582\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002582\n",
      "  output mean: 0.462406\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  96%|█████████▋| 482/500 [01:57<00:04,  4.11it/s, loss=1.38, v_num=179, train_loss=2.660]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798468\n",
      "  lora_A(x) mean: 0.455382\n",
      "  lora_B(lora_A(x)) mean: 0.002572\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002572\n",
      "  output mean: 0.462691\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  97%|█████████▋| 483/500 [01:57<00:04,  4.12it/s, loss=1.34, v_num=179, train_loss=0.865]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798945\n",
      "  lora_A(x) mean: 0.437954\n",
      "  lora_B(lora_A(x)) mean: 0.002521\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002521\n",
      "  output mean: 0.462326\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  97%|█████████▋| 484/500 [01:57<00:03,  4.12it/s, loss=1.29, v_num=179, train_loss=0.606]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798654\n",
      "  lora_A(x) mean: 0.430963\n",
      "  lora_B(lora_A(x)) mean: 0.002461\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002461\n",
      "  output mean: 0.461034\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  97%|█████████▋| 485/500 [01:57<00:03,  4.12it/s, loss=1.25, v_num=179, train_loss=0.731]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797503\n",
      "  lora_A(x) mean: 0.455234\n",
      "  lora_B(lora_A(x)) mean: 0.002570\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002570\n",
      "  output mean: 0.460743\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  97%|█████████▋| 486/500 [01:58<00:03,  4.12it/s, loss=1.23, v_num=179, train_loss=0.434]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799854\n",
      "  lora_A(x) mean: 0.450517\n",
      "  lora_B(lora_A(x)) mean: 0.002548\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002548\n",
      "  output mean: 0.463867\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  97%|█████████▋| 487/500 [01:58<00:03,  4.12it/s, loss=1.3, v_num=179, train_loss=2.000] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797656\n",
      "  lora_A(x) mean: 0.450874\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.461192\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  98%|█████████▊| 488/500 [01:58<00:02,  4.12it/s, loss=1.22, v_num=179, train_loss=0.655]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798302\n",
      "  lora_A(x) mean: 0.444391\n",
      "  lora_B(lora_A(x)) mean: 0.002515\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002515\n",
      "  output mean: 0.462220\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  98%|█████████▊| 489/500 [01:58<00:02,  4.12it/s, loss=1.23, v_num=179, train_loss=1.550]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798596\n",
      "  lora_A(x) mean: 0.458095\n",
      "  lora_B(lora_A(x)) mean: 0.002552\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002552\n",
      "  output mean: 0.463310\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  98%|█████████▊| 490/500 [01:59<00:02,  4.12it/s, loss=1.27, v_num=179, train_loss=2.070]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798865\n",
      "  lora_A(x) mean: 0.449433\n",
      "  lora_B(lora_A(x)) mean: 0.002576\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002576\n",
      "  output mean: 0.462094\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  98%|█████████▊| 491/500 [01:59<00:02,  4.12it/s, loss=1.28, v_num=179, train_loss=1.190]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797081\n",
      "  lora_A(x) mean: 0.441023\n",
      "  lora_B(lora_A(x)) mean: 0.002516\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002516\n",
      "  output mean: 0.460673\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  98%|█████████▊| 492/500 [01:59<00:01,  4.12it/s, loss=1.3, v_num=179, train_loss=1.130] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797230\n",
      "  lora_A(x) mean: 0.434713\n",
      "  lora_B(lora_A(x)) mean: 0.002503\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002503\n",
      "  output mean: 0.462663\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  99%|█████████▊| 493/500 [01:59<00:01,  4.12it/s, loss=1.34, v_num=179, train_loss=1.630]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799605\n",
      "  lora_A(x) mean: 0.441396\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.459293\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  99%|█████████▉| 494/500 [01:59<00:01,  4.12it/s, loss=1.31, v_num=179, train_loss=1.190]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799232\n",
      "  lora_A(x) mean: 0.459327\n",
      "  lora_B(lora_A(x)) mean: 0.002612\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002612\n",
      "  output mean: 0.461721\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  99%|█████████▉| 495/500 [02:00<00:01,  4.12it/s, loss=1.33, v_num=179, train_loss=1.310]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3126e-11, -6.6315e-02,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796547\n",
      "  lora_A(x) mean: 0.424892\n",
      "  lora_B(lora_A(x)) mean: 0.002406\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002406\n",
      "  output mean: 0.461542\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  99%|█████████▉| 496/500 [02:00<00:00,  4.12it/s, loss=1.27, v_num=179, train_loss=0.501]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797809\n",
      "  lora_A(x) mean: 0.440267\n",
      "  lora_B(lora_A(x)) mean: 0.002516\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002516\n",
      "  output mean: 0.461181\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0:  99%|█████████▉| 497/500 [02:00<00:00,  4.12it/s, loss=1.25, v_num=179, train_loss=0.559]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798772\n",
      "  lora_A(x) mean: 0.432520\n",
      "  lora_B(lora_A(x)) mean: 0.002441\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002441\n",
      "  output mean: 0.461730\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0: 100%|█████████▉| 498/500 [02:00<00:00,  4.12it/s, loss=1.25, v_num=179, train_loss=0.969]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797167\n",
      "  lora_A(x) mean: 0.435898\n",
      "  lora_B(lora_A(x)) mean: 0.002489\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002489\n",
      "  output mean: 0.465194\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 0: 100%|█████████▉| 499/500 [02:01<00:00,  4.12it/s, loss=1.19, v_num=179, train_loss=1.270]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 7.9100e-11, -1.3262e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797985\n",
      "  lora_A(x) mean: 0.433422\n",
      "  lora_B(lora_A(x)) mean: 0.002493\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002493\n",
      "  output mean: 0.461687\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   0%|          | 0/500 [00:00<?, ?it/s, loss=1.15, v_num=179, train_loss=0.759]          get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799979\n",
      "  lora_A(x) mean: 0.460071\n",
      "  lora_B(lora_A(x)) mean: 0.002597\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002597\n",
      "  output mean: 0.462343\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799359\n",
      "  lora_A(x) mean: 0.448562\n",
      "  lora_B(lora_A(x)) mean: 0.002568\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002568\n",
      "  output mean: 0.462678\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799247\n",
      "  lora_A(x) mean: 0.449138\n",
      "  lora_B(lora_A(x)) mean: 0.002573\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002573\n",
      "  output mean: 0.462133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799114\n",
      "  lora_A(x) mean: 0.450161\n",
      "  lora_B(lora_A(x)) mean: 0.002580\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002580\n",
      "  output mean: 0.461526\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798836\n",
      "  lora_A(x) mean: 0.451075\n",
      "  lora_B(lora_A(x)) mean: 0.002588\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002588\n",
      "  output mean: 0.460893\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798595\n",
      "  lora_A(x) mean: 0.451576\n",
      "  lora_B(lora_A(x)) mean: 0.002596\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002596\n",
      "  output mean: 0.460334\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798485\n",
      "  lora_A(x) mean: 0.452848\n",
      "  lora_B(lora_A(x)) mean: 0.002603\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002603\n",
      "  output mean: 0.459875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798462\n",
      "  lora_A(x) mean: 0.454215\n",
      "  lora_B(lora_A(x)) mean: 0.002609\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002609\n",
      "  output mean: 0.459524\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798371\n",
      "  lora_A(x) mean: 0.455392\n",
      "  lora_B(lora_A(x)) mean: 0.002615\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002615\n",
      "  output mean: 0.459282\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798257\n",
      "  lora_A(x) mean: 0.456230\n",
      "  lora_B(lora_A(x)) mean: 0.002620\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002620\n",
      "  output mean: 0.459071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "DDIM Sampler:  60%|██████    | 12/20 [00:00<00:00, 31.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798164\n",
      "  lora_A(x) mean: 0.457037\n",
      "  lora_B(lora_A(x)) mean: 0.002624\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002624\n",
      "  output mean: 0.458900\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798123\n",
      "  lora_A(x) mean: 0.457670\n",
      "  lora_B(lora_A(x)) mean: 0.002628\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002628\n",
      "  output mean: 0.458707\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798108\n",
      "  lora_A(x) mean: 0.458439\n",
      "  lora_B(lora_A(x)) mean: 0.002631\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002631\n",
      "  output mean: 0.458555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798068\n",
      "  lora_A(x) mean: 0.459235\n",
      "  lora_B(lora_A(x)) mean: 0.002633\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002633\n",
      "  output mean: 0.458439\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798002\n",
      "  lora_A(x) mean: 0.459947\n",
      "  lora_B(lora_A(x)) mean: 0.002635\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002635\n",
      "  output mean: 0.458342\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797943\n",
      "  lora_A(x) mean: 0.460542\n",
      "  lora_B(lora_A(x)) mean: 0.002637\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002637\n",
      "  output mean: 0.458262\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797890\n",
      "  lora_A(x) mean: 0.461026\n",
      "  lora_B(lora_A(x)) mean: 0.002639\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002639\n",
      "  output mean: 0.458186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797838\n",
      "  lora_A(x) mean: 0.461357\n",
      "  lora_B(lora_A(x)) mean: 0.002640\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002640\n",
      "  output mean: 0.458145\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797778\n",
      "  lora_A(x) mean: 0.461713\n",
      "  lora_B(lora_A(x)) mean: 0.002641\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002641\n",
      "  output mean: 0.458119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 30.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797709\n",
      "  lora_A(x) mean: 0.461963\n",
      "  lora_B(lora_A(x)) mean: 0.002641\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002641\n",
      "  output mean: 0.458082\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797590\n",
      "  lora_A(x) mean: 0.462234\n",
      "  lora_B(lora_A(x)) mean: 0.002642\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002642\n",
      "  output mean: 0.458066\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797816\n",
      "  lora_A(x) mean: 0.449878\n",
      "  lora_B(lora_A(x)) mean: 0.002488\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002488\n",
      "  output mean: 0.458950\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797686\n",
      "  lora_A(x) mean: 0.453056\n",
      "  lora_B(lora_A(x)) mean: 0.002505\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002505\n",
      "  output mean: 0.459163\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797639\n",
      "  lora_A(x) mean: 0.455546\n",
      "  lora_B(lora_A(x)) mean: 0.002523\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002523\n",
      "  output mean: 0.459391\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797646\n",
      "  lora_A(x) mean: 0.458947\n",
      "  lora_B(lora_A(x)) mean: 0.002542\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002542\n",
      "  output mean: 0.459670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797623\n",
      "  lora_A(x) mean: 0.462369\n",
      "  lora_B(lora_A(x)) mean: 0.002560\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002560\n",
      "  output mean: 0.459998\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797586\n",
      "  lora_A(x) mean: 0.465514\n",
      "  lora_B(lora_A(x)) mean: 0.002577\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002577\n",
      "  output mean: 0.460258\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797613\n",
      "  lora_A(x) mean: 0.468793\n",
      "  lora_B(lora_A(x)) mean: 0.002592\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002592\n",
      "  output mean: 0.460470\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797634\n",
      "  lora_A(x) mean: 0.471642\n",
      "  lora_B(lora_A(x)) mean: 0.002605\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002605\n",
      "  output mean: 0.460627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797708\n",
      "  lora_A(x) mean: 0.473848\n",
      "  lora_B(lora_A(x)) mean: 0.002617\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002617\n",
      "  output mean: 0.460769\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797793\n",
      "  lora_A(x) mean: 0.475612\n",
      "  lora_B(lora_A(x)) mean: 0.002626\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002626\n",
      "  output mean: 0.460925\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797848\n",
      "  lora_A(x) mean: 0.477068\n",
      "  lora_B(lora_A(x)) mean: 0.002635\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002635\n",
      "  output mean: 0.461036\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797884\n",
      "  lora_A(x) mean: 0.478220\n",
      "  lora_B(lora_A(x)) mean: 0.002642\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002642\n",
      "  output mean: 0.461118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797928\n",
      "  lora_A(x) mean: 0.479145\n",
      "  lora_B(lora_A(x)) mean: 0.002648\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002648\n",
      "  output mean: 0.461206\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797965\n",
      "  lora_A(x) mean: 0.479921\n",
      "  lora_B(lora_A(x)) mean: 0.002653\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002653\n",
      "  output mean: 0.461272\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797998\n",
      "  lora_A(x) mean: 0.480612\n",
      "  lora_B(lora_A(x)) mean: 0.002657\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002657\n",
      "  output mean: 0.461347\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798054\n",
      "  lora_A(x) mean: 0.481198\n",
      "  lora_B(lora_A(x)) mean: 0.002661\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002661\n",
      "  output mean: 0.461401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798126\n",
      "  lora_A(x) mean: 0.481706\n",
      "  lora_B(lora_A(x)) mean: 0.002664\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002664\n",
      "  output mean: 0.461448\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798195\n",
      "  lora_A(x) mean: 0.482105\n",
      "  lora_B(lora_A(x)) mean: 0.002667\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002667\n",
      "  output mean: 0.461489\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798283\n",
      "  lora_A(x) mean: 0.482501\n",
      "  lora_B(lora_A(x)) mean: 0.002669\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002669\n",
      "  output mean: 0.461541\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798415\n",
      "  lora_A(x) mean: 0.482865\n",
      "  lora_B(lora_A(x)) mean: 0.002672\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002672\n",
      "  output mean: 0.461623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 30.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798087\n",
      "  lora_A(x) mean: 0.405393\n",
      "  lora_B(lora_A(x)) mean: 0.002344\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002344\n",
      "  output mean: 0.459525\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798054\n",
      "  lora_A(x) mean: 0.406147\n",
      "  lora_B(lora_A(x)) mean: 0.002349\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002349\n",
      "  output mean: 0.459628\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798009\n",
      "  lora_A(x) mean: 0.407211\n",
      "  lora_B(lora_A(x)) mean: 0.002356\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002356\n",
      "  output mean: 0.459761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798077\n",
      "  lora_A(x) mean: 0.408584\n",
      "  lora_B(lora_A(x)) mean: 0.002365\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002365\n",
      "  output mean: 0.459868\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798201\n",
      "  lora_A(x) mean: 0.410449\n",
      "  lora_B(lora_A(x)) mean: 0.002376\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002376\n",
      "  output mean: 0.460028\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798313\n",
      "  lora_A(x) mean: 0.412393\n",
      "  lora_B(lora_A(x)) mean: 0.002386\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002386\n",
      "  output mean: 0.460129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798407\n",
      "  lora_A(x) mean: 0.414154\n",
      "  lora_B(lora_A(x)) mean: 0.002396\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002396\n",
      "  output mean: 0.460263\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798461\n",
      "  lora_A(x) mean: 0.416028\n",
      "  lora_B(lora_A(x)) mean: 0.002406\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002406\n",
      "  output mean: 0.460374\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798518\n",
      "  lora_A(x) mean: 0.417838\n",
      "  lora_B(lora_A(x)) mean: 0.002414\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002414\n",
      "  output mean: 0.460508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798567\n",
      "  lora_A(x) mean: 0.419200\n",
      "  lora_B(lora_A(x)) mean: 0.002422\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002422\n",
      "  output mean: 0.460613\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798615\n",
      "  lora_A(x) mean: 0.420432\n",
      "  lora_B(lora_A(x)) mean: 0.002429\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002429\n",
      "  output mean: 0.460732\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798641\n",
      "  lora_A(x) mean: 0.421322\n",
      "  lora_B(lora_A(x)) mean: 0.002434\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002434\n",
      "  output mean: 0.460851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798657\n",
      "  lora_A(x) mean: 0.422099\n",
      "  lora_B(lora_A(x)) mean: 0.002439\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002439\n",
      "  output mean: 0.460953\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798663\n",
      "  lora_A(x) mean: 0.422793\n",
      "  lora_B(lora_A(x)) mean: 0.002443\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002443\n",
      "  output mean: 0.461017\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798687\n",
      "  lora_A(x) mean: 0.423351\n",
      "  lora_B(lora_A(x)) mean: 0.002447\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002447\n",
      "  output mean: 0.461092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798716\n",
      "  lora_A(x) mean: 0.423717\n",
      "  lora_B(lora_A(x)) mean: 0.002449\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002449\n",
      "  output mean: 0.461169\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798732\n",
      "  lora_A(x) mean: 0.424039\n",
      "  lora_B(lora_A(x)) mean: 0.002451\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002451\n",
      "  output mean: 0.461236\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798752\n",
      "  lora_A(x) mean: 0.424277\n",
      "  lora_B(lora_A(x)) mean: 0.002453\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002453\n",
      "  output mean: 0.461311\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798786\n",
      "  lora_A(x) mean: 0.424322\n",
      "  lora_B(lora_A(x)) mean: 0.002454\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002454\n",
      "  output mean: 0.461380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798871\n",
      "  lora_A(x) mean: 0.424093\n",
      "  lora_B(lora_A(x)) mean: 0.002455\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002455\n",
      "  output mean: 0.461521\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796329\n",
      "  lora_A(x) mean: 0.439919\n",
      "  lora_B(lora_A(x)) mean: 0.002490\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002490\n",
      "  output mean: 0.462603\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796402\n",
      "  lora_A(x) mean: 0.444177\n",
      "  lora_B(lora_A(x)) mean: 0.002506\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002506\n",
      "  output mean: 0.462631\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796511\n",
      "  lora_A(x) mean: 0.447719\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.462661\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796505\n",
      "  lora_A(x) mean: 0.451267\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.462652\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796522\n",
      "  lora_A(x) mean: 0.454876\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.462519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796488\n",
      "  lora_A(x) mean: 0.458002\n",
      "  lora_B(lora_A(x)) mean: 0.002564\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002564\n",
      "  output mean: 0.462374\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796452\n",
      "  lora_A(x) mean: 0.460264\n",
      "  lora_B(lora_A(x)) mean: 0.002576\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002576\n",
      "  output mean: 0.462248\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796344\n",
      "  lora_A(x) mean: 0.461983\n",
      "  lora_B(lora_A(x)) mean: 0.002586\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002586\n",
      "  output mean: 0.462135\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796244\n",
      "  lora_A(x) mean: 0.463294\n",
      "  lora_B(lora_A(x)) mean: 0.002595\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002595\n",
      "  output mean: 0.462075\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796166\n",
      "  lora_A(x) mean: 0.464253\n",
      "  lora_B(lora_A(x)) mean: 0.002602\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002602\n",
      "  output mean: 0.462035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796163\n",
      "  lora_A(x) mean: 0.464897\n",
      "  lora_B(lora_A(x)) mean: 0.002608\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002608\n",
      "  output mean: 0.461992\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796170\n",
      "  lora_A(x) mean: 0.465427\n",
      "  lora_B(lora_A(x)) mean: 0.002613\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002613\n",
      "  output mean: 0.461956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796175\n",
      "  lora_A(x) mean: 0.465994\n",
      "  lora_B(lora_A(x)) mean: 0.002616\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002616\n",
      "  output mean: 0.461946\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796160\n",
      "  lora_A(x) mean: 0.466450\n",
      "  lora_B(lora_A(x)) mean: 0.002619\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002619\n",
      "  output mean: 0.461938\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796128\n",
      "  lora_A(x) mean: 0.466806\n",
      "  lora_B(lora_A(x)) mean: 0.002621\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002621\n",
      "  output mean: 0.461937\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796112\n",
      "  lora_A(x) mean: 0.467007\n",
      "  lora_B(lora_A(x)) mean: 0.002623\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002623\n",
      "  output mean: 0.461937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796091\n",
      "  lora_A(x) mean: 0.467176\n",
      "  lora_B(lora_A(x)) mean: 0.002624\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002624\n",
      "  output mean: 0.461918\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796055\n",
      "  lora_A(x) mean: 0.467300\n",
      "  lora_B(lora_A(x)) mean: 0.002625\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002625\n",
      "  output mean: 0.461926\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.796023\n",
      "  lora_A(x) mean: 0.467228\n",
      "  lora_B(lora_A(x)) mean: 0.002625\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002625\n",
      "  output mean: 0.461941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:00<00:00, 29.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.795943\n",
      "  lora_A(x) mean: 0.466766\n",
      "  lora_B(lora_A(x)) mean: 0.002624\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002624\n",
      "  output mean: 0.462041\n",
      "[VIS] Logged multiview predictions at step 500\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad mean: 0.0\n",
      "\n",
      "[GRAD CHECK] Step 500\n",
      "  LoRA params (24):\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_A: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "    lora_B: norm=0.00e+00, mean=0.00e+00, max=0.00e+00\n",
      "  [WARNING] All LoRA gradients are zero or None!\n",
      "  pose_net params (4):\n",
      "    base_model.model.pose_net.0.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.0.bias: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.weight: norm=0.00e+00\n",
      "    base_model.model.pose_net.2.bias: norm=0.00e+00\n",
      "Epoch 1:   0%|          | 1/500 [00:03<25:33,  3.07s/it, loss=1.17, v_num=179, train_loss=1.320]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797603\n",
      "  lora_A(x) mean: 0.452861\n",
      "  lora_B(lora_A(x)) mean: 0.002540\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002540\n",
      "  output mean: 0.465802\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   0%|          | 2/500 [00:03<13:40,  1.65s/it, loss=1.11, v_num=179, train_loss=1.480]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798813\n",
      "  lora_A(x) mean: 0.448099\n",
      "  lora_B(lora_A(x)) mean: 0.002551\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002551\n",
      "  output mean: 0.461799\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   1%|          | 3/500 [00:03<09:43,  1.17s/it, loss=1.15, v_num=179, train_loss=1.650]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.9934e-10, 1.6574e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798285\n",
      "  lora_A(x) mean: 0.450983\n",
      "  lora_B(lora_A(x)) mean: 0.002588\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002588\n",
      "  output mean: 0.463417\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   1%|          | 4/500 [00:03<07:43,  1.07it/s, loss=1.16, v_num=179, train_loss=0.772]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([2.8058e-10, 1.9886e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799077\n",
      "  lora_A(x) mean: 0.434149\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.462320\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   1%|          | 5/500 [00:03<06:32,  1.26it/s, loss=1.15, v_num=179, train_loss=0.573]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797921\n",
      "  lora_A(x) mean: 0.453543\n",
      "  lora_B(lora_A(x)) mean: 0.002604\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002604\n",
      "  output mean: 0.462503\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   1%|          | 6/500 [00:04<05:44,  1.43it/s, loss=1.17, v_num=179, train_loss=0.866]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799059\n",
      "  lora_A(x) mean: 0.440021\n",
      "  lora_B(lora_A(x)) mean: 0.002537\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002537\n",
      "  output mean: 0.463985\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   1%|▏         | 7/500 [00:04<05:10,  1.59it/s, loss=1.17, v_num=179, train_loss=1.930]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797037\n",
      "  lora_A(x) mean: 0.430094\n",
      "  lora_B(lora_A(x)) mean: 0.002489\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002489\n",
      "  output mean: 0.462717\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   2%|▏         | 8/500 [00:04<04:44,  1.73it/s, loss=1.2, v_num=179, train_loss=1.360] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798700\n",
      "  lora_A(x) mean: 0.456193\n",
      "  lora_B(lora_A(x)) mean: 0.002575\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002575\n",
      "  output mean: 0.462095\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   2%|▏         | 9/500 [00:04<04:24,  1.85it/s, loss=1.24, v_num=179, train_loss=2.300]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798364\n",
      "  lora_A(x) mean: 0.462437\n",
      "  lora_B(lora_A(x)) mean: 0.002639\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002639\n",
      "  output mean: 0.463240\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   2%|▏         | 10/500 [00:05<04:08,  1.97it/s, loss=1.21, v_num=179, train_loss=1.450]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798304\n",
      "  lora_A(x) mean: 0.445137\n",
      "  lora_B(lora_A(x)) mean: 0.002553\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002553\n",
      "  output mean: 0.465108\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   2%|▏         | 11/500 [00:05<03:55,  2.07it/s, loss=1.19, v_num=179, train_loss=0.720]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 2.8057e-10, -2.3202e-01,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799605\n",
      "  lora_A(x) mean: 0.454079\n",
      "  lora_B(lora_A(x)) mean: 0.002558\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002558\n",
      "  output mean: 0.463920\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   2%|▏         | 12/500 [00:05<03:44,  2.17it/s, loss=1.19, v_num=179, train_loss=1.240]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797253\n",
      "  lora_A(x) mean: 0.443689\n",
      "  lora_B(lora_A(x)) mean: 0.002478\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002478\n",
      "  output mean: 0.461466\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   3%|▎         | 13/500 [00:05<03:35,  2.26it/s, loss=1.15, v_num=179, train_loss=0.809]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797938\n",
      "  lora_A(x) mean: 0.437662\n",
      "  lora_B(lora_A(x)) mean: 0.002522\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002522\n",
      "  output mean: 0.463966\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   3%|▎         | 14/500 [00:05<03:27,  2.34it/s, loss=1.17, v_num=179, train_loss=1.480]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799539\n",
      "  lora_A(x) mean: 0.450967\n",
      "  lora_B(lora_A(x)) mean: 0.002569\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002569\n",
      "  output mean: 0.460988\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   3%|▎         | 15/500 [00:06<03:20,  2.42it/s, loss=1.17, v_num=179, train_loss=1.420]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([3.9446e-11, 6.6311e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797597\n",
      "  lora_A(x) mean: 0.439201\n",
      "  lora_B(lora_A(x)) mean: 0.002489\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002489\n",
      "  output mean: 0.463058\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   3%|▎         | 16/500 [00:06<03:14,  2.49it/s, loss=1.19, v_num=179, train_loss=0.948]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798265\n",
      "  lora_A(x) mean: 0.452445\n",
      "  lora_B(lora_A(x)) mean: 0.002561\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002561\n",
      "  output mean: 0.463152\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   3%|▎         | 17/500 [00:06<03:09,  2.55it/s, loss=1.28, v_num=179, train_loss=2.230]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798116\n",
      "  lora_A(x) mean: 0.450063\n",
      "  lora_B(lora_A(x)) mean: 0.002512\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002512\n",
      "  output mean: 0.459606\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   4%|▎         | 18/500 [00:06<03:04,  2.61it/s, loss=1.29, v_num=179, train_loss=1.320]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797672\n",
      "  lora_A(x) mean: 0.440491\n",
      "  lora_B(lora_A(x)) mean: 0.002548\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002548\n",
      "  output mean: 0.462762\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   4%|▍         | 19/500 [00:07<02:59,  2.67it/s, loss=1.27, v_num=179, train_loss=0.856]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3230e-10, 1.3260e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798387\n",
      "  lora_A(x) mean: 0.449856\n",
      "  lora_B(lora_A(x)) mean: 0.002555\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002555\n",
      "  output mean: 0.462441\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   4%|▍         | 20/500 [00:07<02:55,  2.73it/s, loss=1.28, v_num=179, train_loss=0.839]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798650\n",
      "  lora_A(x) mean: 0.449384\n",
      "  lora_B(lora_A(x)) mean: 0.002530\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002530\n",
      "  output mean: 0.462773\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   4%|▍         | 21/500 [00:07<02:52,  2.78it/s, loss=1.28, v_num=179, train_loss=1.260]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.7646e-10, -2.6514e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798361\n",
      "  lora_A(x) mean: 0.442733\n",
      "  lora_B(lora_A(x)) mean: 0.002536\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002536\n",
      "  output mean: 0.463968\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   4%|▍         | 22/500 [00:07<02:49,  2.83it/s, loss=1.25, v_num=179, train_loss=0.886]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 0.0000, -0.0332,  0.0000], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798335\n",
      "  lora_A(x) mean: 0.440040\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.463761\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   5%|▍         | 23/500 [00:08<02:45,  2.87it/s, loss=1.2, v_num=179, train_loss=0.731] get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799106\n",
      "  lora_A(x) mean: 0.444262\n",
      "  lora_B(lora_A(x)) mean: 0.002543\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002543\n",
      "  output mean: 0.465934\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   5%|▍         | 24/500 [00:08<02:43,  2.92it/s, loss=1.22, v_num=179, train_loss=1.170]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 6.1445e-10, -3.3132e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798321\n",
      "  lora_A(x) mean: 0.453627\n",
      "  lora_B(lora_A(x)) mean: 0.002500\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002500\n",
      "  output mean: 0.461534\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   5%|▌         | 25/500 [00:08<02:40,  2.96it/s, loss=1.23, v_num=179, train_loss=0.764]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([1.3126e-11, 3.3157e-02, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798510\n",
      "  lora_A(x) mean: 0.434266\n",
      "  lora_B(lora_A(x)) mean: 0.002487\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002487\n",
      "  output mean: 0.462081\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   5%|▌         | 26/500 [00:08<02:38,  3.00it/s, loss=1.27, v_num=179, train_loss=1.720]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([6.1445e-10, 2.9816e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.799088\n",
      "  lora_A(x) mean: 0.460055\n",
      "  lora_B(lora_A(x)) mean: 0.002608\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002608\n",
      "  output mean: 0.463335\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   5%|▌         | 27/500 [00:08<02:35,  3.03it/s, loss=1.19, v_num=179, train_loss=0.314]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 4.8754e-10, -2.9824e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797812\n",
      "  lora_A(x) mean: 0.432270\n",
      "  lora_B(lora_A(x)) mean: 0.002466\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002466\n",
      "  output mean: 0.461361\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   6%|▌         | 28/500 [00:09<02:33,  3.07it/s, loss=1.19, v_num=179, train_loss=1.320]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798091\n",
      "  lora_A(x) mean: 0.444369\n",
      "  lora_B(lora_A(x)) mean: 0.002531\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002531\n",
      "  output mean: 0.463752\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   6%|▌         | 29/500 [00:09<02:31,  3.10it/s, loss=1.11, v_num=179, train_loss=0.772]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([4.8754e-10, 2.6508e-01, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797944\n",
      "  lora_A(x) mean: 0.444535\n",
      "  lora_B(lora_A(x)) mean: 0.002535\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002535\n",
      "  output mean: 0.462184\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   6%|▌         | 30/500 [00:09<02:30,  3.13it/s, loss=1.07, v_num=179, train_loss=0.696]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798859\n",
      "  lora_A(x) mean: 0.433380\n",
      "  lora_B(lora_A(x)) mean: 0.002474\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002474\n",
      "  output mean: 0.463378\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   6%|▌         | 31/500 [00:09<02:28,  3.16it/s, loss=1.05, v_num=179, train_loss=0.305]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.3230e-10, -1.6576e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797535\n",
      "  lora_A(x) mean: 0.444772\n",
      "  lora_B(lora_A(x)) mean: 0.002502\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002502\n",
      "  output mean: 0.460034\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   6%|▋         | 32/500 [00:10<02:26,  3.19it/s, loss=1.01, v_num=179, train_loss=0.306]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.795902\n",
      "  lora_A(x) mean: 0.440459\n",
      "  lora_B(lora_A(x)) mean: 0.002508\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002508\n",
      "  output mean: 0.462257\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   7%|▋         | 33/500 [00:10<02:25,  3.22it/s, loss=0.989, v_num=179, train_loss=0.453]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 1.9934e-10, -1.9890e-01,  0.0000e+00], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797058\n",
      "  lora_A(x) mean: 0.452251\n",
      "  lora_B(lora_A(x)) mean: 0.002570\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002570\n",
      "  output mean: 0.464497\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   7%|▋         | 34/500 [00:10<02:23,  3.24it/s, loss=0.997, v_num=179, train_loss=1.640]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([ 3.9445e-11, -9.9469e-02,  4.7684e-07], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.797123\n",
      "  lora_A(x) mean: 0.443119\n",
      "  lora_B(lora_A(x)) mean: 0.002514\n",
      "  scaling: 1.0\n",
      "  lora_contribution mean: 0.002514\n",
      "  output mean: 0.462259\n",
      "LOSS shape 1: torch.Size([4, 4, 32, 32])\n",
      "LOSS shape 2: torch.Size([])\n",
      "LOSS_SHAPE: torch.Size([])\n",
      "\n",
      "[BACKWARD HOOK] lora_B gradient received!\n",
      "  grad shape: torch.Size([1280, 16])\n",
      "  grad mean: 0.0\n",
      "Epoch 1:   7%|▋         | 35/500 [00:10<02:22,  3.27it/s, loss=0.972, v_num=179, train_loss=0.908]get_input raw control: torch.Size([4, 3, 256, 256])\n",
      "DEBUG_DELTA_POSE: tensor([0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[HOOK] base_model.model.middle_block.1.transformer_blocks.0.attn1.to_q:\n",
      "  input mean: 0.798390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = Portrait4dDataset(\n",
    "    root_dir=\"datasets/portrait4d\", transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/v2/\",\n",
    "    filename=\"toss-{step}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=500\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=25000, # 1 step = 1 batch(4) process\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "model.learning_rate = 1e-5\n",
    "model.sd_locked = True\n",
    "model.first_stage_key = \"jpg\"\n",
    "model.control_key = \"hint\"\n",
    "model.cond_stage_key = \"txt\"\n",
    "\n",
    "import torch.utils.checkpoint as cp\n",
    "cp.checkpoint = lambda func, *args, **kwargs: func(*args)\n",
    "\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# print(\"Initializing iterator...\")\n",
    "# data_iter = iter(dataloader)\n",
    "\n",
    "# print(\"Fetching first batch...\")\n",
    "# batch = next(data_iter)2\n",
    "# print(\"Success! Batch keys:\", batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10062e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.1\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "print(peft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c6d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses shape: (20, 16)\n",
      "Poses dtype: float32\n",
      "Reshaped to: (20, 4, 4)\n",
      "\n",
      "Pose[0] (view 00000):\n",
      "[[ 9.5087630e-01  3.8671235e-09  3.0957130e-01  1.2411852e+00]\n",
      " [-3.9345893e-16  1.0000000e+00 -1.2491868e-08 -5.0084495e-08]\n",
      " [-3.0957130e-01  1.1878219e-08  9.5087630e-01  4.0124130e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]\n",
      "\n",
      "Pose[9] (view 00009 - center):\n",
      "[[ 9.9986261e-01  2.0757676e-10  1.6578235e-02  6.6313364e-02]\n",
      " [-5.3748079e-18  1.0000000e+00 -1.2521043e-08 -5.0084495e-08]\n",
      " [-1.6578235e-02  1.2519321e-08  9.9986261e-01  4.1994762e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]\n",
      "\n",
      "Rotation matrix R (from pose[0]):\n",
      "[[ 9.5087630e-01  3.8671235e-09  3.0957130e-01]\n",
      " [-3.9345893e-16  1.0000000e+00 -1.2491868e-08]\n",
      " [-3.0957130e-01  1.1878219e-08  9.5087630e-01]]\n",
      "det(R) = 1.000000 (should be ~1.0 for valid rotation)\n",
      "R @ R.T =\n",
      "[[ 1.0000001e+00 -8.8817842e-16  0.0000000e+00]\n",
      " [-8.8817842e-16  1.0000000e+00 -1.7763568e-15]\n",
      " [ 0.0000000e+00 -1.7763568e-15  1.0000001e+00]] (should be ~identity)\n",
      "\n",
      "Extracted angles from pose[0]:\n",
      "  pitch = 0.0000 rad = 0.00°\n",
      "  yaw = 0.3147 rad = 18.03°\n",
      "\n",
      "All yaw values (degrees):\n",
      "  View 00: pitch=  +0.00°, yaw= +18.03°\n",
      "  View 01: pitch=  +0.00°, yaw= +16.14°\n",
      "  View 02: pitch=  +0.00°, yaw= +14.24°\n",
      "  View 03: pitch=  +0.00°, yaw= +12.34°\n",
      "  View 04: pitch=  +0.00°, yaw= +10.45°\n",
      "  View 05: pitch=  +0.00°, yaw=  +8.55°\n",
      "  View 06: pitch=  +0.00°, yaw=  +6.65°\n",
      "  View 07: pitch=  +0.00°, yaw=  +4.75°\n",
      "  View 08: pitch=  +0.00°, yaw=  +2.85°\n",
      "  View 09: pitch=  +0.00°, yaw=  +0.95°\n",
      "  View 10: pitch=  +0.00°, yaw=  -0.95°\n",
      "  View 11: pitch=  +0.00°, yaw=  -2.85°\n",
      "  View 12: pitch=  +0.00°, yaw=  -4.75°\n",
      "  View 13: pitch=  +0.00°, yaw=  -6.65°\n",
      "  View 14: pitch=  +0.00°, yaw=  -8.55°\n",
      "  View 15: pitch=  +0.00°, yaw= -10.45°\n",
      "  View 16: pitch=  +0.00°, yaw= -12.34°\n",
      "  View 17: pitch=  +0.00°, yaw= -14.24°\n",
      "  View 18: pitch=  +0.00°, yaw= -16.14°\n",
      "  View 19: pitch=  +0.00°, yaw= -18.03°\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check poses.npy format and values\n",
    "import numpy as np\n",
    "\n",
    "sub_path = \"datasets/portrait4d/202483\"\n",
    "poses = np.load(f\"{sub_path}/poses.npy\")\n",
    "\n",
    "print(f\"Poses shape: {poses.shape}\")\n",
    "print(f\"Poses dtype: {poses.dtype}\")\n",
    "\n",
    "# Reshape if needed\n",
    "if poses.ndim == 2 and poses.shape[1] == 16:\n",
    "    poses = poses.reshape(-1, 4, 4)\n",
    "    print(f\"Reshaped to: {poses.shape}\")\n",
    "\n",
    "# Check a single pose matrix\n",
    "print(f\"\\nPose[0] (view 00000):\\n{poses[0]}\")\n",
    "print(f\"\\nPose[9] (view 00009 - center):\\n{poses[9]}\")\n",
    "\n",
    "# Check if it looks like a valid rotation matrix\n",
    "R = poses[0][:3, :3]\n",
    "print(f\"\\nRotation matrix R (from pose[0]):\\n{R}\")\n",
    "print(f\"det(R) = {np.linalg.det(R):.6f} (should be ~1.0 for valid rotation)\")\n",
    "print(f\"R @ R.T =\\n{R @ R.T} (should be ~identity)\")\n",
    "\n",
    "# Extract angles using your function\n",
    "pitch, yaw = rotation_matrix_to_euler(R)\n",
    "print(f\"\\nExtracted angles from pose[0]:\")\n",
    "print(f\"  pitch = {pitch:.4f} rad = {np.degrees(pitch):.2f}°\")\n",
    "print(f\"  yaw = {yaw:.4f} rad = {np.degrees(yaw):.2f}°\")\n",
    "\n",
    "# Check all poses' yaw values\n",
    "print(f\"\\nAll yaw values (degrees):\")\n",
    "for i in range(min(20, len(poses))):\n",
    "    p, y = rotation_matrix_to_euler(poses[i][:3, :3])\n",
    "    print(f\"  View {i:02d}: pitch={np.degrees(p):+7.2f}°, yaw={np.degrees(y):+7.2f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ad55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\lightning_fabric\\__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "TOSS: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 863.65 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['model_ema.decay', 'model_ema.num_updates', 'model_ema.diffusion_modeltime_embed0weight', 'model_ema.diffusion_modeltime_embed0bias', 'model_ema.diffusion_modeltime_embed2weight', 'model_ema.diffusion_modeltime_embed2bias', 'model_ema.diffusion_modelinput_blocks00weight', 'model_ema.diffusion_modelinput_blocks00bias', 'model_ema.diffusion_modelinput_blocks10in_layers0weight', 'model_ema.diffusion_modelinput_blocks10in_layers0bias', 'model_ema.diffusion_modelinput_blocks10in_layers2weight', 'model_ema.diffusion_modelinput_blocks10in_layers2bias', 'model_ema.diffusion_modelinput_blocks10emb_layers1weight', 'model_ema.diffusion_modelinput_blocks10emb_layers1bias', 'model_ema.diffusion_modelinput_blocks10out_layers0weight', 'model_ema.diffusion_modelinput_blocks10out_layers0bias', 'model_ema.diffusion_modelinput_blocks10out_layers3weight', 'model_ema.diffusion_modelinput_blocks10out_layers3bias', 'model_ema.diffusion_modelinput_blocks11normweight', 'model_ema.diffusion_modelinput_blocks11normbias', 'model_ema.diffusion_modelinput_blocks11proj_inweight', 'model_ema.diffusion_modelinput_blocks11proj_inbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks11transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks11proj_outweight', 'model_ema.diffusion_modelinput_blocks11proj_outbias', 'model_ema.diffusion_modelinput_blocks20in_layers0weight', 'model_ema.diffusion_modelinput_blocks20in_layers0bias', 'model_ema.diffusion_modelinput_blocks20in_layers2weight', 'model_ema.diffusion_modelinput_blocks20in_layers2bias', 'model_ema.diffusion_modelinput_blocks20emb_layers1weight', 'model_ema.diffusion_modelinput_blocks20emb_layers1bias', 'model_ema.diffusion_modelinput_blocks20out_layers0weight', 'model_ema.diffusion_modelinput_blocks20out_layers0bias', 'model_ema.diffusion_modelinput_blocks20out_layers3weight', 'model_ema.diffusion_modelinput_blocks20out_layers3bias', 'model_ema.diffusion_modelinput_blocks21normweight', 'model_ema.diffusion_modelinput_blocks21normbias', 'model_ema.diffusion_modelinput_blocks21proj_inweight', 'model_ema.diffusion_modelinput_blocks21proj_inbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks21transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks21proj_outweight', 'model_ema.diffusion_modelinput_blocks21proj_outbias', 'model_ema.diffusion_modelinput_blocks30opweight', 'model_ema.diffusion_modelinput_blocks30opbias', 'model_ema.diffusion_modelinput_blocks40in_layers0weight', 'model_ema.diffusion_modelinput_blocks40in_layers0bias', 'model_ema.diffusion_modelinput_blocks40in_layers2weight', 'model_ema.diffusion_modelinput_blocks40in_layers2bias', 'model_ema.diffusion_modelinput_blocks40emb_layers1weight', 'model_ema.diffusion_modelinput_blocks40emb_layers1bias', 'model_ema.diffusion_modelinput_blocks40out_layers0weight', 'model_ema.diffusion_modelinput_blocks40out_layers0bias', 'model_ema.diffusion_modelinput_blocks40out_layers3weight', 'model_ema.diffusion_modelinput_blocks40out_layers3bias', 'model_ema.diffusion_modelinput_blocks40skip_connectionweight', 'model_ema.diffusion_modelinput_blocks40skip_connectionbias', 'model_ema.diffusion_modelinput_blocks41normweight', 'model_ema.diffusion_modelinput_blocks41normbias', 'model_ema.diffusion_modelinput_blocks41proj_inweight', 'model_ema.diffusion_modelinput_blocks41proj_inbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks41transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks41proj_outweight', 'model_ema.diffusion_modelinput_blocks41proj_outbias', 'model_ema.diffusion_modelinput_blocks50in_layers0weight', 'model_ema.diffusion_modelinput_blocks50in_layers0bias', 'model_ema.diffusion_modelinput_blocks50in_layers2weight', 'model_ema.diffusion_modelinput_blocks50in_layers2bias', 'model_ema.diffusion_modelinput_blocks50emb_layers1weight', 'model_ema.diffusion_modelinput_blocks50emb_layers1bias', 'model_ema.diffusion_modelinput_blocks50out_layers0weight', 'model_ema.diffusion_modelinput_blocks50out_layers0bias', 'model_ema.diffusion_modelinput_blocks50out_layers3weight', 'model_ema.diffusion_modelinput_blocks50out_layers3bias', 'model_ema.diffusion_modelinput_blocks51normweight', 'model_ema.diffusion_modelinput_blocks51normbias', 'model_ema.diffusion_modelinput_blocks51proj_inweight', 'model_ema.diffusion_modelinput_blocks51proj_inbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks51transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks51proj_outweight', 'model_ema.diffusion_modelinput_blocks51proj_outbias', 'model_ema.diffusion_modelinput_blocks60opweight', 'model_ema.diffusion_modelinput_blocks60opbias', 'model_ema.diffusion_modelinput_blocks70in_layers0weight', 'model_ema.diffusion_modelinput_blocks70in_layers0bias', 'model_ema.diffusion_modelinput_blocks70in_layers2weight', 'model_ema.diffusion_modelinput_blocks70in_layers2bias', 'model_ema.diffusion_modelinput_blocks70emb_layers1weight', 'model_ema.diffusion_modelinput_blocks70emb_layers1bias', 'model_ema.diffusion_modelinput_blocks70out_layers0weight', 'model_ema.diffusion_modelinput_blocks70out_layers0bias', 'model_ema.diffusion_modelinput_blocks70out_layers3weight', 'model_ema.diffusion_modelinput_blocks70out_layers3bias', 'model_ema.diffusion_modelinput_blocks70skip_connectionweight', 'model_ema.diffusion_modelinput_blocks70skip_connectionbias', 'model_ema.diffusion_modelinput_blocks71normweight', 'model_ema.diffusion_modelinput_blocks71normbias', 'model_ema.diffusion_modelinput_blocks71proj_inweight', 'model_ema.diffusion_modelinput_blocks71proj_inbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks71transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks71proj_outweight', 'model_ema.diffusion_modelinput_blocks71proj_outbias', 'model_ema.diffusion_modelinput_blocks80in_layers0weight', 'model_ema.diffusion_modelinput_blocks80in_layers0bias', 'model_ema.diffusion_modelinput_blocks80in_layers2weight', 'model_ema.diffusion_modelinput_blocks80in_layers2bias', 'model_ema.diffusion_modelinput_blocks80emb_layers1weight', 'model_ema.diffusion_modelinput_blocks80emb_layers1bias', 'model_ema.diffusion_modelinput_blocks80out_layers0weight', 'model_ema.diffusion_modelinput_blocks80out_layers0bias', 'model_ema.diffusion_modelinput_blocks80out_layers3weight', 'model_ema.diffusion_modelinput_blocks80out_layers3bias', 'model_ema.diffusion_modelinput_blocks81normweight', 'model_ema.diffusion_modelinput_blocks81normbias', 'model_ema.diffusion_modelinput_blocks81proj_inweight', 'model_ema.diffusion_modelinput_blocks81proj_inbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0vae_projweight', 'model_ema.diffusion_modelinput_blocks81transformer_blocks0vae_projbias', 'model_ema.diffusion_modelinput_blocks81proj_outweight', 'model_ema.diffusion_modelinput_blocks81proj_outbias', 'model_ema.diffusion_modelinput_blocks90opweight', 'model_ema.diffusion_modelinput_blocks90opbias', 'model_ema.diffusion_modelinput_blocks100in_layers0weight', 'model_ema.diffusion_modelinput_blocks100in_layers0bias', 'model_ema.diffusion_modelinput_blocks100in_layers2weight', 'model_ema.diffusion_modelinput_blocks100in_layers2bias', 'model_ema.diffusion_modelinput_blocks100emb_layers1weight', 'model_ema.diffusion_modelinput_blocks100emb_layers1bias', 'model_ema.diffusion_modelinput_blocks100out_layers0weight', 'model_ema.diffusion_modelinput_blocks100out_layers0bias', 'model_ema.diffusion_modelinput_blocks100out_layers3weight', 'model_ema.diffusion_modelinput_blocks100out_layers3bias', 'model_ema.diffusion_modelinput_blocks110in_layers0weight', 'model_ema.diffusion_modelinput_blocks110in_layers0bias', 'model_ema.diffusion_modelinput_blocks110in_layers2weight', 'model_ema.diffusion_modelinput_blocks110in_layers2bias', 'model_ema.diffusion_modelinput_blocks110emb_layers1weight', 'model_ema.diffusion_modelinput_blocks110emb_layers1bias', 'model_ema.diffusion_modelinput_blocks110out_layers0weight', 'model_ema.diffusion_modelinput_blocks110out_layers0bias', 'model_ema.diffusion_modelinput_blocks110out_layers3weight', 'model_ema.diffusion_modelinput_blocks110out_layers3bias', 'model_ema.diffusion_modelmiddle_block0in_layers0weight', 'model_ema.diffusion_modelmiddle_block0in_layers0bias', 'model_ema.diffusion_modelmiddle_block0in_layers2weight', 'model_ema.diffusion_modelmiddle_block0in_layers2bias', 'model_ema.diffusion_modelmiddle_block0emb_layers1weight', 'model_ema.diffusion_modelmiddle_block0emb_layers1bias', 'model_ema.diffusion_modelmiddle_block0out_layers0weight', 'model_ema.diffusion_modelmiddle_block0out_layers0bias', 'model_ema.diffusion_modelmiddle_block0out_layers3weight', 'model_ema.diffusion_modelmiddle_block0out_layers3bias', 'model_ema.diffusion_modelmiddle_block1normweight', 'model_ema.diffusion_modelmiddle_block1normbias', 'model_ema.diffusion_modelmiddle_block1proj_inweight', 'model_ema.diffusion_modelmiddle_block1proj_inbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0ffnet2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm1bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm2bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3weight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0norm3bias', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0vae_projweight', 'model_ema.diffusion_modelmiddle_block1transformer_blocks0vae_projbias', 'model_ema.diffusion_modelmiddle_block1proj_outweight', 'model_ema.diffusion_modelmiddle_block1proj_outbias', 'model_ema.diffusion_modelmiddle_block2in_layers0weight', 'model_ema.diffusion_modelmiddle_block2in_layers0bias', 'model_ema.diffusion_modelmiddle_block2in_layers2weight', 'model_ema.diffusion_modelmiddle_block2in_layers2bias', 'model_ema.diffusion_modelmiddle_block2emb_layers1weight', 'model_ema.diffusion_modelmiddle_block2emb_layers1bias', 'model_ema.diffusion_modelmiddle_block2out_layers0weight', 'model_ema.diffusion_modelmiddle_block2out_layers0bias', 'model_ema.diffusion_modelmiddle_block2out_layers3weight', 'model_ema.diffusion_modelmiddle_block2out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00in_layers0weight', 'model_ema.diffusion_modeloutput_blocks00in_layers0bias', 'model_ema.diffusion_modeloutput_blocks00in_layers2weight', 'model_ema.diffusion_modeloutput_blocks00in_layers2bias', 'model_ema.diffusion_modeloutput_blocks00emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks00emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks00out_layers0weight', 'model_ema.diffusion_modeloutput_blocks00out_layers0bias', 'model_ema.diffusion_modeloutput_blocks00out_layers3weight', 'model_ema.diffusion_modeloutput_blocks00out_layers3bias', 'model_ema.diffusion_modeloutput_blocks00skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks00skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks10in_layers0weight', 'model_ema.diffusion_modeloutput_blocks10in_layers0bias', 'model_ema.diffusion_modeloutput_blocks10in_layers2weight', 'model_ema.diffusion_modeloutput_blocks10in_layers2bias', 'model_ema.diffusion_modeloutput_blocks10emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks10emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks10out_layers0weight', 'model_ema.diffusion_modeloutput_blocks10out_layers0bias', 'model_ema.diffusion_modeloutput_blocks10out_layers3weight', 'model_ema.diffusion_modeloutput_blocks10out_layers3bias', 'model_ema.diffusion_modeloutput_blocks10skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks10skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks20in_layers0weight', 'model_ema.diffusion_modeloutput_blocks20in_layers0bias', 'model_ema.diffusion_modeloutput_blocks20in_layers2weight', 'model_ema.diffusion_modeloutput_blocks20in_layers2bias', 'model_ema.diffusion_modeloutput_blocks20emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks20emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks20out_layers0weight', 'model_ema.diffusion_modeloutput_blocks20out_layers0bias', 'model_ema.diffusion_modeloutput_blocks20out_layers3weight', 'model_ema.diffusion_modeloutput_blocks20out_layers3bias', 'model_ema.diffusion_modeloutput_blocks20skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks20skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks21convweight', 'model_ema.diffusion_modeloutput_blocks21convbias', 'model_ema.diffusion_modeloutput_blocks30in_layers0weight', 'model_ema.diffusion_modeloutput_blocks30in_layers0bias', 'model_ema.diffusion_modeloutput_blocks30in_layers2weight', 'model_ema.diffusion_modeloutput_blocks30in_layers2bias', 'model_ema.diffusion_modeloutput_blocks30emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks30emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks30out_layers0weight', 'model_ema.diffusion_modeloutput_blocks30out_layers0bias', 'model_ema.diffusion_modeloutput_blocks30out_layers3weight', 'model_ema.diffusion_modeloutput_blocks30out_layers3bias', 'model_ema.diffusion_modeloutput_blocks30skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks30skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks31normweight', 'model_ema.diffusion_modeloutput_blocks31normbias', 'model_ema.diffusion_modeloutput_blocks31proj_inweight', 'model_ema.diffusion_modeloutput_blocks31proj_inbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks31transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks31proj_outweight', 'model_ema.diffusion_modeloutput_blocks31proj_outbias', 'model_ema.diffusion_modeloutput_blocks40in_layers0weight', 'model_ema.diffusion_modeloutput_blocks40in_layers0bias', 'model_ema.diffusion_modeloutput_blocks40in_layers2weight', 'model_ema.diffusion_modeloutput_blocks40in_layers2bias', 'model_ema.diffusion_modeloutput_blocks40emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks40emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks40out_layers0weight', 'model_ema.diffusion_modeloutput_blocks40out_layers0bias', 'model_ema.diffusion_modeloutput_blocks40out_layers3weight', 'model_ema.diffusion_modeloutput_blocks40out_layers3bias', 'model_ema.diffusion_modeloutput_blocks40skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks40skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks41normweight', 'model_ema.diffusion_modeloutput_blocks41normbias', 'model_ema.diffusion_modeloutput_blocks41proj_inweight', 'model_ema.diffusion_modeloutput_blocks41proj_inbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks41transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks41proj_outweight', 'model_ema.diffusion_modeloutput_blocks41proj_outbias', 'model_ema.diffusion_modeloutput_blocks50in_layers0weight', 'model_ema.diffusion_modeloutput_blocks50in_layers0bias', 'model_ema.diffusion_modeloutput_blocks50in_layers2weight', 'model_ema.diffusion_modeloutput_blocks50in_layers2bias', 'model_ema.diffusion_modeloutput_blocks50emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks50emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks50out_layers0weight', 'model_ema.diffusion_modeloutput_blocks50out_layers0bias', 'model_ema.diffusion_modeloutput_blocks50out_layers3weight', 'model_ema.diffusion_modeloutput_blocks50out_layers3bias', 'model_ema.diffusion_modeloutput_blocks50skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks50skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks51normweight', 'model_ema.diffusion_modeloutput_blocks51normbias', 'model_ema.diffusion_modeloutput_blocks51proj_inweight', 'model_ema.diffusion_modeloutput_blocks51proj_inbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks51transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks51proj_outweight', 'model_ema.diffusion_modeloutput_blocks51proj_outbias', 'model_ema.diffusion_modeloutput_blocks52convweight', 'model_ema.diffusion_modeloutput_blocks52convbias', 'model_ema.diffusion_modeloutput_blocks60in_layers0weight', 'model_ema.diffusion_modeloutput_blocks60in_layers0bias', 'model_ema.diffusion_modeloutput_blocks60in_layers2weight', 'model_ema.diffusion_modeloutput_blocks60in_layers2bias', 'model_ema.diffusion_modeloutput_blocks60emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks60emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks60out_layers0weight', 'model_ema.diffusion_modeloutput_blocks60out_layers0bias', 'model_ema.diffusion_modeloutput_blocks60out_layers3weight', 'model_ema.diffusion_modeloutput_blocks60out_layers3bias', 'model_ema.diffusion_modeloutput_blocks60skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks60skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks61normweight', 'model_ema.diffusion_modeloutput_blocks61normbias', 'model_ema.diffusion_modeloutput_blocks61proj_inweight', 'model_ema.diffusion_modeloutput_blocks61proj_inbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks61transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks61proj_outweight', 'model_ema.diffusion_modeloutput_blocks61proj_outbias', 'model_ema.diffusion_modeloutput_blocks70in_layers0weight', 'model_ema.diffusion_modeloutput_blocks70in_layers0bias', 'model_ema.diffusion_modeloutput_blocks70in_layers2weight', 'model_ema.diffusion_modeloutput_blocks70in_layers2bias', 'model_ema.diffusion_modeloutput_blocks70emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks70emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks70out_layers0weight', 'model_ema.diffusion_modeloutput_blocks70out_layers0bias', 'model_ema.diffusion_modeloutput_blocks70out_layers3weight', 'model_ema.diffusion_modeloutput_blocks70out_layers3bias', 'model_ema.diffusion_modeloutput_blocks70skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks70skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks71normweight', 'model_ema.diffusion_modeloutput_blocks71normbias', 'model_ema.diffusion_modeloutput_blocks71proj_inweight', 'model_ema.diffusion_modeloutput_blocks71proj_inbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks71transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks71proj_outweight', 'model_ema.diffusion_modeloutput_blocks71proj_outbias', 'model_ema.diffusion_modeloutput_blocks80in_layers0weight', 'model_ema.diffusion_modeloutput_blocks80in_layers0bias', 'model_ema.diffusion_modeloutput_blocks80in_layers2weight', 'model_ema.diffusion_modeloutput_blocks80in_layers2bias', 'model_ema.diffusion_modeloutput_blocks80emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks80emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks80out_layers0weight', 'model_ema.diffusion_modeloutput_blocks80out_layers0bias', 'model_ema.diffusion_modeloutput_blocks80out_layers3weight', 'model_ema.diffusion_modeloutput_blocks80out_layers3bias', 'model_ema.diffusion_modeloutput_blocks80skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks80skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks81normweight', 'model_ema.diffusion_modeloutput_blocks81normbias', 'model_ema.diffusion_modeloutput_blocks81proj_inweight', 'model_ema.diffusion_modeloutput_blocks81proj_inbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks81transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks81proj_outweight', 'model_ema.diffusion_modeloutput_blocks81proj_outbias', 'model_ema.diffusion_modeloutput_blocks82convweight', 'model_ema.diffusion_modeloutput_blocks82convbias', 'model_ema.diffusion_modeloutput_blocks90in_layers0weight', 'model_ema.diffusion_modeloutput_blocks90in_layers0bias', 'model_ema.diffusion_modeloutput_blocks90in_layers2weight', 'model_ema.diffusion_modeloutput_blocks90in_layers2bias', 'model_ema.diffusion_modeloutput_blocks90emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks90emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks90out_layers0weight', 'model_ema.diffusion_modeloutput_blocks90out_layers0bias', 'model_ema.diffusion_modeloutput_blocks90out_layers3weight', 'model_ema.diffusion_modeloutput_blocks90out_layers3bias', 'model_ema.diffusion_modeloutput_blocks90skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks90skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks91normweight', 'model_ema.diffusion_modeloutput_blocks91normbias', 'model_ema.diffusion_modeloutput_blocks91proj_inweight', 'model_ema.diffusion_modeloutput_blocks91proj_inbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks91transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks91proj_outweight', 'model_ema.diffusion_modeloutput_blocks91proj_outbias', 'model_ema.diffusion_modeloutput_blocks100in_layers0weight', 'model_ema.diffusion_modeloutput_blocks100in_layers0bias', 'model_ema.diffusion_modeloutput_blocks100in_layers2weight', 'model_ema.diffusion_modeloutput_blocks100in_layers2bias', 'model_ema.diffusion_modeloutput_blocks100emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks100emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks100out_layers0weight', 'model_ema.diffusion_modeloutput_blocks100out_layers0bias', 'model_ema.diffusion_modeloutput_blocks100out_layers3weight', 'model_ema.diffusion_modeloutput_blocks100out_layers3bias', 'model_ema.diffusion_modeloutput_blocks100skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks100skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks101normweight', 'model_ema.diffusion_modeloutput_blocks101normbias', 'model_ema.diffusion_modeloutput_blocks101proj_inweight', 'model_ema.diffusion_modeloutput_blocks101proj_inbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks101transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks101proj_outweight', 'model_ema.diffusion_modeloutput_blocks101proj_outbias', 'model_ema.diffusion_modeloutput_blocks110in_layers0weight', 'model_ema.diffusion_modeloutput_blocks110in_layers0bias', 'model_ema.diffusion_modeloutput_blocks110in_layers2weight', 'model_ema.diffusion_modeloutput_blocks110in_layers2bias', 'model_ema.diffusion_modeloutput_blocks110emb_layers1weight', 'model_ema.diffusion_modeloutput_blocks110emb_layers1bias', 'model_ema.diffusion_modeloutput_blocks110out_layers0weight', 'model_ema.diffusion_modeloutput_blocks110out_layers0bias', 'model_ema.diffusion_modeloutput_blocks110out_layers3weight', 'model_ema.diffusion_modeloutput_blocks110out_layers3bias', 'model_ema.diffusion_modeloutput_blocks110skip_connectionweight', 'model_ema.diffusion_modeloutput_blocks110skip_connectionbias', 'model_ema.diffusion_modeloutput_blocks111normweight', 'model_ema.diffusion_modeloutput_blocks111normbias', 'model_ema.diffusion_modeloutput_blocks111proj_inweight', 'model_ema.diffusion_modeloutput_blocks111proj_inbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn1to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet0projbias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0ffnet2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_qweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_kweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_vweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0attn2to_out0bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm1bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm2bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3weight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0norm3bias', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0vae_projweight', 'model_ema.diffusion_modeloutput_blocks111transformer_blocks0vae_projbias', 'model_ema.diffusion_modeloutput_blocks111proj_outweight', 'model_ema.diffusion_modeloutput_blocks111proj_outbias', 'model_ema.diffusion_modelout0weight', 'model_ema.diffusion_modelout0bias', 'model_ema.diffusion_modelout2weight', 'model_ema.diffusion_modelout2bias', 'model_ema.diffusion_modelpose_net0weight', 'model_ema.diffusion_modelpose_net0bias', 'model_ema.diffusion_modelpose_net2weight', 'model_ema.diffusion_modelpose_net2bias', 'cond_stage_model.transformer.text_model.embeddings.position_ids'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model (TOSS original)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the last checkpoint\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "state_dict = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "\n",
    "# Remove the old pose_net so your new one can take its place\n",
    "# keys_to_remove = [k for k in state_dict.keys() if \"pose_net\" in k]\n",
    "# for k in keys_to_remove:\n",
    "#     del state_dict[k]\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# trained_ckpt = torch.load(\"checkpoints/v2/last-v1.ckpt\", map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# This will inject your LoRA weights and your trained PoseNet\n",
    "# m, u = model.load_state_dict(trained_ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "# print(\"Unexpected keys (should be empty):\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4617aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\lightning_fabric\\__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\.conda\\envs\\toss\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from C:\\Users\\user\\_netrc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwndyd0131\u001b[0m (\u001b[33mwndyd0131-sungkyunkwan-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Documents\\JY\\TOSS\\wandb\\run-20260201_105115-t021fv5a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project/runs/t021fv5a' target=\"_blank\">proud-field-7</a></strong> to <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project/runs/t021fv5a' target=\"_blank\">https://wandb.ai/wndyd0131-sungkyunkwan-university/my-awesome-project/runs/t021fv5a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TossLoraModule: Running in eps-prediction mode\n",
      "DiffusionWrapper has 863.65 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\n",
      "[INIT] Fixing zero-initialized lora_B: base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\n",
      "[INIT] Enabled requires_grad for 12 LoRA parameters\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight: shape=torch.Size([16, 768]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight: shape=torch.Size([16, 768]), requires_grad=True\n",
      "  -> base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight: shape=torch.Size([16, 768]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight: shape=torch.Size([16, 768]), requires_grad=True\n",
      "  -> base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "trainable params: 225,156 || all params: 863,859,396 || trainable%: 0.0261\n",
      "[INIT] PEFT config active: {'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules=['middle_block.1.transformer_blocks.0.attn2.to_q', 'middle_block.1.transformer_blocks.0.attn2.to_k', 'middle_block.1.transformer_blocks.0.attn2.to_v', 'output_blocks.3.1.transformer_blocks.0.attn2.to_q', 'output_blocks.3.1.transformer_blocks.0.attn2.to_k', 'output_blocks.3.1.transformer_blocks.0.attn2.to_v'], exclude_modules=None, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n",
      "\n",
      "[DEBUG] LoRA Layer Analysis:\n",
      "  base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q:\n",
      "    scaling: {'default': 1.0}\n",
      "    disable_adapters: False\n",
      "    merged: False\n",
      "    lora_A[default]: shape=torch.Size([16, 1280]), requires_grad=True\n",
      "    lora_B[default]: shape=torch.Size([1280, 16]), requires_grad=True\n",
      "\n",
      "[DEBUG] Active adapter: default\n",
      "[DEBUG] Active adapters: ['default']\n",
      "True\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight 0.014037154614925385\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight 8.042696890697698e-07\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight 0.01804303750395775\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight 7.980265763762873e-07\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight 0.018023831769824028\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight 8.022669817364658e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight 0.013958360068500042\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight 7.921534006527509e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight 0.018192728981375694\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight 7.979955398695893e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight 0.018136894330382347\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight 8.039267527237826e-07\n",
      "model.diffusion_model.base_model.model.out.0.weight 1.0\n",
      "model.diffusion_model.base_model.model.out.0.bias 0.0\n",
      "model.diffusion_model.base_model.model.out.2.weight 0.0\n",
      "model.diffusion_model.base_model.model.out.2.bias 0.0\n",
      "Found 12 LoRA keys in checkpoint:\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight: 0.013943\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight: 0.000001\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight: 0.018114\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight: 0.000001\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight: 0.017908\n",
      "  model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight: 0.000001\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight: 0.013972\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight: 0.000001\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight: 0.018125\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight: 0.000001\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight: 0.018128\n",
      "  model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight: 0.000001\n",
      "\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight 0.013942793011665344\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight 8.086264529083564e-07\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight 0.0181143656373024\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight 7.988072070475027e-07\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight 0.017907753586769104\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight 8.004552114471153e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight 0.013972143642604351\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight 7.977511131684878e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight 0.018125459551811218\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight 7.975280027494591e-07\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight 0.018128393217921257\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight 7.929428420538898e-07\n",
      "model.diffusion_model.base_model.model.out.0.weight 1.0099819898605347\n",
      "model.diffusion_model.base_model.model.out.0.bias 0.012702658772468567\n",
      "model.diffusion_model.base_model.model.out.2.weight 0.0046215602196753025\n",
      "model.diffusion_model.base_model.model.out.2.bias 0.003563903272151947\n",
      "Missing: []\n",
      "Unexpected keys (should be empty): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# Load Model (Inference)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the last checkpoint\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.toss_lora import TossLoraModule\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "print(isinstance(model, TossLoraModule))\n",
    "\n",
    "state_dict = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "\n",
    "# Remove the old pose_net so your new one can take its place\n",
    "# keys_to_remove = [k for k in state_dict.keys() if \"pose_net\" in k]\n",
    "# for k in keys_to_remove:\n",
    "#     del state_dict[k]\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "   if \"lora\" in n.lower() or \".out.\" in n.lower():\n",
    "       print(n, p.abs().mean().item())\n",
    "\n",
    "\n",
    "trained_ckpt = torch.load(\"checkpoints/v2/last-v7.ckpt\", map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "lora_keys = [k for k in trained_ckpt[\"state_dict\"].keys() if \"lora\" in k.lower()]\n",
    "print(f\"Found {len(lora_keys)} LoRA keys in checkpoint:\")\n",
    "for k in lora_keys:\n",
    "    print(f\"  {k}: {trained_ckpt['state_dict'][k].abs().mean().item():.6f}\")\n",
    "\n",
    "# This will inject your LoRA weights and your trained PoseNet\n",
    "m, u = model.load_state_dict(trained_ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "print()\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "   if \"lora\" in n.lower() or \".out.\" in n.lower():\n",
    "       print(n, p.abs().mean().item())\n",
    "\n",
    "print(\"Missing:\", m)\n",
    "print(\"Unexpected keys (should be empty):\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89ad148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 16\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.middle_block.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\n",
      "model.diffusion_model.base_model.model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\n",
      "model.diffusion_model.base_model.model.out.0.weight\n",
      "model.diffusion_model.base_model.model.out.0.bias\n",
      "model.diffusion_model.base_model.model.out.2.weight\n",
      "model.diffusion_model.base_model.model.out.2.bias\n"
     ]
    }
   ],
   "source": [
    "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", len(trainable))\n",
    "for n in trainable:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b97820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 4, 32, 32])\n",
      "pose_net in_features: 51\n",
      "DELTA_POSE: torch.Size([1, 3])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3])\n",
      "get_input raw control: torch.Size([1, 3, 256, 256])\n",
      "COND {'c_crossattn': [tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n",
      "         [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n",
      "         [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n",
      "         ...,\n",
      "         [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n",
      "         [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n",
      "         [ 0.4924, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]]],\n",
      "       device='cuda:0')], 'c_concat': [tensor([[[[ 0.6863,  0.7176,  0.7098,  ...,  0.7098,  0.7098,  0.6863],\n",
      "          [ 0.7098,  0.7176,  0.7098,  ...,  0.7098,  0.7098,  0.7098],\n",
      "          [ 0.7098,  0.7176,  0.7098,  ...,  0.7098,  0.7098,  0.7176],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7098,  ...,  0.3490,  0.3255,  0.3098],\n",
      "          [ 0.7255,  0.7176,  0.7098,  ...,  0.3490,  0.3333,  0.3176],\n",
      "          [ 0.6706,  0.6941,  0.7098,  ...,  0.3647,  0.3569,  0.3255]],\n",
      "\n",
      "         [[ 0.7020,  0.7333,  0.7255,  ...,  0.7020,  0.6941,  0.6627],\n",
      "          [ 0.7255,  0.7333,  0.7255,  ...,  0.7020,  0.7020,  0.6941],\n",
      "          [ 0.7333,  0.7412,  0.7333,  ...,  0.7098,  0.7098,  0.7098],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7098,  ...,  0.1373,  0.1294,  0.1294],\n",
      "          [ 0.7255,  0.7176,  0.7098,  ...,  0.1373,  0.1373,  0.1451],\n",
      "          [ 0.6706,  0.6941,  0.7098,  ...,  0.1529,  0.1608,  0.1451]],\n",
      "\n",
      "         [[ 0.7961,  0.8275,  0.8196,  ...,  0.7804,  0.7961,  0.7647],\n",
      "          [ 0.8196,  0.8196,  0.8118,  ...,  0.7804,  0.7882,  0.7961],\n",
      "          [ 0.8039,  0.8118,  0.8039,  ...,  0.7725,  0.7804,  0.7961],\n",
      "          ...,\n",
      "          [ 0.7882,  0.7882,  0.7725,  ..., -0.0118, -0.0275, -0.0275],\n",
      "          [ 0.7882,  0.7804,  0.7725,  ..., -0.0118, -0.0196, -0.0196],\n",
      "          [ 0.7333,  0.7569,  0.7725,  ...,  0.0039,  0.0039, -0.0196]]]],\n",
      "       device='cuda:0')], 'delta_pose': tensor([[  0.,  45., 180.]], device='cuda:0'), 'in_concat': [tensor([[[[ 3.2378,  5.3988,  3.7735,  ...,  4.5466,  3.3267,  3.9165],\n",
      "          [ 1.9258,  2.3498,  3.2013,  ...,  2.2227,  3.0409,  4.1641],\n",
      "          [ 2.4785,  4.1557,  5.6152,  ...,  4.2551,  2.3683,  2.2876],\n",
      "          ...,\n",
      "          [ 4.3706,  5.0780,  4.4030,  ..., -1.9944, -1.8945, -4.0099],\n",
      "          [ 1.7636,  2.2103,  5.3228,  ...,  3.0114, -0.5986, -4.0230],\n",
      "          [ 1.4355,  3.1296,  3.2588,  ..., -1.5901, -0.8887, -2.7214]],\n",
      "\n",
      "         [[ 0.0846, -0.7107,  1.5374,  ...,  0.7671, -0.8904,  1.6600],\n",
      "          [ 1.0563, -0.7486,  0.7916,  ...,  0.5852, -1.0487,  1.3265],\n",
      "          [ 2.1103,  0.7014,  0.0816,  ..., -0.1325, -0.1025,  0.9003],\n",
      "          ...,\n",
      "          [ 0.5983, -1.0922,  0.2893,  ..., -4.5722,  0.8106, -3.7704],\n",
      "          [ 1.3090, -0.9492, -1.3039,  ...,  2.0116, -1.8382, -1.8014],\n",
      "          [ 3.1826,  1.5109,  1.3126,  ...,  2.7090, -1.6014, -6.6324]],\n",
      "\n",
      "         [[ 0.6335,  0.5324, -0.0425,  ...,  0.7463,  0.1663,  1.8918],\n",
      "          [-0.5257,  2.7448,  2.8873,  ...,  3.4315,  3.1043,  3.6773],\n",
      "          [ 0.9409,  2.9827,  3.7839,  ...,  0.3259,  1.1634,  1.3005],\n",
      "          ...,\n",
      "          [ 2.4343,  3.7943, -1.8661,  ..., -6.6930, -3.9938, -2.8130],\n",
      "          [ 1.2360,  4.7737,  1.3139,  ..., -3.4355, -3.3752, -1.6453],\n",
      "          [-0.1847,  2.1545,  2.0487,  ..., -4.2997, -3.3536, -2.3685]],\n",
      "\n",
      "         [[-2.7437, -3.6103, -3.8525,  ..., -2.3150, -3.9394, -2.2838],\n",
      "          [-4.2893, -5.6099, -4.7129,  ..., -2.1696, -4.4383, -2.6992],\n",
      "          [-2.2154, -2.8941, -1.6969,  ..., -2.8041, -3.5106, -2.9237],\n",
      "          ...,\n",
      "          [-1.1539, -1.4282, -3.0359,  ...,  1.1260,  7.4663,  8.1956],\n",
      "          [-4.0844, -2.0250, -3.6588,  ...,  1.8176,  8.4793,  3.2844],\n",
      "          [-4.1724, -0.4333, -3.2686,  ..., -4.7261,  0.9080, -0.6385]]]],\n",
      "       device='cuda:0')]}\n",
      "loaded? True\n",
      "z: torch.Size([1, 4, 32, 32]) 0.7400818467140198 0.8741363883018494\n",
      "cond keys: dict_keys(['c_crossattn', 'c_concat', 'delta_pose', 'in_concat']) {'c_crossattn': <class 'list'>, 'c_concat': <class 'list'>, 'delta_pose': <class 'torch.Tensor'>, 'in_concat': <class 'list'>}\n",
      "[  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "x_dec batch: 1\n",
      "cond delta_pose batch: 1\n",
      "cond crossattn batch: 1\n",
      "cond in_concat batch: 1\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 20/20 [00:00<00:00, 33.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference v1 (Manually typed inference)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# source image\n",
    "img_name = \"00010.jpg\"\n",
    "src_img = Image.open(f\"examples/{img_name}\").convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # → [-1,1]\n",
    "])\n",
    "\n",
    "src_img = transform(src_img)\n",
    "src_img = src_img.unsqueeze(0).cuda()\n",
    "print(src_img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode_first_stage(src_img)\n",
    "    z = model.get_first_stage_encoding(z)\n",
    "print(z.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     hint_post = model.encode_first_stage(src_img)\n",
    "#     hint_latent = hint_post.mode() if hasattr(hint_post, \"mode\") else hint_post\n",
    "\n",
    "\n",
    "\n",
    "# pose (example)\n",
    "# delta_pose = torch.eye(4)[:3]   # (3,4)\n",
    "# delta_pose = delta_pose.unsqueeze(0).cuda()\n",
    "# delta_pose = torch.tensor([[0.0, 0.0, 0.0]], device=\"cuda\")  # [B=1, 3]\n",
    "# delta_pose = create_rotation_matrix(yaw_deg=30).cuda()\n",
    "\n",
    "print(\"pose_net in_features:\", model.model.diffusion_model.pose_net[0].in_features)\n",
    "\n",
    "yaw_deg = 0\n",
    "yaw = yaw_deg * torch.pi / 180\n",
    "delta_pose = torch.tensor([[yaw, 45.0, 180.0]])\n",
    "# delta_pose = torch.zeros(1, 16).cuda()\n",
    "# delta_pose = torch.randn(1, 16).cuda() * 0.5\n",
    "# delta_pose = torch.tensor([[9.50876296e-01, 3.86712351e-09,  3.09571296e-01,  1.24118519e+00,\n",
    "#   -3.93458927e-16,  1.00000000e+00, -1.24918680e-08, -5.00844948e-08,\n",
    "#   -3.09571296e-01,  1.18782193e-08,  9.50876296e-01,  4.01241302e+00,\n",
    "#    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]], device=\"cuda\")\n",
    "print(\"DELTA_POSE:\", delta_pose.shape)\n",
    "\n",
    "# delta_pose_cfg = delta_pose.repeat(2, 1) # Shape becomes [2, 16]\n",
    "\n",
    "\n",
    "# empty text\n",
    "txt = [\"\"]\n",
    "\n",
    "batch = {\n",
    "        \"jpg\": src_img,\n",
    "        \"hint\": src_img,\n",
    "        # 'mask': mask,\n",
    "        \"delta_pose\": delta_pose, # pose from original angle\n",
    "        # \"subject_id\": os.path.basename(sub_path),\n",
    "        \"txt\": txt\n",
    "    }\n",
    "\n",
    "print(src_img.shape)\n",
    "print(delta_pose.shape)\n",
    "\n",
    "x_T = torch.randn_like(z)\n",
    "\n",
    "# cond = {\n",
    "#     \"in_concat\": [z],\n",
    "#     \"c_crossattn\": [model.get_learned_conditioning(txt)],\n",
    "#     \"c_concat\": [src_img],\n",
    "#     \"delta_pose\": delta_pose,\n",
    "# }\n",
    "\n",
    "# uc_cond = {\n",
    "#     \"in_concat\": [z * 0],\n",
    "#     \"c_crossattn\": [model.get_unconditional_conditioning(1)],\n",
    "#     \"c_concat\": [src_img],\n",
    "#     \"delta_pose\": delta_pose,\n",
    "# }\n",
    "\n",
    "x, cond = model.get_input(batch, model.first_stage_key)\n",
    "# cond[\"delta_pose\"] = torch.zeros_like(cond[\"delta_pose\"])\n",
    "print(\"COND\", cond)\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     samples, _ = sampler.sample(\n",
    "#         S=50,                      # inference steps\n",
    "#         batch_size=1,\n",
    "#         shape=(4, 64, 64),         # latent shape (예시)\n",
    "#         conditioning=cond,         # conditioning (same as training)\n",
    "#         unconditional_guidance_scale=7.5,\n",
    "#         unconditional_conditioning=uc_cond,\n",
    "#         eta=0.0\n",
    "#     )\n",
    "\n",
    "# txt2img\n",
    "# with torch.no_grad():\n",
    "#     samples, _ = sampler.sample(\n",
    "#         S=20,                      # inference steps\n",
    "#         batch_size=1,\n",
    "#         shape=(z.shape[1], z.shape[2], z.shape[3]),         # latent shape\n",
    "#         conditioning=cond,         # conditioning (same as training)\n",
    "#         unconditional_guidance_scale=1,\n",
    "#         # unconditional_conditioning=uc_cond,\n",
    "#         # eta=0.0\n",
    "#     )\n",
    "\n",
    "print(\"loaded?\", any((p.abs().mean().item() > 0) for p in model.parameters()))\n",
    "print(\"z:\", z.shape, z.abs().mean().item(), z.std().item())\n",
    "print(\"cond keys:\", cond.keys(), {k: type(v) for k,v in cond.items()})\n",
    "\n",
    "# img2img\n",
    "sampler.make_schedule(\n",
    "    ddim_num_steps=50,\n",
    "    ddim_eta=0.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(sampler.ddim_timesteps)\n",
    "\n",
    "strength = 0.4\n",
    "t_enc = int(strength * len(sampler.ddim_timesteps))\n",
    "z_enc = sampler.stochastic_encode(z, torch.tensor([t_enc]).cuda())\n",
    "\n",
    "print(\"x_dec batch:\", z_enc.shape[0])\n",
    "print(\"cond delta_pose batch:\", cond[\"delta_pose\"].shape[0])\n",
    "print(\"cond crossattn batch:\", cond[\"c_crossattn\"][0].shape[0])\n",
    "print(\"cond in_concat batch:\", cond[\"in_concat\"][0].shape[0])\n",
    "\n",
    "\n",
    "# Decode\n",
    "samples = sampler.decode(\n",
    "    z_enc,\n",
    "    cond,\n",
    "    t_enc,\n",
    "    unconditional_guidance_scale=3.0,\n",
    "    unconditional_conditioning=None\n",
    ")\n",
    "\n",
    "# Decode\n",
    "with torch.no_grad():\n",
    "    out = model.decode_first_stage(samples)\n",
    "out = torch.clamp((out + 1) / 2, 0, 1)\n",
    "print(out.shape)\n",
    "save_image(out, \"test_output.png\")\n",
    "\n",
    "\n",
    "# LOSS\n",
    "    # Perceptual Loss, Contrastive Loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c88e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 4, 32, 32])\n",
      "pose_net in_features: 51\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:01<00:00, 25.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference v2 (Inference code used in Gradio)\n",
    "# TOSS app.py to ipynb\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# source image\n",
    "img_name = \"00010.jpg\"\n",
    "src_img = Image.open(f\"examples/{img_name}\").convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.5]*3, [0.5]*3)  # → [-1,1]\n",
    "])\n",
    "\n",
    "src_img = transform(src_img)\n",
    "src_img = src_img.unsqueeze(0).cuda()\n",
    "print(src_img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode_first_stage(src_img)\n",
    "    z = model.get_first_stage_encoding(z)\n",
    "print(z.shape)\n",
    "\n",
    "print(\"pose_net in_features:\", model.model.diffusion_model.pose_net[0].in_features)\n",
    "\n",
    "# empty text\n",
    "txt = [\"\"]\n",
    "\n",
    "# batch = {\n",
    "#         \"jpg\": src_img,\n",
    "#         \"hint\": src_img,\n",
    "#         # 'mask': mask,\n",
    "#         \"delta_pose\": delta_pose, # pose from original angle\n",
    "#         # \"subject_id\": os.path.basename(sub_path),\n",
    "#         \"txt\": txt\n",
    "#     }\n",
    "\n",
    "n_samples = 1\n",
    "prompt_scale = 0.0\n",
    "img_scale = 1.0\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "h = 256\n",
    "w = 256\n",
    "# yaw_deg = 0\n",
    "# yaw = yaw_deg * torch.pi / 180\n",
    "\n",
    "# delta_pose = torch.tensor([[9.50876296e-01, 3.86712351e-09,  3.09571296e-01,  1.24118519e+00,\n",
    "#   -3.93458927e-16,  1.00000000e+00, -1.24918680e-08, -5.00844948e-08,\n",
    "#   -3.09571296e-01,  1.18782193e-08,  9.50876296e-01,  4.01241302e+00,\n",
    "#    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]], device=\"cuda\")\n",
    "\n",
    "import math\n",
    "def get_T_from_relative(x, y, z, pose_enc=\"freq\")->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: relative polar degree\n",
    "        y: relative azimuth degree\n",
    "        z: relative distance\n",
    "        \n",
    "    example:\n",
    "        (0., -90., 0.): left view\n",
    "        (0., 90., 0.): right view\n",
    "        (0., 180., 0.): back view\n",
    "        (-90., 0., 0.): top view\n",
    "        (90., 0., 0.): bottom view\n",
    "    \"\"\"\n",
    "    print(\"POSE_ENC:\", pose_enc)\n",
    "    if pose_enc in [\"freq\",\"identity\", \"vae\"]:\n",
    "        d_T = torch.tensor([math.radians(x), math.radians(y), z])\n",
    "    elif pose_enc == \"zero\":\n",
    "        d_T = torch.tensor([math.radians(x), math.sin(\n",
    "                math.radians(y)), math.cos(math.radians(y)), z])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return d_T\n",
    "\n",
    "\n",
    "delta_pose = torch.zeros((1, 3), device=\"cuda\")\n",
    "yaw_deg = -20.0\n",
    "yaw = yaw_deg * torch.pi / 180\n",
    "delta_pose = torch.tensor([[0, 0, 0]], device=\"cuda\")\n",
    "\n",
    "# print(\"DELTA_POSE:\", delta_pose.shape)\n",
    "\n",
    "# delta_pose = get_T_from_relative(0, 0, 0, pose_enc=model.model.diffusion_model.pose_enc)\n",
    "\n",
    "# delta_pose_cfg = delta_pose.repeat(2, 1) # Shape becomes [2, 16]\n",
    "\n",
    "\n",
    "# hint\n",
    "c_cat = src_img\n",
    "# text\n",
    "uc_cross = model.get_unconditional_conditioning(n_samples)\n",
    "c = model.get_learned_conditioning(txt)\n",
    "# camera pose\n",
    "# delta_pose = T[None, :].repeat(n_samples, 1).to(c.device)\n",
    "# concat for concat pipline\n",
    "in_concat = model.encode_first_stage(((src_img*2-1).to(c.device))).mode().detach()\n",
    "\n",
    "\n",
    "cond = {}\n",
    "cond['delta_pose'] = delta_pose\n",
    "cond['c_crossattn'] = [c]\n",
    "cond['c_concat'] = [c_cat]\n",
    "cond['in_concat'] = [in_concat]\n",
    "\n",
    "# uc2 for prompt\n",
    "uc2 = {}\n",
    "uc2['delta_pose'] = delta_pose\n",
    "uc2['c_crossattn'] = [uc_cross]\n",
    "uc2['c_concat'] = [c_cat]\n",
    "uc2['in_concat'] = [in_concat]\n",
    "            \n",
    "# uc for image\n",
    "uc = {}\n",
    "uc['delta_pose'] = delta_pose\n",
    "uc['c_crossattn'] = [uc_cross]\n",
    "uc['c_concat'] = [c_cat]\n",
    "uc['in_concat'] = [in_concat*0] \n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "shape = [4, h // 8, w // 8]\n",
    "x_T = torch.randn(in_concat.shape, device=c.device)\n",
    "samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                conditioning=cond,\n",
    "                                batch_size=n_samples,\n",
    "                                shape=shape,\n",
    "                                verbose=False,\n",
    "                                unconditional_guidance_scale=img_scale,\n",
    "                                unconditional_conditioning=uc,\n",
    "                                unconditional_guidance_scale2=prompt_scale,\n",
    "                                unconditional_conditioning2=uc2,\n",
    "                                eta=ddim_eta,\n",
    "                                x_T=x_T)\n",
    "print(samples_ddim.shape)\n",
    "x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "\n",
    "out = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0).cpu()\n",
    "\n",
    "print(out.shape)\n",
    "save_image(out, \"test_output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
