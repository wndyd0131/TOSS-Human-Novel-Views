{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model (Training)\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "state_dict = load_state_dict(\"ckpt/toss.ckpt\")\n",
    "\n",
    "keys_to_remove = [\n",
    "    k for k in state_dict.keys()\n",
    "    if \"pose_net\" in k\n",
    "]\n",
    "\n",
    "for k in keys_to_remove:\n",
    "    print(\"Removing:\", k)\n",
    "    del state_dict[k]\n",
    "\n",
    "m, u = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"Missing keys:\", m)\n",
    "print(\"Unexpected keys:\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portrait4D Dataset Class\n",
    "\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# root\n",
    "    # 251514-296055\n",
    "        # src_img.png\n",
    "        # 00000.jpg-00020.jpg # tgt\n",
    "        # 00000_mask.png-00020.png # mask\n",
    "        # poses.npy # pose\n",
    "        \n",
    "        # src_img\n",
    "        # tgt_img\n",
    "        # mask\n",
    "        # delta_pose\n",
    "        # txt\n",
    "import os\n",
    "\n",
    "class Portrait4dDataset:\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # self.root = Path(root)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.subjects = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.samples = []\n",
    "        \n",
    "        for sub in self.subjects:\n",
    "            sub_path = os.path.join(root_dir, sub)\n",
    "            views = sorted([f for f in os.listdir(sub_path) if f.endswith('.jpg')])\n",
    "\n",
    "            for view_file in views:\n",
    "                view_id = view_file.split('.')[0]\n",
    "                mask_file = f\"{view_id}_mask.png\"\n",
    "                \n",
    "                if os.path.exists(os.path.join(sub_path, mask_file)):\n",
    "                    self.samples.append({\n",
    "                        'sub_path': sub_path,\n",
    "                        'view_file': view_file,\n",
    "                        'mask_file': mask_file,\n",
    "                        'view_index': int(view_id)\n",
    "                    })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        sub_path = sample['sub_path']\n",
    "        src_idx = 9\n",
    "        \n",
    "        # Load Source Image\n",
    "        src_path = os.path.join(sub_path, \"src.jpg\")\n",
    "        src = Image.open(src_path).convert(\"RGB\")\n",
    "\n",
    "        # Load View Image\n",
    "        img_path = os.path.join(sub_path, sample['view_file'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load Mask\n",
    "        mask_path = os.path.join(sub_path, sample['mask_file'])\n",
    "        mask = Image.open(mask_path).convert(\"L\") # 1-channel grayscale\n",
    "        \n",
    "        # Load Pose\n",
    "        poses = np.load(os.path.join(sub_path, 'poses.npy'))\n",
    "        \n",
    "        src_pose = torch.from_numpy(poses[src_idx]).float()\n",
    "        \n",
    "        # Target: The pose of the current view (000xx.jpg)\n",
    "        tgt_pose = torch.from_numpy(poses[sample['view_index']]).float()\n",
    "        \n",
    "        # Delta: The movement required to get from 'hint' to 'target'\n",
    "        delta_pose = tgt_pose - src_pose\n",
    "        \n",
    "        if self.transform:\n",
    "            src = self.transform(src)\n",
    "            image = self.transform(image)\n",
    "            mask_transform = T.Compose([\n",
    "                T.Resize((256, 256)),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "            mask = mask_transform(mask)\n",
    "    \n",
    "        return {\n",
    "            \"jpg\": image,\n",
    "            \"hint\": src,\n",
    "            'mask': mask,\n",
    "            \"delta_pose\": delta_pose, # pose from original angle\n",
    "            \"subject_id\": os.path.basename(sub_path),\n",
    "            \"txt\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c534043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = Portrait4dDataset(\n",
    "    root_dir=\"datasets/portrait4d\", transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/v2/\",\n",
    "    filename=\"toss-{step}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=500\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=3000,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "model.learning_rate = 3e-5\n",
    "model.sd_locked = True\n",
    "model.first_stage_key = \"jpg\"\n",
    "model.control_key = \"hint\"\n",
    "model.cond_stage_key = \"txt\"\n",
    "\n",
    "import torch.utils.checkpoint as cp\n",
    "cp.checkpoint = lambda func, *args, **kwargs: func(*args)\n",
    "\n",
    "trainer.fit(model, dataloader)\n",
    "\n",
    "# print(\"Initializing iterator...\")\n",
    "# data_iter = iter(dataloader)\n",
    "\n",
    "# print(\"Fetching first batch...\")\n",
    "# batch = next(data_iter)\n",
    "# print(\"Success! Batch keys:\", batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model (Inference)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the last checkpoint\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "state_dict = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "\n",
    "# Remove the old pose_net so your new one can take its place\n",
    "keys_to_remove = [k for k in state_dict.keys() if \"pose_net\" in k]\n",
    "for k in keys_to_remove:\n",
    "    del state_dict[k]\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "trained_ckpt = torch.load(\"checkpoints/v2/last.ckpt\", map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# This will inject your LoRA weights and your trained PoseNet\n",
    "m, u = model.load_state_dict(trained_ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "print(\"Unexpected keys (should be empty):\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b97820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference v1 (Manually typed inference)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# source image\n",
    "src_img = Image.open(\"minion-toy-vbr.png\").convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # → [-1,1]\n",
    "])\n",
    "\n",
    "src_img = transform(src_img)\n",
    "src_img = src_img.unsqueeze(0).cuda()\n",
    "print(src_img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode_first_stage(src_img)\n",
    "    z = model.get_first_stage_encoding(z)\n",
    "print(z.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     hint_post = model.encode_first_stage(src_img)\n",
    "#     hint_latent = hint_post.mode() if hasattr(hint_post, \"mode\") else hint_post\n",
    "\n",
    "\n",
    "\n",
    "# pose (example)\n",
    "# delta_pose = torch.eye(4)[:3]   # (3,4)\n",
    "# delta_pose = delta_pose.unsqueeze(0).cuda()\n",
    "# delta_pose = torch.tensor([[0.0, 0.0, 0.0]], device=\"cuda\")  # [B=1, 3]\n",
    "# delta_pose = create_rotation_matrix(yaw_deg=30).cuda()\n",
    "\n",
    "print(\"pose_net in_features:\", model.model.diffusion_model.pose_net[0].in_features)\n",
    "\n",
    "yaw_deg = 0\n",
    "yaw = yaw_deg * torch.pi / 180\n",
    "delta_pose = torch.tensor([[0.0, 0.0, 0.0]], device=\"cuda\")\n",
    "# delta_pose = torch.zeros(1, 16).cuda()\n",
    "# delta_pose = torch.randn(1, 16).cuda() * 0.5\n",
    "# delta_pose = torch.tensor([[9.50876296e-01, 3.86712351e-09,  3.09571296e-01,  1.24118519e+00,\n",
    "#   -3.93458927e-16,  1.00000000e+00, -1.24918680e-08, -5.00844948e-08,\n",
    "#   -3.09571296e-01,  1.18782193e-08,  9.50876296e-01,  4.01241302e+00,\n",
    "#    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]], device=\"cuda\")\n",
    "print(\"DELTA_POSE:\", delta_pose.shape)\n",
    "\n",
    "# delta_pose_cfg = delta_pose.repeat(2, 1) # Shape becomes [2, 16]\n",
    "\n",
    "\n",
    "# empty text\n",
    "txt = [\"\"]\n",
    "\n",
    "batch = {\n",
    "        \"jpg\": src_img,\n",
    "        \"hint\": src_img,\n",
    "        # 'mask': mask,\n",
    "        \"delta_pose\": delta_pose, # pose from original angle\n",
    "        # \"subject_id\": os.path.basename(sub_path),\n",
    "        \"txt\": txt\n",
    "    }\n",
    "\n",
    "print(src_img.shape)\n",
    "print(delta_pose.shape)\n",
    "\n",
    "x_T = torch.randn_like(z)\n",
    "\n",
    "# cond = {\n",
    "#     \"in_concat\": [z],\n",
    "#     \"c_crossattn\": [model.get_learned_conditioning(txt)],\n",
    "#     \"c_concat\": [src_img],\n",
    "#     \"delta_pose\": delta_pose,\n",
    "# }\n",
    "\n",
    "# uc_cond = {\n",
    "#     \"in_concat\": [z * 0],\n",
    "#     \"c_crossattn\": [model.get_unconditional_conditioning(1)],\n",
    "#     \"c_concat\": [src_img],\n",
    "#     \"delta_pose\": delta_pose,\n",
    "# }\n",
    "\n",
    "x, cond = model.get_input(batch, model.first_stage_key)\n",
    "# cond[\"delta_pose\"] = torch.zeros_like(cond[\"delta_pose\"])\n",
    "print(\"COND\", cond)\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     samples, _ = sampler.sample(\n",
    "#         S=50,                      # inference steps\n",
    "#         batch_size=1,\n",
    "#         shape=(4, 64, 64),         # latent shape (예시)\n",
    "#         conditioning=cond,         # conditioning (same as training)\n",
    "#         unconditional_guidance_scale=7.5,\n",
    "#         unconditional_conditioning=uc_cond,\n",
    "#         eta=0.0\n",
    "#     )\n",
    "\n",
    "# txt2img\n",
    "# with torch.no_grad():\n",
    "#     samples, _ = sampler.sample(\n",
    "#         S=20,                      # inference steps\n",
    "#         batch_size=1,\n",
    "#         shape=(z.shape[1], z.shape[2], z.shape[3]),         # latent shape\n",
    "#         conditioning=cond,         # conditioning (same as training)\n",
    "#         unconditional_guidance_scale=1,\n",
    "#         # unconditional_conditioning=uc_cond,\n",
    "#         # eta=0.0\n",
    "#     )\n",
    "\n",
    "print(\"loaded?\", any((p.abs().mean().item() > 0) for p in model.parameters()))\n",
    "print(\"z:\", z.shape, z.abs().mean().item(), z.std().item())\n",
    "print(\"cond keys:\", cond.keys(), {k: type(v) for k,v in cond.items()})\n",
    "\n",
    "# img2img\n",
    "sampler.make_schedule(\n",
    "    ddim_num_steps=50,\n",
    "    ddim_eta=0.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(sampler.ddim_timesteps)\n",
    "\n",
    "strength = 0.4\n",
    "t_enc = int(strength * len(sampler.ddim_timesteps))\n",
    "z_enc = sampler.stochastic_encode(z, torch.tensor([t_enc]).cuda())\n",
    "\n",
    "print(\"x_dec batch:\", z_enc.shape[0])\n",
    "print(\"cond delta_pose batch:\", cond[\"delta_pose\"].shape[0])\n",
    "print(\"cond crossattn batch:\", cond[\"c_crossattn\"][0].shape[0])\n",
    "print(\"cond in_concat batch:\", cond[\"in_concat\"][0].shape[0])\n",
    "\n",
    "\n",
    "# Decode\n",
    "samples = sampler.decode(\n",
    "    z_enc,\n",
    "    cond,\n",
    "    t_enc,\n",
    "    unconditional_guidance_scale=3.0,\n",
    "    unconditional_conditioning=None\n",
    ")\n",
    "\n",
    "# Decode\n",
    "with torch.no_grad():\n",
    "    out = model.decode_first_stage(samples)\n",
    "out = torch.clamp((out + 1) / 2, 0, 1)\n",
    "print(out.shape)\n",
    "save_image(out, \"test_output.png\")\n",
    "\n",
    "\n",
    "# LOSS\n",
    "    # Perceptual Loss, Contrastive Loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference v2 (Inference code used in Gradio)\n",
    "# TOSS app.py to ipynb\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# source image\n",
    "src_img = Image.open(\"examples/00010.jpg\").convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize([0.5]*3, [0.5]*3)  # → [-1,1]\n",
    "])\n",
    "\n",
    "src_img = transform(src_img)\n",
    "src_img = src_img.unsqueeze(0).cuda()\n",
    "print(src_img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode_first_stage(src_img)\n",
    "    z = model.get_first_stage_encoding(z)\n",
    "print(z.shape)\n",
    "\n",
    "print(\"pose_net in_features:\", model.model.diffusion_model.pose_net[0].in_features)\n",
    "\n",
    "# empty text\n",
    "txt = [\"\"]\n",
    "\n",
    "# batch = {\n",
    "#         \"jpg\": src_img,\n",
    "#         \"hint\": src_img,\n",
    "#         # 'mask': mask,\n",
    "#         \"delta_pose\": delta_pose, # pose from original angle\n",
    "#         # \"subject_id\": os.path.basename(sub_path),\n",
    "#         \"txt\": txt\n",
    "#     }\n",
    "\n",
    "n_samples = 1\n",
    "prompt_scale = 0.0\n",
    "img_scale = 1.0\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "h = 256\n",
    "w = 256\n",
    "# yaw_deg = 0\n",
    "# yaw = yaw_deg * torch.pi / 180\n",
    "\n",
    "# delta_pose = torch.tensor([[9.50876296e-01, 3.86712351e-09,  3.09571296e-01,  1.24118519e+00,\n",
    "#   -3.93458927e-16,  1.00000000e+00, -1.24918680e-08, -5.00844948e-08,\n",
    "#   -3.09571296e-01,  1.18782193e-08,  9.50876296e-01,  4.01241302e+00,\n",
    "#    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00]], device=\"cuda\")\n",
    "\n",
    "import math\n",
    "def get_T_from_relative(x, y, z, pose_enc=\"freq\")->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: relative polar degree\n",
    "        y: relative azimuth degree\n",
    "        z: relative distance\n",
    "        \n",
    "    example:\n",
    "        (0., -90., 0.): left view\n",
    "        (0., 90., 0.): right view\n",
    "        (0., 180., 0.): back view\n",
    "        (-90., 0., 0.): top view\n",
    "        (90., 0., 0.): bottom view\n",
    "    \"\"\"\n",
    "    print(\"POSE_ENC:\", pose_enc)\n",
    "    if pose_enc in [\"freq\",\"identity\"]:\n",
    "        d_T = torch.tensor([math.radians(x), math.radians(y), z])\n",
    "    elif pose_enc == \"zero\":\n",
    "        d_T = torch.tensor([math.radians(x), math.sin(\n",
    "                math.radians(y)), math.cos(math.radians(y)), z])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return d_T\n",
    "\n",
    "\n",
    "# delta_pose = torch.zeros((1, 3), device=\"cuda\")\n",
    "# print(\"DELTA_POSE:\", delta_pose.shape)\n",
    "\n",
    "delta_pose = get_T_from_relative(0, 0, 0, pose_enc=model.model.diffusion_model.pose_enc)\n",
    "\n",
    "# delta_pose_cfg = delta_pose.repeat(2, 1) # Shape becomes [2, 16]\n",
    "\n",
    "\n",
    "# hint\n",
    "c_cat = src_img\n",
    "# text\n",
    "uc_cross = model.get_unconditional_conditioning(n_samples)\n",
    "c = model.get_learned_conditioning(txt)\n",
    "# camera pose\n",
    "# delta_pose = T[None, :].repeat(n_samples, 1).to(c.device)\n",
    "# concat for concat pipline\n",
    "in_concat = model.encode_first_stage(((src_img*2-1).to(c.device))).mode().detach()\n",
    "\n",
    "\n",
    "cond = {}\n",
    "cond['delta_pose'] = delta_pose\n",
    "cond['c_crossattn'] = [c]\n",
    "cond['c_concat'] = [c_cat]\n",
    "cond['in_concat'] = [in_concat]\n",
    "\n",
    "# uc2 for prompt\n",
    "uc2 = {}\n",
    "uc2['delta_pose'] = delta_pose\n",
    "uc2['c_crossattn'] = [uc_cross]\n",
    "uc2['c_concat'] = [c_cat]\n",
    "uc2['in_concat'] = [in_concat]\n",
    "            \n",
    "# uc for image\n",
    "uc = {}\n",
    "uc['delta_pose'] = delta_pose\n",
    "uc['c_crossattn'] = [uc_cross]\n",
    "uc['c_concat'] = [c_cat]\n",
    "uc['in_concat'] = [in_concat*0] \n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "shape = [4, h // 8, w // 8]\n",
    "x_T = torch.randn(in_concat.shape, device=c.device)\n",
    "samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                conditioning=cond,\n",
    "                                batch_size=n_samples,\n",
    "                                shape=shape,\n",
    "                                verbose=False,\n",
    "                                unconditional_guidance_scale=img_scale,\n",
    "                                unconditional_conditioning=uc,\n",
    "                                unconditional_guidance_scale2=prompt_scale,\n",
    "                                unconditional_conditioning2=uc2,\n",
    "                                eta=ddim_eta,\n",
    "                                x_T=x_T)\n",
    "print(samples_ddim.shape)\n",
    "x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "\n",
    "out = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0).cpu()\n",
    "\n",
    "print(out.shape)\n",
    "save_image(out, \"test_output.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
