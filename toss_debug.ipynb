{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# root\n",
    "    # 251514-296055...\n",
    "        # src_img.png\n",
    "        # 00000.jpg-00020.jpg # tgt\n",
    "        # 00000_mask.png-00020.png # mask\n",
    "        # poses.npy # pose\n",
    "\n",
    "class Portrait4dDataset:\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # self.root = Path(root)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.subjects = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.samples = []\n",
    "        \n",
    "        for sub in self.subjects:\n",
    "            sub_path = os.path.join(root_dir, sub)\n",
    "            views = sorted([f for f in os.listdir(sub_path) if f.endswith('.jpg')])\n",
    "\n",
    "            for view_file in views:\n",
    "                view_id = view_file.split('.')[0]\n",
    "                mask_file = f\"{view_id}_mask.png\"\n",
    "                \n",
    "                if os.path.exists(os.path.join(sub_path, mask_file)):\n",
    "                    self.samples.append({\n",
    "                        'sub_path': sub_path,\n",
    "                        'view_file': view_file,\n",
    "                        'mask_file': mask_file,\n",
    "                        'view_index': int(view_id)\n",
    "                    })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        sub_path = sample['sub_path']\n",
    "        src_idx = 9\n",
    "        \n",
    "        # Load Source Image\n",
    "        src_path = os.path.join(sub_path, \"src.jpg\")\n",
    "        src = Image.open(src_path).convert(\"RGB\")\n",
    "\n",
    "        # Load View Image\n",
    "        img_path = os.path.join(sub_path, sample['view_file'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load Mask\n",
    "        mask_path = os.path.join(sub_path, sample['mask_file'])\n",
    "        mask = Image.open(mask_path).convert(\"L\") # 1-channel grayscale\n",
    "        \n",
    "        # Load Pose\n",
    "        poses = np.load(os.path.join(sub_path, 'poses.npy'))\n",
    "        \n",
    "        src_pose = torch.from_numpy(poses[src_idx]).float()\n",
    "        \n",
    "        # Target: The pose of the current view (000xx.jpg)\n",
    "        tgt_pose = torch.from_numpy(poses[sample['view_index']]).float()\n",
    "        \n",
    "        # Delta: The movement required to get from 'hint' to 'target'\n",
    "        delta_pose = tgt_pose - src_pose\n",
    "        \n",
    "        if self.transform:\n",
    "            src = self.transform(src)\n",
    "            image = self.transform(image)\n",
    "            mask_transform = T.Compose([\n",
    "                T.Resize((256, 256)),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "            mask = mask_transform(mask)\n",
    "    \n",
    "        return {\n",
    "            \"jpg\": image,\n",
    "            \"hint\": src,\n",
    "            'mask': mask,\n",
    "            \"delta_pose\": delta_pose, # pose from original angle\n",
    "            \"subject_id\": os.path.basename(sub_path),\n",
    "            \"txt\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the last checkpoint\n",
    "import torch\n",
    "from cldm.toss import TOSS\n",
    "from cldm.model import load_state_dict\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "config = OmegaConf.load(\"models/toss_vae.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "\n",
    "state_dict = torch.load(\"ckpt/toss.ckpt\", map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "# Remove the old pose_net so your new one can take its place\n",
    "keys_to_remove = [k for k in state_dict.keys() if \"pose_net\" in k]\n",
    "for k in keys_to_remove:\n",
    "    del state_dict[k]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "trained_ckpt = torch.load(\"checkpoints/v2/last.ckpt\", map_location=\"cpu\", weights_only=False)\n",
    "# This will inject your LoRA weights and your trained PoseNet\n",
    "m, u = model.load_state_dict(trained_ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "print(\"Unexpected keys (should be empty):\", u)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# source image\n",
    "src_img = Image.open(\"00010.jpg\").convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # â†’ [-1,1]\n",
    "])\n",
    "\n",
    "src_img = transform(src_img)\n",
    "src_img = src_img.unsqueeze(0).cuda()\n",
    "print(src_img.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode_first_stage(src_img)\n",
    "    z = model.get_first_stage_encoding(z)\n",
    "print(z.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     hint_post = model.encode_first_stage(src_img)\n",
    "#     hint_latent = hint_post.mode() if hasattr(hint_post, \"mode\") else hint_post\n",
    "\n",
    "\n",
    "\n",
    "# pose (example)\n",
    "# delta_pose = torch.eye(4)[:3]   # (3,4)\n",
    "# delta_pose = delta_pose.unsqueeze(0).cuda()\n",
    "# delta_pose = torch.tensor([[0.0, 0.0, 0.0]], device=\"cuda\")  # [B=1, 3]\n",
    "# delta_pose = create_rotation_matrix(yaw_deg=30).cuda()\n",
    "delta_pose = torch.tensor([[0.0, 0.0, 0.0]], device=\"cuda\")\n",
    "\n",
    "# delta_pose_cfg = delta_pose.repeat(2, 1) # Shape becomes [2, 16]\n",
    "\n",
    "\n",
    "# empty text\n",
    "txt = [\"\"]\n",
    "\n",
    "print(src_img.shape)\n",
    "print(delta_pose.shape)\n",
    "\n",
    "x_T = torch.randn_like(z)\n",
    "\n",
    "cond = {\n",
    "    \"in_concat\": [z],\n",
    "    \"c_crossattn\": [model.get_learned_conditioning(txt)],\n",
    "    \"c_concat\": [src_img],\n",
    "    \"delta_pose\": delta_pose,\n",
    "}\n",
    "\n",
    "uc = {\n",
    "    \"in_concat\": [z * 0],\n",
    "    \"c_crossattn\": [model.get_unconditional_conditioning(1)],\n",
    "    \"c_concat\": [src_img],\n",
    "    \"delta_pose\": delta_pose,\n",
    "}\n",
    "\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "sampler.make_schedule(\n",
    "    ddim_num_steps=50,\n",
    "    ddim_eta=0.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(sampler.ddim_timesteps)\n",
    "\n",
    "strength = 0.4\n",
    "t_enc = int(strength * len(sampler.ddim_timesteps))\n",
    "\n",
    "z_enc = sampler.stochastic_encode(z, torch.tensor([t_enc]).cuda())\n",
    "\n",
    "# Decode\n",
    "samples = sampler.decode(\n",
    "    z_enc,\n",
    "    cond,\n",
    "    t_enc,\n",
    "    unconditional_guidance_scale=3.0,\n",
    "    unconditional_conditioning=None\n",
    ")\n",
    "\n",
    "# Decode\n",
    "with torch.no_grad():\n",
    "    out = model.decode_first_stage(samples)\n",
    "out = torch.clamp((out + 1) / 2, 0, 1)\n",
    "print(out.shape)\n",
    "save_image(out, \"test_output.png\")\n",
    "\n",
    "\n",
    "# LOSS\n",
    "    # Perceptual Loss, Contrastive Loss\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
